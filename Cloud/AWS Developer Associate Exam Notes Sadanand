#15-> AWS Region-> It is the geographical area such as London,North Virginia or us-east-1.
              Each region consists of 2 or more availability zones such as us-east-1a or us-east-1b

Availability Zone(AZ)-> Each Availability Zone is a physical data center in the region, but seperate from the other ones(so that
they're isolated from disasters).
                          
- Their are generally minimum 3 AZ's in a region

- AWS Console (or we can say all AWS Services) are region scoped except IAM and S3 they are global

- To check how many AZ are their in region go to 'AWS Global Infrastructure' then it will show a map in which where these 
  regions are and also show how many AZ's they have.
  
IAM(Identity Access Management)->
- your whole AWS security is in IAM such as -
  Users, groups, roles
- Root account should never be used(and shared) // Root account is with which you create ur AWS account
- IAM is at the center of AWS (means every service in AWS is helped with IAM security).
- Policies are written in JSON
- Roles-> Roles are for the internal usage within the AWS resources and AWS Services, so basically roles is given to machines

- Policies define basically that what the Users, Groups and Roles can and cannot do.

- As IAM is global so the users, groups and roles created are available across all the regions

- One IAM role per application(Each application has it's own lifecycle,their own independence and so you want one IAM role
    per application, nothing shared)    - @@@
    
- Never write IAM credentials in Code(Application Code).

- MFA(Multi Factor Authentication) can be setup for root account and Users.

- IAM has predefined managed policies which means we do not need to write the whole policy we can use the provided one too.

- It's best to give users the minimal amount of permissions they need to perform their job it is termed least priviege principles

IAM Federation -> Big enterprises usally integrate their own repository of users with IAM such as AD(Active Directory) this way
the users can login into AWS using their company credentials.   - @@@
- this IAM federation or we can say Identity Federation uses the SAML standard(such as Active Directory).   - @@@

- IAM Dashboard->
  - We can create user and decide their access type whether Programmtic access (means users can me managed from the aws 
    cli also) or AWS Management Console access(means managed through dashboard only)
  - If you choose AWS Management Console access for a newly created user then select 'require password reset' option so 
    that the user can reset the password according to him at the time of log in 
  - then provide permissions by adding the user to a group or attach policy
  - the access key ID and secret access key ID are shown only once So either download csv file or you had to create them again.
  - after users created we can add users to a group which would have the users having the same type of permissions or we
    can say policies
  - Group can contain many users, and a user can belong to multiple groups
  - Groups can't be nested; they can contain only users, not other groups.
  - There's no default group that automatically includes all users in the AWS account
  - There's a limit to the number of groups you can have, and a limit to how many groups a user can be in (not sure about this point)
  - we can also assign permissions or policies to a user individually 
  - we can also define password policy for IAM users

we can also define the group or we can say the whole AWS sign in same way for the 'ifs-training' and 'cloud-practice' is done
by going to the 'IAM user sign in link'(this link contains the id number of user you can see it on the right
side of IAM dashboard) which is customerizable we can edit it and it will provide us a link which the users have to use every
time to sign in the AWS console so basically the use of root account credentials to access the AWS Console is not needed
same way as in 'ifs-training' is done for every user

EC2(Elastic Compute Cloud) ->
  It mainly consists in the capability of:
  - Renting virtual machines(EC2)
  - Storing Data on Virtual drives(EBS)
  - Distributing load across machines(ELB).
  - Scaling the services using an auto-scaling group(ASG)
  
  AMI(Amazon Machine Image) -> it is basically the operating system which is been used on server or we can say the ec2 instance
  
  SSH-> It allows u to control a remote machine, all using the command line - @@@
  
  How does SSH works?   - @@@
  Suppose on our EC2 machine of which we have public IP and allowed SSH at port 22 in the security group. Now if we want to 
  access ec2 machine we will go to that port 22 of SSH.
  
  - If we go to the ec2 dashboard and check the particular security group's inbound rule we will see the port 22 their. - @@@
  
  - Exam Question for MAC-> When you first download the key pair file of ec2 instance and then for first time try to connect we 
    get warning which is "Warning: Unprotected Private Key File" with permissions 0644      - @@@
  - to fix this we use cmd-> chmod 0400 the_key_name    - @@@
  
  - cmds-> whoami (to know user name)
           ping google.com (to check whether the google.com is responding to our server)
           exit (to close putty)
           logout (to logout the user)
            
 Security Group -> they are the fundamental of network security in AWS.
 - They control how traffic will be allowed into ec2 machines
 - security group controls the inbound and outbound traffic.
 - Security Group is the most fundamental skill to learn to troubleshoot networking issues.     - @@@
 
 - Go to security group dashboard select the security group and then the inbound and outbound of security group will appear
 - Inbound is for the traffic which is coming into the machine and outbound is for the traffic which is going out of the 
    machine.
 - if we delete the inbound of the security group and then we try to connect to ec2 machine we will see it is not connected
 - if in the inbound rules we select the source as anywhere then we will get the IP as 0.0.0.0/0,::/0 which is IP6 format   - @@@
 - if we choose custom then we can generally write as 0.0.0.0/0 which means that SSH is allowed from anywhere   - @@@
 - Security Groups act like firewall
 - They regulate Access of Ports, Authorised IP ranges - IPv4 and IPv6, Control Inbound network(from other to the instance),
   Control outbound network(from instance to the other).
 - Security Groups are attached to multiple instances
 - They are Locked down to a region/VPC combination (u cannot use the security group of other region or VPC into other).    - @@@
 - They live outside the ec2, which means if the traffic is blocked the EC2 instance won't see it   - @@@
 - Best Practice -> it's good to maintain one seperate security group for SSH access    - @@@
 - If ur application is not accessible(time out), then it's a security group issue.     - @@@
 - If your application receives a "connection refused" error, then it's a application error or it's not launched.   - @@@
 - By default all inbound traffic is blocked.   - @@@
 - By default all outbound traffic is authorized.   - @@@
 - Extra feature we can allow multiple ec2 instances of different security groups to connect with one ec2 instance if that
   one ec2 instance has the security groups of all the instances so that way the ec2 instances of different ips can also
   get communicate with each other 

 Private and Public IP->
  - Networking has two sorts of IP's IPv4 and IPv6
  - IPv4: 1.160.10.240 // IPv4 is used most commonly used online
  - IPv6: 3ffe:1900:4545:3:200:f8ff:fe21:67cf // it solves the problems for IoT(Internet of Things).    - @@@
  - IPv4 allows for 3.7 billion different addresses in the public space.
  - IPv4: [0-255].[0-255].[0-255].[0-255] // number could vary between 0-255
  - Servers talk to each other using public Ip's    - @@@
  - Company Networks have generally Private Ip range such as 192.168.0.1/22 so basically it means that all the computers
    in that company network can talk to each other using the private ip and the internet gateway for that company network
    is a public gateway and has public ip range , So basically it means if you have public Ip you can access internet and
    if you have private ip you can only access within network.  - @@@
    
    Public IP's
    - Public IP means the machine can be identified on the internet(WWW).
    - Must be unique across the whole web(not two machines can have the same public IP)
    - Can be geo-located easily
    
    Private IP's
    - Private IP means the machine can only be identified on private network only
    - The IP must be unique only across the private network
    - But two different private networks(two companies) can have the same IP's  - @@@
    - Machines connect to WWW using an internet gateway(a proxy).   - @@@
    - Only a specified range of IP's can be used as private IP's
    
    Elastic Ip's
    - When you start or stop the EC2 instance, it can change it's pubic IP
    - If you need a fixed public IP for your instance, you need an Elastic IP   - @@@
    - An elastic IP is a public IPv4 IP and you own it as long as you don't delete it.  - @@@
    - you can attach a elastic IP to only one instance at a time.   - @@@
    - With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to
      another instance in your account.     - @@@
    - You can only have 5 elastic IP in your account (you can ask AWS to increase that).    - @@@
    - Overall try to avoid using the elastic IP:
      - They often reflect poor architectural decisions.
      - Instead, use a random public IP and registered DNS name to it.
      - Or we can use a load balancer and don't use a Public IP.
      
 - By default a ec2 machine comes with a private IP for the internal AWS network and public IP for the internet access
 - When we are doing SSH into the EC2 machine We can't use a private IP, because we are not in the same network unless
   you have a VPN, we can only use public IP if you don't have a VPN    - @@@
    
  - when we are successfuly connected using SSH we will see that our private IP is shown as like as ->
      [ec2-user@ip-172-31-17-147] (in the terminal of ec2 machine as we have access to private IP after the SSH connection
      is made).
      
 - For the elastic IP's we can simply create a elastic ip from Elastic IP dashboard and associate or release that elastic Ip
   with any ec2 instance.

 EC2 User Data ->
  - It is possible to bootstrap our instances using an EC2 User data script. (bootstrapping means launching commands when the
    machine starts).    - @@@
  - means that script is only run once at the instance first start
  - EC2 user data is used to automate boot tasks such as->
      - Install updates
      - installing softwares
      - download common files from the internet etc.
     but remember more u add to user data more ur instance had to do at the boot time   - @@@
      
   - The ec2 user data script runs with root user   - @@@
   
   - To add the user data script at the time of instance creation at the configure instance details step scroll down to Advance
     details then u will see the user data text box
      
     the script of user data ->
      #!/bin/bash       // it's must to be mentioned in user data script
      sudo su           // then paste all the lines or commands u wants to execute
      
      // the input in the user data text box is going to get base64 encoded
      
      - then just configure the other details for the ec2 instance the common things to create ec2 instance
      
   - On the deletion of instance the security group attached to that instance is not deleted    - @@@
      
  EC2 Pricing ->    - @@@
    - EC2 instances prizes(per hour) varies based on these parameters
      - Region you're in
      - Instance type you're using
      - On demand vs spot vs reserved vs dedicated host
      - linux vs windows vs Private OS(RHEL, SLES, Windows SQL)
      - you are billed by the second , with minimum of 60 seconds means if you create a instance and terminate it in 30 seconds
        then you will be billed for 60 seconds and if you create and terminate the instance in 90 seconds then u will be billed
        for 90 seconds
      - you also have to pay for other factors such as storage, data transfer, fixed IP public addresses, load balancing etc
      - you do not need to pay when the instance is stopped
      - so for 6 seconds the calculation would be done like $0.023/60 = $0.000383(cost for 60 seconds)
    
  What is AMI?
  AMI comes with the base image such as Ubuntu, Fedora, Windows etc
  These images can be customised at runtime using  EC2 user data
  
  what if we could create out own image, ready to go?   - @@@
  we can do this with custom AMI - an image to use to create our instances, custom AMI's can be build for Linux and Windows
  machines
  
  Why would u use a custom AMI?
  Advantages-> - Pre installed packages needed  - @@@
               - Faster boot time(no need for long ec2 user data at boot time)  - @@@
               - Machine comes configured with monitoring/enterprise software
               - Security Concerns - control over the machines in the network
               - Control of maintenance and updates of AMI's over time  - @@@
               - Active Directory Integration out of the box    - @@@
               - Installing you app ahead of time(for faster deploys when auto-scaling) - @@@
               - Use someone else's AMI that is optimized for running an app, DB etc    - @@@
               
  - AMI's are built for specific AWS region - @@@

  - EC2 instances Overview
    - Instances have 5 distinct characterstics advertised on the website:   - @@@
        - The RAM(type,amount,generation)
        - The CPU(type, make, frequency, generation, number of cores)
        - The I/O(disk performances, EBS optimisations)
        - The Network(network bandwidth, network latency)
        - The Graphical Processing Unit(GPU)

   - Their are about 50 instance types on ec2   - @@@

   - https://ec2instances.info/ can help with summarizing the type of instances

   - R instances comes with lot of RAM      - @@@
     C instances comes with lot of CPU      - @@@
     M instances are balanced           - @@@
     T2/T3 instance types are "burstable"           - @@@

     Burstable Instances->      - @@@
     - AWS has concept of burstable instances(T2 Machines)
     - Burst means that overall, the instances has OK CPU performance.
     - When the machine needs to process something unexpected(a spike in load for example),it can burst,and CPU can be very good
     - If the machine bursts, it utilizes "burst credits"
     - If all the credits are gone, the CPU become BAD
     - If the machine stops bursting, credits are accumulated over time
     - Burst instances can be amazing to handle unexpected traffic and getting the insurance that it will be handled correctly
     - If your instances consistently runs low credit, you need to move to a different kind of non-burstable instance

     T2 Unlimited-> - @@@
        - Nov 2017: it is possible to have "unlimited burst credit"
        - You pay extra money if you go over your credit balance, but you don't lose in performance
        - Overall, it is new offering, so be careful, cost's could go high if you are not monitoring the health of your instance

     Question -> You are getting a permission error exception when trying to SSH into your Linux Instance?  - @@@
     Answer -> the key is missing permissions chmod 0400

     Question -> Security groups can reference all of the following except:     - @@@
     Answer -> DNS Name

#1-> Load Balancing -> Load Balancers are servers that forward internet traffic to multiple servers(EC2 instances) downstream

    - So users do not connect to ec2 instances they connect to Load Balancer and then load balancer redirects the traffic to ec2
    instance and then the response from the ec2 comes back to the load balancer and then it is send to the user

    - Scenerio example not real case -> Suppose we have 3 ec2 instances and 3 users then the load balancer will not redirect
    the 3 users to one ec2 instance it will redirect each user to individual ec2 instance so that means each user would be
    communicating with different ec2 instances so this way load is balanced.

    Why use the load balancer ?
    - Spread load across multiple downstream ec2 instances
    - Expose a single point of access(DNS) to your application      - @@@
    - Seamlessly handle failures of downstream instances
    - Do regular health checks to your instances
    - Provide SSL termination(HTTPS) for your websites which means the termination and encryption of the connection is between
      the client and ELB and then the ELB goes and talks to ec2 instances in HTTP traffic       - @@@
    - Enforce stickness with cookies means user will talk to an instance over time
    - High availability across zones which means your load balancer can run across many zones and so do your instances So it
      basically allows your application to be highly available in case an availability zone just fails.      - @@@
    - Seperate public traffic from private traffic

    Why use an EC2 Load Balancer?
    - An ELB is a managed load balancer
        - AWS guarantees that it will be working
        - AWS takes care of upgrades, maintenance, high availability
        - AWS provides few configuration knobs

    - It costs less to setup your own load balancer but it will be a lot more effort on your end,So in the end the total cost
      of ownership is much higher

    - It is integrated with many AWS services such as for monitoring or for compute

  Types of load balancer on AWS
       - Their are 3 kinds of load balancers
          - Classic Load Balancer(v1 - old generation) - 2009
          - Application Load Balancer(v2 - new generation) - 2016
          - Network Load Balancer(v2 - new generation) - 2017

        Overall it is recommended to use the newer / v2 generation load balancers as they provide more features

      - You can setup internal(private) ELB's or external(public) ELB's         - @@@

Health Checks -> Health Checks are crucial for load balancers as they redirect ur traffic to instances that are healthy
      - They enable the load balancer to know if instances it forwards traffic to are available to reply to requests
      - The health check is done on a port and a route(/health is common)
      - If the response is not 200(OK), then the instances is unhealthy

      So the health check is done between the load balancer and ec2 instance and on the port u r using and it will then ask
      for the route slash health and if the instance says "Okay" then it means the instance is healthy and load balancer will
      start sending the traffic to that load balancer otherwise not.

    Application Load Balancer(v2) they are also called "Layer 7" , layer 7 because they allow u to work at http level so it allow
        - Load balancing to multiple HTTP applications across machines(target groups)means we can group these applications in target
            groups.
        - We can do Load balancing to multiple applications on the same machine Ex. containers    - @@@
        - We can load balance based on the route in URL         - @@@
        - We can load balance based on hostname in URL         - @@@

        Question-> We need something to load balance across the same application running on the same machine how we do this?
        Answer -> By using Application load balancer

        - Basically, they're awesome for micro services & container-based application (Ex-> Docker and Amazon ECS and EC2).  - @@@

        - Has a port mapping feature to redirect to a dynamic port as this allows to redirect to the same instance on the application
          running in the same machine.      - @@@

        - In comparison to Classic load balancer , we would need to create one classic laod balancer per application(or microservice)
          before which was very inefficient and expensive.

        - Target Group can be understand as the Group for application which would contain the ec2 instances in it and communication
          between the Application Load Balancer and the Target Group is done by HTTP.

        - Scenerio for target group:- suppose we have two applications one User and one Search and both the aplications need two
          ec2 instances and we have one load balancer so we can bind the User application and it's instances in one target
          group and bind the Search application and it's instances in another target group and now both these target groups will
          be connected to Application Load balancer and communication between target groups and Application load balancer would be
          through HTTP.
        Now the user will access these User and Search Applications in target groups by the help of routes so if the user enters the
        url for User application such as by '/user' then the User target group would be accessed and if user inputs '/search' then
        the Search target group would be accessed but the Application load balancer is only one which is connected to both the target
        groups .

        ALB(Application Load Balancer) v2 Good to know
        - Stickiness can be enabled at the target group level (means the same user or request goes to the same instance)
@@@     - Stickiness is directly generated by ALB(not the application) // means it will add a cookie at the ALB level, not application
@@@     - ALB supports HTTP/HTTPS & Websockets protocols
@@@     - The application servers don't see the IP of the client directly basically the true IP of the client is inserted in the
          header called X-Forwarded-For
        - We can also get Port(X-Forwarded-Port) and proto(X-Forwarded-Proto)
        -> Scenerio for above points :- so the Client IP talks to ALB then the ALB do the connection termination So it establish a
           new connection to your ec2 instance So the ec2 instance sees the private IP of ur load balancer it doesn't see the private
           IP of the client and for the ec2 instance to see the IP of clients then it needs to look at the header X-Forwarded-For and
           by it u can see the clinet IP.

        Network Load Balancer (layer 4)-> Network Load Balancer is for TCP traffic and layer 7 for ALB is for HTTP traffic, network
        balancers gives super high performance and they can handle millions of requests per seconds.
            - It Forward the TCP traffic to ur instances        - @@@
            - Handle millions of requests per seconds           - @@@
            - They support static IP and elastic IP             - @@@
            - Less latency ~100ms(vs 400ms for ALB)             - @@@

        - Network Load Balancers are mostly used for extreme performance and should not be the default load balancer you choose
        - the creation process for the Network load balancer is same as the Application load balancer

        Scenerio for Network Load Balancer-> suppose we have two applications one User and one Search and both the aplications need
        two ec2 instances and we have one load balancer so we can bind the User application and it's instances in one target
        group and bind the Search application and it's instances in another target group and now both these target groups will
        be connected to Network Load Balancer.

        The communication between the client and the Network Load balancer is done by 'TCP + Rules' traffic and the communication
        between the Network Load Balancer and the Traget Group is done by TCP traffic.

        Load Balancers Good to Know->
        - Classic Load Balancers are depricated
        - ALB's are for HTTP/HTTPs and Websocket
        - Network Load Balancer is for TCP
        - The CLB and ALB support SSL certificates and provide SSL termination      - @@@
        - All load balancers have health check capability
        - ALB can route on based on hostname and the path
        - ALB is the great fit with the ECS(Docker)                             - @@@
        - All load balancers have static host name which means we get a url we should not resolve the url and use underlying IP
        - Load Balancers can scale but not instantenously, So basically if u expect a lot of load so then contact AWS for them
          to warm up load balancer so that they scale with you.          - @@@
        - NLB directly see the client IP oppose to ALB and CLB so there is no X-Forwarded-for-Proto header    - @@@
        - 4xx such as 400 are client induced errors                 - @@@
        - 5xx errors are application induced errors (if u get error 503 then it means ur ELB has no more capacity or no registered
            targets).                   - @@@
        - If the load balancer can't connect to your application then check ur security groups      - @@@

    - To create a Load Balancer go to Load Balancer dashboard select the type of Load Balancer u want
    - At the time of Configuring the laod balancer Their is a 'Scheme' option in which if u choose the 'internet-facing' then it will
    be public and if u choose 'internal' then it will be private.

    - The DNS provided by the Load Balancer after it's creation will not change no matter what we do with the instances in the
      back end.
    - In the Load Balancer dashboard after the ALB is created if we select the ALB and then go to it's Listeners tab we will see
      that it is listening at port 80 and see that to which target group it is forwarded to and we can edit and add more target
      groups their.
    - Now u can see that the instance could be accessed directly using the instance public IP and the Load balancer DNS also
    - But we can change it in this way that the instance would be accessed only by using the load balancer for that (importnat fact
      the security groups can reference security groups) , So got to the Security group dashboard and what we want is that the
      inbound traffic of the instance security group should be coming from the load balancer to do this select the instance security
      group and edit the HTTP inbound rule as the 0.0.0.0/0 to the 'sg' and after typing sg the security group load balancer id
      will appear and select it now the instance will be accessible only through the load balancer DNS and if we use the public IP
      of the instance to access it we will get the time out error.

Auto Scaling Group -> In real life , the load on websites and application can change, so more the users more the load their is
     So the goal of an Auto Scaling Group(ASG) is to:-
        - Scale out(add EC2 instances) to match an increased load
        - Scale in(remove EC2 instances) to match an decreased load
        - Ensure that ec2 instances could group to certain amount and reduce to certain amount so we can define the minimum and
          maximum number of machines running in an ASG.
        - Automatically Register new instances to a load Balancer

     So in ASG we have the minimum size of ec2 instances which you will have sure running into this ASG and then their is actual size
     or we 'desired capacity parameter' is the number of EC2's running at the current moment of ASG and then their is maximum size
     parameter in which u decide who many instances can be added for scale out when the load goes up

     How ASG works -> Web traffic goes to load balancer and ASG at the bottom end or we can say after load balancer, so basically
     the load balancer will know directly how to connect to these ASG instances So we will direct the traffic to these instances but
     if the ASG scales out so we add ec2 instances and the load balancers will also register these new targets or instances
     So load balancers and ASG work really well with each other.

   ASG has following attributes->
        - A launch Configuration has        - @@@
            AMI and Instance type,
            EC2 user data,
            EBS Volumes
            Security Groups
            SSH key pair

        - Min/Max Size and also Initial Capacity and as well as the desired capacity
        - Network and Subnets Information       - @@@
        - Load Balancer Information or target group information based on which load balancer we use
        - When we create ASG we will able to define scaling policies means we define what will trigger scale out and what will
            trigger scale in

  Auto Scaling Alarms ->
    - It is possible to scale ur ASG's based on CloudWatch Alarms       - @@@
    - Basically the ASG will scale based on the alarms and the alarms can be anything u want to be metrics such as average CPU
    - Metrics are computed for the overall ASG instances
    - Based on the alarm we can create scale-out policies and scale-in policies     - @@@

Auto Scaling New Rules->
  - It is now possible to define "better" auto scaling rules that are directly managed by ec2       - @@@
      - Such as I want to have a target average CPU usage in my ASG and basically it will scale in and scale out based on
        ur load to meet the target CPU usage        - @@@
      - You can also have a rule based on the number of requests on the ELB per instance.
      - Average network in
      - Average network out
      So u can decide the better scaling policy for ur application
  These rules are easier to setup

Auto Scaling Custom Metric
  - We can auto scale based on a custom metric(ex: number of connected users) to do this we have to create a custom
  metric for application on ec2 and then send it to cloudwatch using the PutMetric API then create a cloud watch alarm
  to react to low/high values and then use these alarms as the scaling policy for ASG - @@@

 So conclusion is the ASG isn't tied to the metrics that only AWS exposes ,as it can be any metric u want and also custom metric - @@@

        ASG Brain dump
            - Scaling policies can be on CPU, Network and can even be on custom metrics or based on a schedule
            - ASG's use launch configuration and u update an ASG by providing a new launch configuration  - @@@
            - IAM roles attached to an ASG will get assigned to EC2 instances   - @@@
            - ASG is free, u pay for the resources under it being launched.  - @@@
            - Having instances under an ASG means that if they get terminated for whatever reason, the ASG will restart them means
              extra safety
            - ASG can terminate instances marked as unhealthy by load balancer and automatically replace them by new once  - @@@

        Now if we go to the DNS provided by the load balancer and refresh it again and again and check the hostname ip we will see
        that on each refresh a new hostname is disaplaying this is because the load balancer is actually spreading the load between
        the instances

  EBS Volumes-> An ec2 machine loses its root volume(main drive) when it is manually terminated and unexpected terminations of
  ec2 instances also happens time to time in which AWS will mail you that instance is terminated because of Hard ware fail or
  any other reason
  So sometimes u need a way to store ur instance data somewhere safe and so that even ur ec2 instance get terminated ur EBS
  volume lives by.
  So EBS volume is a network drive that u can attach to ur instances while they run and it allows ur instances to persist data - @@@

  - EBS Volume is a network drive(i.e not physical drive)
  - It uses to network to communicate the instances, which means there might be a bit of latency  - @@@
  - It can be detached from an EC2 instance and attached to another one quickly   - @@@
  - EBS volume is locked to a certain Availability Zone(AZ) //means if an EBS volume in us-east-1a cannot be attached to us-east-1b  - @@@
  - To move a volume across, you first need snapshot it.        - @@@
  - EBS volumes have a provisioned capacity(size in GBs and IOPS) //IOPs are IO operations per sec      - @@@
  - You are billed for all the provisioned capacity
  - You can increase the capacity of the drive over time in GBs and IOPS
  - We can attach multiple EBS volumes to the same instance  - @@@

EBS Volume Types-> There are 4 types of EBS volumes which are as follows   - @@@
    - GP2(SSD): General purpose SSD volume that balances price and performance for a wide variety of workloads
    - IO1(SSD): Highest performance SSD volume for mission-critical, low-latency or high-throughput workloads. they are expensive
    - ST1(HDD): Low cost HDD volume designed for frequently accesses, throughput-intensive workloads , it is good for big
                data applications
    - SC1(HDD): Lowest cost HDD volume designed for less frequently accessed workloads

     - EBS volumes are characterized is Size,Throughput,IOPS
     - Check AWS documnetion on EBS types for more information.
     - Generally we use GP2 volumes

   EBS Volume Resizing      - @@@
      - Feb 2017: You can resize the EBS volumes.
      - You can only increases the EBS volumes size(of any volume type),IOPS(only for IO1)
      - After the resizing an EBS volume, you need to repartition your drive on EC2 instance and then only ur EC2 instance will be
        leverage the full, new size of drive.

   EBS Snapshots -> EBS volumes can be backed up using "snapshots"
      - Snapshots only take the actual space of the blocks on the volume
      - So if u have a 100GB drive that only has 5GB of data, then ur EBS snapshot will only be 5GB    - @@@
      - Snapshots are used for:  - @@@
          - Backups: ensuring you can save your data in case of catastrope.
          - Volume migration such as resizing a volume down , change a volume type and encrypt a volume


   EBS Encryption -> When u create an encrypted EBS volume , you get the following  - @@@
      - Data at rest is encrypted inside the volume (means that it is encrypted when it is written in the volume)
      - All the data in flight moving between the instance and volume is encrypted
      - All the snapshots are encrypted
      - All volumes created from the snapshots are also encrypted
      - Encryption and Decryption are handled transparently(you have nothing to do)
      - Encryption has a minimal impact on latency
      - EBS Encryption leverages keys from KMS(AES-256)
      - If u copy an unencrypted snapshot you can start encrypting that snapshot

   EBS vs Instance Store ->
      - Some instances do not come with Root EBS volumes instead they comes with "Instance Store" which means the root volume
        attached to that instance is physical   -  @@@
      - Instance store is physically attached to the machine
      - Pros of Instance Store         - @@@
          - Better I/O performance
       - Cons   - @@@
          - On termination the instance store is lost
          - You can't resize the instance store
          - Backups must be operated by the user

           - Overall, EBS-backed instances should fit most applications workloads only people who need extremely high performance can
            use Instance store in AWS cloud

          EBS Brain Dump->                  - @@@
            - EBS can be attached to only one instance at a time
            - EBS are locked at the AZ level
            - Migrating an EBS volume acoss AZ means first backing it up(snapshot), then recreating it in the other AZ.
            - EBS backups use IO and you shouldn't run them while ur application is handling a lot of traffic
            - Root EBS Volumes of instances get terminated by default if the EC2 instance gets terminated(you can disable that)


#2-> AWS Route53 -> Route53 is a managed DNS(Domain name system)
    - DNS is a collection of rules and records which helps clients understand how to reach a server through URLs
    - In AWS , the most common records are   - @@@
        - A: URL to IPv4
        - AAAA: URL to IPv6
        - CNAME: URL to URL
        - Alias: URL to AWS resource.

   How the user connect to Application server for 'A' Record?
   The user on browser enter the DNS such as http://myapp.mydomain.com which redirects the user to the Route53 and Route53 knows the
   IP for that DNS so the route 53 send back the response and says okay it's A record so it's mapping a URL to a IP and sends back
   the IP of that DNS and then the browser makes a HTTP request using the same domain but this time it goes directly to the
   Application server and the application server sends back the HTTP response.
   Now the DNS  request don't happen at every request so they get cached so my browser will know that now myapp.mydomain.com has IP
   32.45.67.89 So basically the browser now goes directly to the applications server
   But it works in the same way for AAAA Record, CNAME Record and Alias Record
   And all these things don't have to be implemented by ur web browser or anything they just work out of the box

   Route53 can use public domain so u need to own them or buy them or private domains and you can just create whatever domain u want -@@@

   -These private domains can only be resolvable within ur VPC,So within ur Instances and as of public domain can be resolved by
    anyone such as Google account.      - @@@

 - Route 53 has advanced features such as ->     - @@@
      - Load balancing(through DNS- also called client load balancing)
      - Health checks(although they are limited)
      - Routing Policy such as simple, failover, geolocation, geoproximity ,latency, weighted. So u can route based on wherever ur
      server are located in the world or what is ur closest with minimal latency. We can also perform a weighted balancing saying
      I want this amount of traffic to go to this instance and this another amount of traffic to go to that other instance

Question -> You want to route traffic based on the location of your resources and, optionally, shift traffic from resources in one
location to resources in another.Which routing policy is suitable for your case ?
Answer -> Geoproximity          - @@@
//-Simple routing policy : Use for a single resource that performs a given function for your domain.
-Failover routing policy : Use when you want to configure active-passive failover.
-Geolocation routing policy : Use when you want to route traffic based on the location of your users.
-Geoproximity routing policy : Use when you want to route traffic based on the location of your resources and, optionally,
shift traffic from resources in one location to resources in another.
-Latency routing policy : Use when you have resources in multiple AWS Regions and you want to route traffic to the region that
 provides the best latency.
-Multivalue answer routing policy : Use when you want Route 53 to respond to DNS queries with up to eight healthy records selected
 at random.
-Weighted routing policy : Use to route traffic to multiple resources in proportions that you specify.

      - Prefer Alias over CNAME for AWS resource(for performance reasons)

    - Route53 Hands On
       - Go to Route53 dashboard and select DNS Management then Create a Hosted zone
       - then enter domain name and type
       - If u don't have domain name then go to 'Registered domains'
       - On the Hosted Zone the registerd domain will be their then click on the domain and then u will see two domains NS and SOA
       - Then go to load balancers and copy the load balancer u r going to use DNS name
       - then back on the Hosted Zones dashboard select 'Create Record set' then select the record type, Alias as yes and alias
         target which would contain the load balancer DNS
       - If we had select type as CNAME then the name would be two long such cnamepract.sadanand.com

 AWS RDS-> RDS stands for Relational Database Service and it is managed database service for DB use SQL as a query language - @@@
        - It allows u to  create databases in the cloud that are managed by AWS
        - Databases could be created are ->
                - Postgres
                - Oracle
                - MySQL
                - MariaDB
                - Microsoft SQL Server
                - Aurora(AWS Prorietary Database)    - @@@

     Advantage over using RDS versus deploying DB on EC2        - @@@
        - It is a managed Service
        - OS patching level
        - Continous backups and restore to specific timestamp(Point in time restore)
        - Monitoring Dashboards
        - Read replicas for improved read performance
        - Multi AZ setup for DR(Disater Recovery)
        - Maintenance Windows for upgrades
        - Scaling capability(vertical and horizontal)       - @@@

     Drawback -> you can't SSH into ur instances            - @@@

     RDS Read Replicas for read scalibilty -> So when u first set up RDS, U have one application or multiple instances of ur application
     and one database instance and all the read and writes go their ,but sometimes, after a while when u realize that ur application
     is to read a lot for ur database and so u need to increase ur read scalibility So then u can create read replicas

        - We can create upto 5 read replicas        - @@@
        - These read replicas can be within the same AZ or cross AZ or cross region     - @@@
        - So u can create an RDS instance replica at different places and to replicate the data between the master db instance and
          replica instance it is asynchronous   - @@@
        - So asynchronus means the reads will be eventually consistent.  - @@@
        - So when u writes to the master there will be a small latency, a small delay between when the write was happening between
          the master and when it will be replicated to the replicas.
        - The replicas themselves help with read capabilities, so ur application can now scale on reads  - @@@
        - If u want then the replicas could be promoted to their own database (It's a very quick and easy way to replicate a database
          and work with something on it's own)
        - Application must update the connection string to leverage read replicas       - @@@
        - So in read replicas their are two things to remember it is multiple instances and only one the master takes the writes and
          all the other ones takes the reads and second thing the replication is asynchronous

  RDS Multi AZ(Disaster Recovery) -> So Multi AZ is basically ur application , still ties to master database instance and this
  master DB will have a synchronous replication to on DB instance that is called Standby that will be in another AZ , So basically
  only one DNS name will be exposed to ur application and that DNS name will have automatic failover from the master to standby in
  case their is any issue  - @@@

  Why we need to do this? - @@@
  To increase availability, So if whole instance goes down or whole AZ goes down or network,instance,storage failure occurs, So
  basically our application can failover from the master to the standby and the standby will be parametered as master and for this
  their is no manual intervention that needs to happen in ur application.

  Why u need Multi AZ?
  For Disaster Recovery

  - Application still reads and writes to only one database and to make sure that u can failover seamlessly and the replication
  is synchronous   - @@@

  - It is not use for scaling   - @@@

  - Read replicas are use for read scaling and Multi AZ is used for disaster recovery and we can use combination of both - @@@

  RDS Backups -> Backups are automatically enabled in RDS                       - @@@
    - They are automated backups, so there's daily full snapshot of database        - @@@
    - Capture transaction logs in real time         - @@@
    - ability to restore to any point in time
    - All backs are retain for 7 days(can be increased to 35 days)          - @@@

    - U can also trigger manual DB snapshots and retention is as long as u want, so it could be used for long term backup or any
    other reason u can use database snapshots           - @@@

      RDS Encryption ->
        - Encryption is at rest capability with AWS KMS (AES-256) encryption,(u need to enable it using KMS)        - @@@
        - We can use SSL certificate to encrypt data to RDS in flight       - @@@
        - To enforce SSL:           - @@@
            - In PostgreSQL we can set one parameter in RDS console which is rds.force_ssl=1
            - In MySql within the DB u need to execute statement which is -> GRANT USAGE ON *.* TO 'mysqluser'@'% REQUIRE SSL
              once u run this statement basically no user will be able to connect to MySql unless on SSL connection

         - To connect using SSL:        - @@@
            - Provide the SSL Trust Cetificate(can be download from AWS)
            - Provide SSL options when connection to database

     Connecting using SSL doesn't mean that SSL is necessarily enforced     - @@@

        RDS Security ->
            - RDS Databases are usally deployed within a private subnet, not in a public one u don't expose them publicly.      - @@@
            - RDS Security works by leveraging Security Groups(the same concept of ec2)- it controls who can connect with RDS   - @@@
            - IAM policy help control who can manage AWS RDS.
            - Traditional Username and Password can be used to login to the database            - @@@
            - u can also integrate it with IAM users (for MySql/Aurora - New Feature)

    RDS vs Aurora       - @@@
        - Aurora is proprietary technology from AWS(not open sourced)
        - Postgres and MySql are both supported as Aurora DB (that means ur drives will work as if Aurora was a Postgres or MySql
        database)
        - Aurora is "AWS cloud optimized" and claims 5x performance improvement over the MySql on RDS, Over 3X performance of
        Postgres on RDS.        - @@@
        - Aurora storage automatically grows in increments of 10GB up to 64 TB      - @@@
        - Aurora can have 15 replicas while MySql has 5 ,and the replication process is faster(sub 10ms replica lag)    - @@@
        - Failover in Aurora is instantenous, it's High availability native so ur application  will failover directly - @@@
        - Aurora costs more then RDS (20% more) - but is more efficient so overall cost decrease in Aurora  - @@@
        - With Aurora DB , amazon is trying to push a technology that's going to be easily setup, bit more scalable and cloud friendly

AWS ElastiCache -> The same way RDS is to get managed Relational Databases the ElastiCache is to get the managed Redis or Memcached
                   Caches which are basically in memory databases, so they run on RAM and they have really high performance and usally
                   a very low latency   - @@@
        - The role of caches is to help reduce the load off of databases by caching data so they read intensive workloads ,So they read
          from the cache instead of database
        - Helps make ur applications stateless by storing state in a common cache       - @@@
        - It has Write Scaling capability using sharding        - @@@
        - It has Read Scaling capability using Read Replicas        - @@@
        - Multi AZ capability with Failover Capability, Just like RDS       - @@@
        - AWS takes care of OS maintenance/patching, optimizations, setup, configuration, monitoring, failure recovery and backups

        - So it is basically pretty much exact same thing like RDS, It's an RDS for caches and it's called ElastiCache

ElastiCache Solution Architecture - DB Cache
- Scenerio for ElastiCache for DB Cache -> Our application connects to RDS but we can put elasticache in between RDS and application, So
that way our application will basically first query with ElastiCache and if that query is not available then we will query it
from RDS and store it in ElastiCache and that's caled a 'CacheHit' when u get into ElastiCache from the application and it
works. So we have a application which do cache hit and we get data from ElastiCache and this way the retrival of data is super
fast and RDS does not see a thing.
When their is no query in the ElastiCache and the application needs to query from the RDS that is called 'Cache Miss',In the
application their should be programmed this way that after the cache miss and read data from the RDS the application should
write back to the cache the results in Elasticache so the next time their would be no cache miss for the same query.
So the caches this way relieve load on the RDS usally the read load.
- The cache must also come with the invalidation strategy to make sure only the most current data and the most relevant data
  is in ur cache

  ElastiCache Solution Architecture - User Session Store
  - Scenerio for ElastiCache for User Session Store -> user login to our application , application is stateless, which means
  their are bunch of applications running may be in ASG and so all of them need to know that user is logged in.
  So the process is like this the user logs in to the application and the application will write the session data into
  Elasticache, now if the user hits another application or another instance of application in ASG then that application needs
  to know that user is logged in or not and for this the application is going to retrive the session off of Elasticache and say
  "oh yes, it exists" so the user is logged in. and basically all the instances can retreive this data and make sure that
  user doesn't need to re-authenticate every time.

- So that's basically another common usage of solution architecture and patent Elaticache    - @@@
    - to relieve load of database
    - to share some states such as user session store into a common ground and all the applications can be stateless and
      read and write these sessions in real time.

        Elasticache can be of two technologies which are Redis and Memcached
        - Redis is more popular, it is an in-memory key-value store.        - @@@
        - It has super low latency(sub ms)
        - Cache survice reboots by default(it's called persistence) // so u don't lose data     - @@@
        - Great to host     - @@@
            - User Sessions
            - Leaderboard (for gaming) // because there is sorting capability
            - Distributed States
            - Relieve Pressure on databases (such as RDS)
            - Pub/Sub Capability for messaging
            - Mutli AZ with Automatic Failover for disaster recovery if u don't want to lose ur cache data
            - Support for Read Replicas

         Memcached -> It's an in memory object storage
            - the cache didn't survive the reboots          - @@@
            - Use Cases                                     - @@@
                - Quick Retrival of Objects from Memory
                - Cache often accessed objects
            - Overall Redis is more popular and has better feature sets than Memcached

         To Configure and setup elastic cache -> Go to Elastcache dashboard
            - then choose the cluster engine either Redis or Memcached      - @@@
            - Then fill other details and choose the cache size u want
            - U can also create a subnet group in the Advance settings      - @@@
            - We can also choose the Security Group and enable automatic backup     - @@@

       VPC -> Within a region u r able to create a VPCs(Vitual Private Cloud)
        - When u first create ur AWS account their is a default VPC     - @@@
        - Each VPC contains subnets and subnets are just networks
        - Each subnet must be mapped to an AZ
        - In a VPC their are two types of subnets Public Subnet and Private Subnet      - @@@
        - The public subnet is assessible through www and private subnet could not be.
        - It's common to have many subnets per AZ and many subnets overall in VPC       - @@@
        - In the public subnet usally contains -        - @@@
            - Load Balancers
            - Static Websites
            - Files
            - Public Authentication Layer
        - In the Private subnets usally contains -          - @@@
            - Web Application Server
            - Database etc

   Question -> What do I need to reach from my browser over the public internet versus what do I need to reach from my private
               network or what does only to be reach from the web, actually?
    Answer -> Our Web Application Servers are behind the load balancer so they don't need to be in the public subnet they could
              be in private subnet and talk directly to the load balancer only

          - Public and Private Subnets are able to talk together if they're in the same VPC     - @@@

        VPC Brain Dump ->
        - All new accounts come with Default VPC
        - It's Possible to use a VPN to connect to a VPC(and access all the private IP straight from ur laptop)  - @@@
        - VPC flow logs allow u to monitor the traffic within, in and out of ur VPC(useful for security, performance, audit)
        - VPC are per account per Region // So if u have five regions u need to have five different VPC and If u have five accounts all
          the VPCs are going to be different        - @@@
        - Subnets are per VPC per AZ // The subnets are only within VPC and the subnets are attached to one VPC only. - @@@
        - Some AWS resources can be deployed in VPC while others can't
        - U can Peer VPC(within or across accounts) to make it look like they're part of same network

 Web App 3-tier Architecture -> -@@@
 - User will first go to Route53 for the public IP and then it will go to ELB and that ELB will be in Public Subnet after ELB
   we have Auto Scaling Group and the ASG to be highly Available is split in multiple AZ and ASG spins up EC2 instances and ELB
   spreads out the load between these EC2 instances and ASG will decide for the scale out and scale in depending on High Traffic and
   low traffic and instances talk directly to database to do some operations in the RDS master and for being resilient to failures
   we setup cross-AZ replication to have a stand by replica , and for better read and writes we can also setup in Elaticache cluster.
   it could be Redis and this basically allows us to read stuff off the cache and cache data such as user sessions or data straight
   from RDS.
 - The ASG is in Private Subnet as it only talks to ELB
 - Elaticache and RDS they are in Data/Private Subnet // So u can create another subnet tier called the data subnets or u can
    reuse the private subnet
 - The user has only access to ELB
 
 Question-> Which RDS feature does not require us to change our SQL connection string ?
 Answer -> Multi AZ
 
 Note -> Route53 is a Global Service
 
 Question -> Our RDS database struggles to keep up with the demand of the users from our website.
 Our million users mostly read news, and we don't post news very often. Which solution is NOT adapted to this problem?
 Answer -> RDS Multi AZ
 
 Question -> You want to ensure your Redis cluster will always be available
 Answer -> Enable Multi AZ

#13 AWS S3->
      S3 Buckets-> S3 allows u to store objects(files) in "buckets"(directories)    v
        - Buckets must have a globally unique name  - @@@
        - Buckets are defined at the regional level - @@@
        - Naming Convention for buckets ->  - @@@
            - No Uppercase
            - No underscore
            - 3-63 characters long name
            - Not an IP
            - Must start with lowercase letter or number

         Objects -> Objects(files) have a key.  - @@@
            - The Key is a full path Ex-> <my_bucket>/my_file.txt and <my_bucket>/my_folder/another_folder/my_file.txt  - @@@
            - Their is no concept of Directories within buckets(although the UI will trick u to think so)   - @@@
            - It's okay for key name to have ('/') slashes. - @@@
            - Object values are the content of the file
                - Max Size u can have is 5TB for Single Object  - @@@
                - If uploading file that is more than 5GB then u must use multi-part upload otherwise u cannot upload it    - @@@
                - Objects come with metadata and metadata is whatever u want and it's a key value pair  - @@@
                - Tags also used to define ur objects u can have upto 10 tags and they are again key value pair - @@@
                - Tags are useful for the security and lifecycle.   - @@@
                - Their is Version ID(if versioning is enabled) - @@@

       S3 Versioning-> u have enable it at the bucket level.    - @@@
        - Versioning means if u overwrite a file u can increment it's version so when u first upload a file it is version 1
            and after overwriting it the file becomes of version 2 and so on (the version is a for a file is a string not as 1,2 and 3
            in real life it is just to show the working)    - @@@
        - It is best practice to version ur buckets because - @@@
            - It protect against unintended deletes(ability to restore a previous version)
            - Easy roll back to previous version

         - The only downside is that u use a bit more space on S3 bucket    - @@@
         - Any file that is not versioned prior to enable versioning will have version "null"   - @@@

         - To enable versioning go to the bucket and then in properties select Versioning and then Enable Versioning
         - every object or file version is kept into the bucket, if we hide the version we can see only the latest version of file
         - Now if u delete a version file it would be just marked as delete and new version id would be provided to it in other words
         the versioned file is not deleted completely - @@@

         S3 Encryption for Objects ->
         - There are 4 methods of encrypting the S3 Objects - @@@
            - SSE-S3: encrypt S3 objects using keys handled and managed by AWS
            - SSE-KMS: we use AWS KMS(key Management Service) for encryption keys
            - SSE-C: when u want to manage ur own encryption keys
            - Client Side Encryption

         SSE-S3 -> U don't actually see this encryption
            - Object is encrypted Server side
            - AES-256 algorithm encryption type
            - To make it work u must set a header: "x-amz-server-side-encryption":"AES256"  - @@@

            For Encrypt the Object with SSE-S3 -> First step to do is to transfer the object by making a HTTP OR HTTP/S request to S3
             and add the header is the request "x-amz-server-side-encryption":"AES256"
             the S3 is also going to create managed key and managed data key and add it to
            object and this will be managed by S3 , after all this process there will be some encryption and after encryption the
            data will be put into S3 bucket
            So encryption is happening here server side and it happens on the S3 side and S3 provides the encryption key.

          SSE-KMS -> It also do the encryption server side except this time the data key will be managed by KMS
            - The advantage of using KMS is that u get more control over the rotation of that key and u can get an audit trail about
                how key is used
            - The encryption is done in server side
            - To make it work u must set a header: "x-amz-server-side-encryption":"aws:kms" - @@@

            For Encrypt the Object with SSE-KMS -> First step to do is to transfer the object by making a HTTP OR HTTP/S request
            to S3 and add the header is the request "x-amz-server-side-encryption":"aws:kms"
             the S3 is also going to use the KMS Customer master key(CMK) and add it to
            object and this will be managed by S3 , after all this process there will be some encryption and after encryption the
            data will be put into S3 bucket

            Note -> Here key is not managed by us but it is managed by KMS
            Ex Question -> https://www.whizlabs.com/learn/course/quiz-result/196967 question 21

          SSE-C ->
            - The S3 will not store the encryption key u provide.   - @@@
            - HTTPS must be use in this case    - @@@
            - Encryption Key must be provided in HTTP headers for every HTTP request made.  - @@@

            For Encrypt the Object with SSE-C -> We provide and we generate a client side data key over HTTPs only because it has to
            be encrypted in a secure connection. We provide the object to transfer and the data key in the header now we have to put
            the object and client provided data key and provide both things to S3. After it S3 does the encryption between the object
            and the client provided data key then the s3 put the encrypted object into the bucket and throws away the client-provided
            data key

          Client Side Encryption ->
            - For this we need to use the library such as S3 encryption client just to make it a little bit easier. - @@@
            - Client must encrypt the data themselves before sending it to S3    - @@@
            - Client must decrypt the data themselves when retrieving from S3   - @@@
            - Customer fully manages the keys and encryption cycle.     - @@@

            For Encrypt the Object with Client Side Encryption -> Using the S3 encryption sdk on the object we will generate the
            client side data key and
             on the client side we will encrypt the object that encrypted object will be transfered to the S3 bucket over
            HTTP or HTTPs connection , So this way the encryption and decryption is done on the client side.

          Encryption in Transit(SSL) -> It is basically Amazon exposes HTTP endpoints for non-encrypted traffic
            - and HTTPs endpoints where u have encryption in flight meaning that the data being exchanged between two servers in
              encrypted in flight
            - u r free to use the endpoint u want, but HTTPs is recommended
            - For SSE-C u have to use HTTPS because u also transfer the data key over the network   - @@@
            - Encryption in flight is also called SSL/TLS.  - @@@

           Note - If we put the encryption in the bucket then all the files uploaded in the bucket will be encrypted    - @@@

     S3 Security -> Their are 2 types of security User Based and Resource Based
        User Based ->
            - IAM Policies -> With IAM we can restrict what the user can do against S3 , which api calls should be allowed for a
            specific user from IAM console.

        Resource Based -> It is more popular to handle Amazon S3 Security.  - @@@
            - Bucket Policies -> bucket wide rules from s3 console and u can use it to allow cross account  - @@@
            - Object Access Control list(ACL) - finer grain // this is less popular
            - Bucket Access Control List(ACL) - these are less common

        Bucket Policies -> It is a JSON based policy, 4 major elements in bucket policies are   - @@@
            - Resources - which is buckets and objects the policy will apply to.
            - Actions - the set of api to allow or deny
            - Effect - to allow or deny
            - Principal - the account or user to apply the policy to.

        - Use the S3 bucket policy to   - @@@
            - Grant public access to the bucket
            - Force objects to encrypted at upload
            - Grant access to another account(cross account)

        S3 Security Other Things ->
        - Networking - S3 supports VPC endpoints for instances in VPC without www internet access and could be talking to S3 directly
         through the internal Network.  - @@@
        - Logging and Audit features -  - @@@
           - the s3 access logs could be store in other s3 bucket // u should not store own access logs in the same bucket
                 otherwise their will be some recursion and u will end u with 50 - 100 GB of files.
           - API calls can be logged in AWS CloudTrail. // cloudtrail basically allows u to audit ur entire API call stack in AWS.
        - User Security -
            - Enable MFA(multi factor authentication) - to make sure that the users need to provide a certain ticket if they
                want to delete an object in a version bucket
            - Signed Urls - these URLs are valid only for a limited time(ex-> premium video service for logged in Users)    - @@@

      Question -> How can we grant access to one user for a little bit time and expire it?  - @@@
      Answer -> Use S3 signed URLs

      Question -> How to ensure that the objects can be encrypted at upload?    - @@@
      Answer -> In Permissions of bucket go to Bucket Policy u can use the the documentation to create ur own JSON policy or use policy
      generator, In the policy Generator select the type of policy in this case select "S3 Bucket Policy"
      Scenerio -> We want to deny any request that  does not contain the header for encryption and other we deny any request that does
      not contain the right value for the header for the encryption
      Steps for the Scenerio -> - @@@
        - Select Effect as Deny
        - Enter Principal as "*" as it is going to apply to everyone
        - Enter the AWS Service as S3
        - Select action as PutObject
        - Paste the S3 Bucket ARN/* // the "/*" always remember to put /* after the ARN
        - In the Add Conditions selct the Condition as Null and selct the key AMZ server-side encryption // which is the header
            for encryption on server side
       - and enter the "Value" as true
            // what above conditions says is "if the AMZ server-side encrytion header is null then we deny it"
       - then click on ADD Statement
       - and again do the above all steps for the second condition for not containing the right value for the header encrytpion till
       the add conditions step
       - In the add conditions selct the condition as "Stringnotequals", selct the key as AMZ server-side encryption and then
        add the condition and then add the statement
       - After all the required statements are added click on generate policy and it will create the JSON for Us
       - Copy the JSON and paste it in the Bucket Policy
       - Now at the time of Uploading a Object I we do not select the encryption option as Amazon S3 master-key the uploading of the
       object will fail or if we had selcted the encryption as Amazon KMS master-key then also the the uplaod would have failed.

       S3 Websites ->
        - S3 can host static websites and have them accessible on the www   - @@@
        - The website URL will be of two forms
            - <bucket-name>.s3-website-<AWS-region>.amazonaws.com
            - <bucket-name>.s3-website.<AWS-region>.amazonaws.com   // the difference between both is that in above url their is -
                // in between s3-website and <AWS-region> and in the second url their is . between them
            - If u get a 403(Forbidden) error, make sure the bucket policy allows public reads.     - @@@
@@@         - Sometimes when u change the bucket policy the result isn't instantenous , it's a little time to change policy for bucket.

             Setup the website on S3 -> Go to bucket Properties and then to Static website hosting, selct the optin use this bucket
             to host a website and then enter the index document name and error document name and select the link it provided

             on going the link u will get the 403 access denied error because we need to add the bucket policy for write permissions

             So go to bucket policy then to policy generator selct effect as 'Allow' , selct AWS service as "Amazon S3" select Actions
             as 'GetObject' and then Paste the S3 Bucket ARN/* ,click on add statements and then click on Generate Policy, then add
             the policy to the bucket policy.

             Now if u go to the url the url willnot give the 403 error as the bucket is now public.

       S3 CORS ->
            - If u request data from another S3 bucket, you need to enable CORS.
            - CORS stands for (Cross Origin Resource Sharing) which basically allows u to limit the number of websites that can request
              ur files in S3(and limit ur costs).     - @@@
- Scenerio -> We have two buckets "mybucket" and "myimagebucket" in mybucket their are staic websites and in myimagebucket their
are images, So client connects to the mybucket , by doing a GET on index.html page and then the bucket replies it has index.html
so now if in the index.html their is usage of one image coffe.jpg which is in another s3 bucket which is myimagebucket, so now
the client or we can say the webbrowser like chrome will make a request for the coffee.jpg and client will also show it's
origin, So this way the client will also make a GET request for coffee.jpg and tell it's origion such as http://mybucket.s3-website...
, now the second bucket myimagebucket will verify whether the provided origion is allowed access or not this verified by CORS
that's why u need to enable the CORS in ur bucket and mentioned the origions which should be allowed , if the CORS are not approved
the client will get 404 error
            
            
    AWS S3 Consistency Model -> S3 take a bit time to replicate and to be active because S3 is a global service and eventually consistent.
      Their are two types of consistency u get from s3 ->   - @@@
        1 -> read after write consistency for PUTS of new object - So when we use the PUT object API and the object does not exist before
        we will get the read after write consistency so that means as soon as the object is written we can retrieve it .
        So when we get status 200 on PUT that means it's OK same way we get 200 on GET which means ok   - @@@

        This is true only if u didn't go a GET before to see if the object existed, Scenerio -> So if in a application if u do a GET to
        see if an object exists , u get 404 and then u do PUT because the object doesn't exists and then u do GET again u may got 404
        again because u are eventually consistent, U get 404 this time because of the eventually consistent as ur previous result
        was caged which was 404 and so u need to wait a little bit to GET 200 status

        2-> Eventual Consistency for DELETES and PUTS of existing objects - Mean if u read an object after updating it u might get the
        older version   - @@@
        Scenerio -> So if we do PUT and get 200 and then we do another PUT and get 200 for the new version of the object and then we do
        GET we will get 200 back but it might be for the first version of object not for the second one that's because it's eventually
        consistent and also if u delete an object u might still retrive it for a shot time afterwards because of eventual consistent,
        which means u might get 200 on GET of Object even when u should get 404

        So base points - for the new objects as soon as u can write it u can retrieve it and if u override an object or delete an object
        u will get the eventual consistency which means u will get the older version or an object after deleted - @@@

    AWS S3 Performance ->
      - Before july,2018 , When u had over 100 TPS(transactions per second) the s3 performance could Degrade    - @@@
      - Behind the scenes each object when u upload to s3 goes to s3 partition and for the best performance, u want the highest partition
      distribution (Each partition is basically a server and so more server involved the highest throughput u get)   - @@@
      - In exam , it was recommended to have random characters such as <my_bucket>/a12d_my_folder/my_file1.txt which basically means
      u can only have 4 random characters before ur key such as in this ex <my_bucket>/a12d_my_folder/my_file1.txt is a12d and this
      basically forces amazon to partition ur data correctly and put it on different partitions - @@@
           - Never use date as prefix keys <my_bucket>/2018_09_09_my_folder/my_file1.txt and <my_bucket>/2018_09_10_my_folder/my_file1.txt
            the partition will be very similar, so improve the performance in s3 it is recommended to include 4 random characters to
            improve performance - @@@
      - For faster upload of large objects (>5gb), use multipart upload.    - @@@
          - this gives parallelize PUTs for the greater throughput
          - maximize ur network bandwidth
          - decrease time to retry in case a part fails

 @@@    - So if u upload 5 tg of file in only one part and the whole thing fails then u have to reupload 5 tg file again but when u
          use multipart upload u basically upload small parts one by one and if the part fails u just only need to reupload that part

    - If u want to do reads a lot around the s3 bucket then use cloudfront that is used to cache s3 objects for u.  - @@@
    - And in case u want to upload files far away to ur s3 bucket such as u have bucket in Europe but wants to upload a file from
    Australia u can use S3 "Transfer Acceleration" which basically uses edge location which means u write locally to server in Australia
    and that server in Australia on the Amazon network will transfer that file automatically to Europe (no code changes are needed for
    this u just on;y need to change the endpoint u write to)    - @@@
    - When u use KMS encryption u may see a performance decrease and the reason is when u use KMS, u start hitting the AWS service limits
      for KMS which basically allows hundreds or thousands downloads/uploads per second

          Question -> Should u use randomized prefix to improve performance?
          Answer -> Yes
         
          - S3 Current Performance (Not yet in exam)
            - As of 17 July 2018, we can scale up to 3500 RPS for PUT and 5500 RPS for GET for EACH PPRFIX.
            - which means u really don't need to randomize objects prefixes anymore to achieve faster performance
            
#14-> - Developing and Performing AWS tasks against AWS can be done in following ways ->
            - Using the AWS CLI on our local computer
            - Using the AWS CLI on ur EC2 machines
            - Using the AWS SDK on our local computer
            - Using the AWS SDK on our EC2 machines
            - Using the AWS Instance Metadata Service for EC2

 AWS CLI->
    - Download the msi installer for the AWS ClI according to ur system needs
    
    - AWS CLI Configuration- ur computer access AWS Network in ur Account using the cli interface over the www after checking the permissions
    and credentials
    - Go to IAM dashboard click on ur user and then go to it's security credentials , then click on 'Create access key' and u got secret
    access key only one time u can not view it or recover it later
    - go to cmd prmnt and type cmd -> aws configure // after this cmd enter access and secret keys  - @@@
      // this aws configure cmd create some files in small folder called .aws , to check whether this file is created in windows
      // use cmd -> dir "%UserProfile%\.aws" In linux use cmd-> ls ~/.aws
    - In .aws folder two files are created config and credentials, if u open the config file it tells the by-default region and if we
      open other file the credentials one we can see the access key and secret key    - @@@

    AWS CLI EC2->
    - we can use cmd -> aws configure in ec2 and boot ur credentials but that is super insecure , so never put ur credentials on ec2 
    machine - @@@
    - So the right way to do it is attach IAM role to EC2 and that IAM role will come with policy so basically we can define exactly what
    the ec2 instance should be able to do and by default it has no rights   - @@@
    - So ec2 instances can use these profiles automatically without any additional configuration when we attach a IAM role to ec2   - @@@
    - This is the best practice in AWS
    - but if u go on ur ec2 instance terminal and do cmd->aws s3 ls u will see it is asking u for to do aws configure so to resolve this
    attach IAM role to ur ec2 instance
    - Steps for attaching IAM role to ec2
      - Go to ur ec2 instance dashboard connect to that ec2 using the ssh method and if u are using Linux 2 cli u can use aws cli directly
      without installing it
      - GO to ur IAM dashboard and then to roles select the AWS service (basically roles in AWS are used so that u can have any service have
      it's own set of permissions) such as EC2 and then click on Next and then attach policy such as s3 read policy to the role and then
      complete other steps to create roles
      - Go to ur ec2 dashboard right click on instance then go to instance settings then select "Attach/Replace IAM Role" and then attach
      ur created role to t he instance
      - Now if we go on ur ec2 instance terminal and do cmd->aws s3 ls and now the s3 list is shown
      - If u apply policy to a role which is attached to ec2 instance it will take little time to fully made that policy applicable on
      the ec2 instance.
      
      IAM Policy ->
      - Go to Policy Dashboard U will see predefined managed policies and u can create ur own policy in which u can define Service , actions,
      Resources and Request Conditions
      - U can also add ur inline policy in ur policy(Inline policies are basically policies that u are going to added inline means on the top
      of whatever you have already chosen)     - @@@
      - These inline policies are not possible to be added to other roles, so this like saying that this policy is just for that role  - @@@
      - It better to use managed policies globally over inline policies, so u can get better management view - @@@
      
      {
        "Version": "2012-10-17",
        "Statement":[
          {
            "Effect": "Allow", // this means we are allowed to Perform actions which is Get something or List Something on Resource *
            "Action": [
              "s3:GET*", // it means u r able to perform API calls that starts with name GET Something
              "s3:List*"
            ],
            "Resources": "*"
        }
       ]
      }     - @@@
       
       - AmazonS3FullAccess - @@@
      
      {
        "Version": "2012-10-17",
        "Statement":[
          {
            "Effect": "Allow",
            "Action": "s3:*", // this means any API column s3 is allowed
            "Resources": "*"
        }
       ]
      }
      
      -> To create a IAM policy from the Visual Editor - @@@
          - Select the AWS Service U are working with such as S3
          - then Select the actions to be allowed on S3
          - then choose resources in which u have to select between either from all buckets or all resources , if we choose 'all resources'
          option then it will we represented as "*" in JSON of policy if we choose the option specific then we need to add the arn of that
          particular resource bucket
          - then u can also add the request conditions
          - and then we can attach this custom policy to our roles.
          
          - We can also use the Policy Generator same like in S3 to create custom policies
          - advantages of creating policies here in policy dashboard is that u can see who is using it and the version so that u can add
          version to that policy , to make sure that u can role back to previous permissions and policy version.
   
   AWS Policy Simulator ->
      - Google aws policy simulator and then navigate to the link for policy simulator
      - on the Simulator Dashboard select ur User, Group or Role and then select the one which u wants to simulate
      - On the simulator select the aws services and the permissions in policy for which u wanna test it and after selecting them click
      on RUN Simulator
      - and then it will shown results and effects accordingly for those permissions    - @@@

   AWS CLI dry runs ->
      - To test the permissions only and not actually run the command
      - Suppose u wants to test that whether the creation of ec2 instance is possible from the cmd line or not but the testing this will
      be expensive so for that we can use '--dry-run' option to simulate API calls (but not all commands contain --dry-run option)  - @@@
      
      Ex -> Run command on the ec2 instance terminal as -> aws ec2 run-instances help // to check whether following command contains
      the --dry-run option or not if it contains then execute
      - cmd -> aws ec2 run-instances --dry-run --image-id ami_id_of_instance --instance-type t2.micro
      - if the  permissions are not their on ec2 instance for above cmd we will get an error as "Unauthorized Operation"
      - and if the permissions are their on ec2 instance for above cmd it will still give error but with a msg that "Request would have
      succeeded, but DryRun flag is set"
      
   AWS CLI STS Decode Errors->
      - When u run API calls and they fail, u can get a long error message that doesn't mean must and we won't be able to decode it
      - This error msg can be decoded using STS command line    - @@@
      - So u need run this sts decode-authorization-message to decode msgs  - @@@
      - Steps ->
            - Google "aws sts decode-authorization-message" and go to link
            - On instance terminal cmd-> aws sts decode-authorization-message --encoded-message encodede_msg_paste_here
            - on executing above command u will get the proper and unterstandible msg
            - we can also add the policy to the ec2 instance role choose service as "STS" and selct DecodeAuthorizationMessage and then just
            review the policy and save the changes
            - Now if u run the above cmd-> aws sts decode-authorization-message --encoded-message encodede_msg_paste_here and execute it
            - u will get now a different un coded msg 
            - echo the_decoded_msg_come_from_above_cmd_execution
            - it will make the format of the msg more readable which is in JSON format , so this msg is more detailed and readable
            
       AWS EC2 Instance Metadata -> It allows EC2 Instances to "learn about themselves" without using an IAM Role for that purpose - @@@
          - The URL is http://169.254.169.254./latest/meta-data/ // this IP is internal IP of AWS so it will not work from ur computer, it
            will only work from EC2 instances - @@@
          - And using this u can retrieve the IAM role name from Metadata, but u cannot retrieve the IAM Policy - @@@
          - The only way to test the IAM policy is through the policy simulator or from the dry-run option  - @@@
          - Metadata is the Info about the EC2 instance - @@@
          - Userdata is to launch a script  - @@@
          
   Question -> When I run the CLI on my EC2 Instances, the CLI uses the ______ service to get _____ credentials
   thanks to the IAM Role that's attached.  - @@@
   Answer -> metadata | temporary
          
          Steps->
            - Go to the instance terminal then cmd -> curl http://169.254.169.254
            - and then we get the buch of numbers and dates this is basically the version of the API curl that we are using
            - then cmd -> curl http://169.254.169.254/latest/
            - We will get three different fields which are "dynamic", "meta-data" and "user-data"
            - then cmd -> curl http://169.254.169.254/latest/meta-data/
            - From above cmd we get bunch of different options such as AMI id, hostname , iam/ etc ;when ever it ends with / such as
            'iam/' that means their is more to it and if it doesn't ends with / then it means it is a value
            - So if we do cmd -> curl http://169.254.169.254/latest/meta-data/instance-id we will get the instance id
            - Any EC2 instance without an IAM role can request all this information , we do not need IAM role permissions for this
            - If we do cmd-> curl http://169.254.169.254/latest/meta-data/iam/security-credentials we can get the Access Key Id , secret
            access key and token
            - So behind the scenes when u attach an IAM role to an EC2 instance, the way for it to perform API goals is that it queries
            the whole URL(http://169.254.169.254/latest/meta-data/iam/security-credentials) and it turnsout this is a shot lived credential
            which also shows the Expiration date with access key, secret key and other details
            - This happens because ur EC2 instance get temporary credentials through the IAM role that it got attached to.
            
       AWS SDK(Software Development Kit) -> So if u wants to perform the actions on AWS but not using the cli and just from ur code then
       u can use aws-sdk    - @@@
          - Their are official sdk and unofficial sdks
          - The official sdks is for Java, .Net ,node.js, PHP,Python, GO ,Ruby, C++
          - boto3 and botocore that's an alternative name for python sdk.
          - SDK is very useful when we are coding against AWS services such as DynamoDB
          - Actually the cli itself uses the Python sdk(boto3) , so aws cli is a wrapper around python sdk  - @@@
          - Exam expects u to know when u should use SDK
          - If u don't specify or configure the default region, then us-east-1 will be choosen by default   - @@@

  AWS SDK Credentials Security ->
    - It's recommended to use the default credentials provider chain , this basically means it works with AWS credentials
    at ~/.aws/credentials when u run aws configure , so it means ur sdk will automatically looks for that file and use the credentials from there.
    - If u use EC2 machine and use IAM roles then u can use an Instance Profile Credentials and then sdk will automatically look for these
    credentials - @@@
    - It is less recommended but still u can use environment variables these AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY , so these
    environment variables still work with SDK (this technique is not recommended)   - @@@
@@@ - So never store aws credentials in ur code and ur code should extract credentials like above methods or the instance profile credentials

  Exponential BackOff SDK-> So when u r using a API call and it fails because it been called too many times across too many applications
  of ur then u go in to strategy called Exponential Backoff - @@@
      - It is only for rate limited API - @@@
      - So SDK usally implements the retry mechanism with Exponential Backoff   - @@@
    Scenerio for Exponential Backoff -> So basically ur first API call after failure will wait maybe for 10 milliseconds and ur second
    api call will run after 20 milliseconds and the third api call will be done with double time so it will run after 40 ms.    - @@@

    So the Exponential Backoff means that if ur API calls still keep on failing we will wait twice as long as the previous API call
    to try again and that over sures that u don't overload the API by trying it every millisecond    - @@@
    - Exponential Backoff is included in most SDKs
        
      AWS CLI Profiles ->
        - if u go to the credentials file or config file in .aws folder and do 
        - cat credentials u will see their is default sections which contains accesskey and secret key
        - So now if I have mutliple aws accounts and I wants to use it here so the we  use 
          cmd->aws configure --profile enter_any_name in instance terminal  - @@@
        - Now u will see the aws access key ID is none otherwise it will show some definately hidden key with it
        - So it means we have now an entirely new different account and hten enter ur new accounts access key and secret key
        - So now we go to the credentials file u will see the new credentials are also added into the file with the default one
        - So now we can switch between different accounts
        - If we do cmd-> aws s3 ls // this will run against the default credentials 
        - but if we use cmd-> aws s3 ls --profile profile_name // now it will run against the defined profile credentials
        
  Question -> I have an on-premise personal server that I'd like to use to perform AWS API calls?   - @@@
  Anwser -> I should run aws configure and put my credentials there, Invalidate them when I am done
  
  Question -> I'd like to deploy an application to an on-premise server. The server needs to perform API calls to Amazon S3. 
  Amongst the following options, the best security I can achieve is... ?    - @@@
  Answer-> create an IAM user for the application and put the credentials into environment variables
  
  Question -> Can EC2 Instances retrieve the IAM Role policy JSON document that's attached to them using the CLI without any role attached?
  Answer -> No //you can retrieve the role name attached to your EC2 instance using the metadata service but not the policy itself  - @@@


        
#16-> AWS ElasticBeanStalk -> ElasticBeanStalk is a developer centric view of deploying an application on AWS
      - BeanStalk is free but u have to pay for the underlying instances    - @@@
      - ElasticBeanStalk is a managed service means - @@@
          - the instance configuration or OS will be handled by ElasticBeanStalk.
          - the deployment strategy, we can configure it, but again it will be performed by ElasticBeanStalk
          
       - Just the application code is developer responsibility
       - Their are 3 architecture models for ElasticBeanStalk   - @@@
          - Single instance deployment: good for dev
          - LB + ASG: great for production or pre-production web applications
          - ASG only: great for non-web apps in production(workers, etc..) or other kind of models that don't need a load balancer, or
          don't need to be accessible.  - @@@
          
        - ElasticBeanStalk has 3 components - @@@
          - Application
          - Application version: each deployment gets assigned a version (every time u upload a new code u will get an application version
          and environment names)
          - Environment name(dev,test,prod...) //u can have as many environments as u like and u can name ur environments as u like
        
        - u deploy applications versions to environments and can promote application versions to next environment   - @@@
        - their is also Rollback feature to previous application version    - @@@
        - We get full control over the lifecycle of environments    - @@@
        
        Scenerio -> U create an application and u create an environment or multiple environments then u are going to upload a version and
        u are going to give it an alias name as u want and then this alias, u will release to the environment   - @@@
        
        Question -> What can we deploy on ElasticBeanStalk? - @@@
        Answer -> GO, Java SE, Java with Tomcat, .NET on windows Server with IIS, Node.js, PHP, Python, Ruby,Packer Builder,
                  Single Container Docker, Multicontainer Docker, Preconfigured Docker
                  If ur platform is not supported then u can write ur custom platform   - @@@
                  
        ElasticBeanStalk First Environment Setup ->
            - GO to ElasticBeanStalk Dashboard and then selct the sample application or upload ur code then click on
              create application
            - After this a msg in terminal of ElasticBeanStalk webpage will say that ur application will be using an S3 bucket
            - So u can go to S3 and check that a new bucket would have been created
            - So whenever u upload some code or whenever u deploy some stuff to ur ElasticBeanStalk then it will create a bucket for u.
            - After some time a u will get the msg of OK on ur ElasticBeanStalk Dashboard means the health is working
            - u will also see the bunch of Events are also their u can go to Events Dshboard on ElasticBeanStalk to explore them more
            - On the Events Dashboard u will see that the all events occured in the creation of ElasticBeanStalk Environment are listed
            their such as creation of security group  , creation of bucket, creation of ec2 instance etc all r listed their
            - On the ElasticBeanStalk dashboard after all the process one url will be provided to u on which u can go and see u deployed
            ElasticBeanStalk application asit is running on a dedicated environment in the AWS Cloud
            - The reason we have only one ec2 instance with an elastic IP because we are running in the simplest possible mode which is
            Developer Mode and in development mode u just get basically one instance and if we update ur application the Elastic IP will
            move across instances
            - U can go to the Logs Dashboard of ElasticBeanStalk to see what happens on the ec2 machine when the application was deployed
            - U can also go to the Health Dashboard of ElasticBeanStalk to see the monotoring of the health checks , we get the info about
            the CPU utilization,latency , average load and other parameters information for the health of the application
            - U can go to on level app into ur ElasticBeanStalk application from the breadcrums section here u can know about ur
            environment and application version , their we can do the create of new environment, restore terminated environments, swap
            environments URL and delte a whole application
            
            So we get the idea of that we can create many applications ,each application have many environments and can be applied to an
            application version.
            
            ElasticBeanStalk Second Environment Setup-> 
              - GO to ur application and then selct the option to create a new environment
              - Choose the environment type u want either Web Server Environement(To run a website, web application or web API that serves
              HTTP requests) or Worker Environment (To run a worker application that processes long-running workloads on demand or performs
              tasks on a schedule)
              - Then on next step enter the environment name, choose domain and it's availability, choose the platform for ur environment
              and then upload ur application code.
              - Then go to the Configure more options and then choose the configuration options as u needed such as capacity , instances,
              load balancer , security etc which are dependent on the type of "Configure presets" u choose such as Low cost,High Availability
              ,Custom Configuration and u can still chnage the parameters such as capacity, instances etc after choosing the the Configure
              presets but their is certain limit for each parameter
              - Then after all the configurations click on Create environment
              - U can check in AWS services such as ALB, Security Groups etc which are creted by this ElasticBeanStalk environment for u.
              - ANd if u go to ur ElasticBeanStalk application dashboard and see that two environments are their now
              
              - This second deployment is going to be more complicated because it has now load balancer, ASG and more instances so it will
              take a little more time for deployment    - @@@
              
@@@           - If u have a RDS database and u delete the application or environment from ElasticBeanStalk the database will be lost too
              So it's better to externalize an RDS database so that it won't be deleted if u delete the ElasticBeanStalk environments.
              
     ElasticBeanStalk Deployment Mode-> - @@@
        - Single Instance deployment is great for development because basically we get one ec2 instance with one elastic ip and one ASG
        and all of this is one AZ , so the DNS name maps straight to elastic IP
        - Second Environment Setup which is done in above steps could have high availability with load balancer or without LB and it's great
        for production type of deployments
        In this we have ASG,span across multiple AZ, in each AZ we r going to get one or more ec2 instances and each instance with
        it's own security group, RDS communication is available and we also have ELB

        Different Kinds of Deployment ->    - @@@(with Scenerios also as mentioned)
          - All at once(deploy all in one go) -> this is the fastest kind of deployment but the instances won't be available to serve
            traffic for a bit, so u will get downtime
          - Rolling Update -> then it will update a few instances at a time(buckets) and then move on to next bucket once the first bucket
            is healthy and updated.
          - Rolling with additional batches -> this like rolling but spins up new instances to move the batch(so that the old application
            is still available and always operating at full capacity)   - @@@
          - Immutable deployments -> where u spin up new instances in a new ASG and u deploy the version updates to these instances and when
            everything is ready then u will swap all the instances when everything is healthy

          Scenerio for All at once deployment -> we have 4 ec2 instances all having version 1 so first ElasticBeanStalk will stop all
          our EC2 instances and then we will be running v2 version of ec2 instances because ElasticBeanStalk will deploy v2 version
          to these instances, during the time of stopping the instance and the deploying of version 2 they will not serve any traffic
          So it is good for quick iterations and development environment and when u want to deploy ur code fast and u don't care about
          the downtime
          Their is no additional cost for this setup    - @@@

          Scenerio for Rolling update deployment -> the application will be running on low capacity which is called the bucket size
          and we can set the bucket size    - @@@
          So suppose we have 4 ec2 instances running on v1 version so the first two instances will stop but we still have 2 instances
          running on v1 so we have half capacity during this time and the first two instances which were stopped will be updated
          to v2 and then we roll on to next batch(next two instances on v1) that's why we call it rolling and then the next two
          instances will be updated to v2 and at the end we will have all the ec2 instances that will be updated to run the v2 application
          code. So we can see the application during the deployment will be running both the versions simultaneously
          There is no additional cost   - @@@
          So if u have small bucket size and many ec2 instances it may take long deployment - @@@
          In this scenerio we have two buckets and four instances.  - @@@

          Scenerio for Rolling with additional batches -> In this case the application is not running under capacity like in Rolling Update
          , So in this mode we run at full capacity and also set the bucket size    - @@@
          Application will be running on both the versions simultaneously but with small additional cost - @@@
          Additional batch is removed at the end of the deployment  - @@@
          Deployment is going to be long   - @@@
          It's good for production

          We have 4 v1 ec2 instances so we are going to deploy two new ec2 instances of v2 on it , So we have now 6 instances and
          the additional two will already be running the newer version , now we take first batch the first two instances and the
          application get stopped and those to instances will be updated to new version and then again the same way like in rolling
          update the other two instances will be get stopped and updated to new version of instances.
          So now we have 6 ec2 instances running on v2 and at the end the additional batch the additional ec2 instances joined in
          the starting will be terminated and taken away

          So this way we are always running at capacity and some time u run at over capacity and that's why their is some additional
          cost for the additional batch - @@@

          Scenerio for Immutable deployment -> This deployment is also Zero downtime    - @@@
@@@       - In this new code is going to be deployed to new instances and before this in other deployments it was on Previous instances
@@@       - These new instances comes from temporary ASG
@@@       - There is high cost because u double the capacity because you get the full new ASG and it's the longest kind of deployements
@@@       - U get very quick rollback in case of failures, u just have to terminate actuaaly not u ElasticBeanStalk will terminate the
          new ASG
@@@       - It's a great choice for Production if u r willing to take a little more cost

      So we have current ASG with 3 applications v1 running on 3 instances and then new temporary ASG being created. At first ElasticBeanStalk
      will launch one instance on it of version v2 to check if it works and if it passes all the health checks  it's going to launch all the
      remaining instances of v2 So now we have New ASG with 3 instances of v2. When it's ok to proceed it's going to merge the ASG with
      temporary ASG so now the current ASG would have 6 instances and when all is done and temporary ASG is empty , in the current ASG all
      the v1 applications will be terminated and the temporary ASG will be removed  - @@@

          Blue/Green -> It is not a direct feature of ElasticBeanStalk  - @@@
            - It has zero downtime and it helps with a release facility and allows for testing  - @@@
@@@         - in this u create a new stage environment which is just another ElasticBeanStalk environment and u deploy ur new v2 their
@@@         - So before all the deployment strategies were within the same environment and here we create a new environment
@@@         - So the new environment(green) can be validated independently in our own time and roll back if issues
@@@         - We use something like Route53 to prevent the traffic being going into two directions so we can setup weighted policies
              to redirect a little bit of traffic to the stage environment so we can test it
            - Using the ElasticBeanStalk console u can swap URL when done with the test environment
@@@         - It's not a direct feature and it's manual to do
@@@         - It's not embedded in to ElasticBeanStalk

          Go to AWS doc for ElasticBeanStalk to explore more about the Deployment modes

              ElasticBeanStalk Deployement Mode Options Hands On ->
                - Go to the ElasticBeanStalk dashboard Selct the ElasticBeanStalk Second Environment from the above setuped example
                - To the the Configuration Dshboard from the ElasticBeanStalk
                - If u look in to the list We will see the the block for Rolling Updates and deployements and then click on its modify
                - So by the default Deployement Policies is "All at once" and u can choose the Deployement Mode or Policy according to ur
                need .
                - Suppose we choose Rolling,Rolling with additional batch then we have to choose the "Batch Size" which we can choose in 
                Percentage or the Number of Instances and bigger the bacth size more the cost
                - If we choose the deployement mode as immutable then we don't get the option for batch size because we are getting a new 
                ASG created
                - We also get the option how the Configuration updates happen out of the 4 deployement modes such as the update should
                happen in the Rolling , Immutable etc
                - When we have to choose the options for health checks
                - After selcting the immutable deployement policy click on Apply.
                - And u will see in the events how the update of the environment is going on.
                - Now if u Upload and deploy the new code u are agin asked the options regarding the Deployement Policy to be changed
                - Then it will do the health check and u will see the health application is put to Gray ,Gray color is for when things
                are running in a undetermined state
                
              ElasticBeanStalk Extentions(Advance Concepts)->   - @@@
                - When u wants to deploy ur code to ElasticBeanStalk u need to zip it   - @@@
                - All the parameters set in the UI for deployment can also be configured with code, using some files and it is done
                by ElasticBeanStalk Extention Files - @@@
                      - These Files ust be in Directory called .ebextension/ in the root of ur source code  - @@@
                      - The format of files within that directory must be in YAML or JSON format    - @@@
                      - The files must end with .config extension(Ex -> logging.config) // U can name file whatever u want  - @@@
                      - U are able to modify some default settings using: option_settings parameter     - @@@
                      - Use case of doing extensions is to add o functionalities to ElasticBeanStalk such as modify Configurations for
                        deployment or to add even extra resources such as RDS databases, ElasticCache, DynamoDB etc - @@@
                    So u can add whatever u need and customerize ElasticBeanStalk according to ur needs
                 - All the resources managed by .ebexensions file get deleted if the environment gets deleted   - @@@
               
             ElasticBeanStalk CLI -> We can install an additional cli called "EB cli" which takes the working with ElasticBeanStalk from CLI
             easier
                  Basic cmds->
                    - eb create
                    - eb status
                    - eb health
                    - eb events
                    - eb logs
                    - eb open
                    - eb deploy
                    - eb config
                    - eb terminate
            - ElasticBeanStalk cli is very helpful when u wants to automate deployment pipelines    - @@@
            
       ElasticBeanStalk Under the hood (how it works)->
            - Under the hood ElasticBeanStalk relies on something called cloudformation - @@@
            - Go to cloudformation dashboard and u will see their are cloudformation stacks for the environments we have created inside
            ElasticBeanStalk    - @@@
            - and on clicking on those ElasticBeanStalk stacks u can check the outputs, resources , events etc
            - So ElasticBeanStalk actually under the hood generates some cloudformation files for u and cloudformation actually performs
            the heavy-lifting of updating everything
            
       ElasticBeanStalk Deployment Mechanism ->
            - When u deploy ur application u describe the dependencies of ur application which could be requirements.txt for pyhton or
            package.json for Node.js or other files.    - @@@
            - Zip files at tht time of deployment is uploaded to the ec2 machines   - @@@
            - Each ec2 machines will resolve the dependencies(it could be slow if u have lot of dependencies or they r big)
            - So in Optimization, in case we have very long and the length deployments is to package with the source code and that
            helps to improve the deployment performance and speed because are the dependencies are already being resolved and they
            get uploaded to each ec2 instances right away   - @@@
            
            - So ElasticBeanStalk takes the source code on each ec2 machine and resolve dependencies there locally on each ec2 machine.
            and that's why it is way safer to have ur dependencies and ur code, all along together into one zip file    - @@@
            
  Question -> I would like to customize the runtime of Elastic Beanstalk and include some of my company wide security software. I should?
  Answer -> Provide a custom Platform   - @@@
  
  Question -> I would like to update my Elastic Beanstalk application so that we are able to roll back very quickly in case of issues with the 
  new application version. Which deployment mode is the best fit?   - @@@
  Answer -> Immutable //to roll back quickly, this deployment mode terminates the temporary ASG that has the new version,
  while the current one is untouched and already running at capacity
  
  Question -> I want to update my Elastic Beanstalk application gradually without incurring new costs on update.
  My application has been over provisioned and can temporarily decrease in size for the number of serving instances,
  but I still want to serve my users without downtime.
  I do not want to incur extra costs over updates. Which deployment mode is the best fit?
  Answer -> Rolling
            
            
#8-> AWS CICD -> Continous Integration is when the developer push the code to a repository(Github,CodeCommit etc) the code is move or we
  can say pushed to testing/build server(Code Build/Jenkins etc) and checks the code and then the developer gets the feedback about
  the tests and checks that have passed/failed
  So the purpose of this is to Find bugs and fix them , deliver faster as the code is tested continously and to deploy often.
  So it results in the increase of Productivity because their is build server which is doing the testing for them
  
  Continous Deleivery -> To ensure that the software can be released reliabily whenever needed
      - Ensures deployments happen often and quick
      - In this u have to automate the deployment by using CodeDeploy or Jenkins as u like
      
     - So after the Continous Integration done we have a deployment server and basically it deploy every build that passes, So the
     deployment server will go and run a bunch of scripts that we have to program and it will make sure that the applications go from
     version 1 to version 2 or further when ever we push the code to the repository
      So continous Delivery is about the deploying as often possible, predictably and reliably.
      
      Technology Stack for CICD which are in increasing manner as follows
          Code -> Build -> Test -> Deploy -> Provision              - @@@
        and services for above stack in AWS is as follows
          CodeCommit(for code part) -> CodeBuild(for build and test part) -> Elasticbean Stack(for Deploy and Provision part) but
          sometime u r managing ur own fleet of EC2 instances maybe using CloudFormation        - @@@
        
          To manage EC2 fleet u may be want to use AWS CodeDeploy which is bit difficult to use
          and to orchestrate all the these things(Code,Build,Test,Deploy,Provision) we have to use Codepipeline     - @@@
          
   AWS CodeCommit -> 
    - Version control is the ability to understand the various changes that happened to the code over time(and possibly role back)
    - And having a Code Repository enables u to do Version Control
    - Git repository can live on any machine,but it usally lives on a central online repository and this is where Version Control happens
    - Benefits of Centralized Git Repository
        - Collaborate with other developers
        - To make sure the code is backed up some where
        - Make sure it's fully viewable and autitable(bcz it tells that who pushed code and when)
            
            For starting a buissness Git repositories can be expensive
            In Github u have free public repositories and private ones too
          So AWS says in CodeCommit is also going to be Git Repository and less Expensive
          
          - AWS codecommit is private Git repository        - @@@
          - Their is no size limit on the Git repository(so it scale seemlessly)    - @@@
          - Codecommit is fully managed and highly available
          - Ur code is only in ur AWS Cloud account(So this increases security and compliance)
          - U can add on to security features such as encryption , access control etc       - @@@
          - Codecommit can integrate with other build servers also such as Jenkins
          
  CodeCommit Security ->
    - Interactions are done using Git(so all the git standard commands can be used)     - @@@
    - When u authenticate in Git u have SSH keys so as been a AWS user u can configure ur own SSH key in ur IAM console or u can use
    HTTPS which is done using the AWS CLI Authentication Helper or Generating HTTPS Credentials     - @@@
    - MFA(Multi Factor Authentication) can be enabled for extra safety      - @@@
    - For Authorizaion u can use the IAM policies which will manage ur IAM users,roles to make sure they have the right to ur repository
    - Repositories are automatically encrypted at rest using KMS        - @@@
    - Ur data is encrypted in transit when u push the code(for this we have to use SSH or HTTPS)
    - For cross account access do not ever share SSH keys or AWS crednetials that's a security Risk, So the right way is to use
    IAM role in ur AWS account and then other person has to use AWS STS(with AssumeRole API) then the other person from the cross
    account will be able to access ur CodeCommit repository         - @@@

          CodeCommit vs Github ->
            Similarities ->
                - Both are git repositories
                - Both support code review(Pull requests)
                - Both integrated with AWS Codebuild
                - Both support SSH and HTTPS method of authentication
            
            Differences ->
                - Their are few differences in security as Github is administered through something called Github Users where as Codecommit
                use directly will use ur AWS IAM roles and users        - @@@
                - Their are differences in Hosting Github is hosted by Github So it's a third party so their is bit of Trust happening and
                if u r using Github enterprise that means u have to self host Github on ur servers and it is not managed anymore as u have
                to manage ur own servers
                While codecommit is managed by AWS and Hosted by AWS so it is more secure   - @@@
                - UI of Github is fully featured and codecommit UI is minimal featured
                
        CodeCommit Notifications->
          - u can trigger notifications in CodeCommit using the AWS services such as SNS(Simple Notification Service) or Lambda or Cloudwatch
          event rules       - @@@
          - Usecase for noifications by SNS/Lambda->    - @@@
              - on deletion of branches
              - trigger for that happens in  master branch
              - Notify external build System
              - Trigger AWS Lambda function to perform codebase analysis(to see if someone by mistake committed AWS credentials in the code
              or any credentails at all)
           - Usecase for CloudWatch Event Rules ->      - @@@
              - It has trigger for pull request updates(created pull request/update pull request/deleted pull request/ commented pull request)
              - Commit comment events (When someone commits the code and comments on top of it then u can use this notification event)
              - Cloudwatch Event rules triggers a SNS topic for notification
                
               Steps to push Code securely and set up users for it->
                - Go to IAM dashboard got to Users dashboard in IAM and click on the user u want then go to securoty credentails then go to
                SSH keys for CodeCommit or HTTPS Git credentials for CodeCommit - @@@
                - So for the SSH u can upload ur SSH key here and then u r automatically able to access codecommit  - @@@
                - And for HTTPS generate credentials , we can only generate two set of credentials and download them and we can reset 
                password or make them inactive if in future we want to      - @@@
                - For HTTPS u have to install GIT and use HTTPS url for connection same like in GIT
                - and use git commands for the further operations
                
        AWS Codepipeline -> It could be considered as the visual tool to perform continous delivery, It basically orchestrate between sources
        such as Github, codecommit, S3 and use build servers such as Codebuild and jenkins , we can also do the load testing using any third 
        party tool and finnaly Codepipeline will deploy using codedeploy , elasticbeanstalk ,cloudformation,ecs etc.
        
        - Pipeline is made of stages -> - @@@
            - Each stage can have sequential actions or parallel actions ,so u can customerize ur pipeline such as to perform load testing or
            other things    - @@@
            - Stages Example-> Build , Test , Deploy, Load Test etc - @@@
            - Their is a feature in which u can ask for manual approval at any stage except on the source pull      - @@@
         - Pipeline basically works with artifacts
            - Each Pipeline will create artifacts       - @@@
            - Artifacts are passed stored in S3 and passed on to the next stage         - @@@
            
            So u have ur S3 bucket and then u get a trigger and so ur source will get triggered and u push some code and source will send
            everything all the code to S3 and this is called source Output artifacts        - @@@
            
            These artifacts will get come through S3 bucket to go in the build stage so code build will get these artifacts and build it
            and after building it , it may also generate artifacts and it may be like generated binaries or zip files and put it into
            S3 bucket and again this output artifact will piped to the deploy stage to code deploy so the code deploy has the output of the
            build stage and then just deploy ur files       - @@@
            
         CodePipeline Trouble Shooting -> When ever the state change in the pipeline it will generate a cloudwatch event and this event can 
         trigger a SNS Notification         - @@@
          So ex for this ->     - @@@
              - U can create an event for failed pipelines
              - U can create events for cancelled stages
              
  When ever a Codepipeline fails a stage then ur pipeline stops and doesn't deploy anything and we can go to console and see the information
  for failing of it.
          
          AWS Cloudtrail can be used to audit AWS API calls

      If Pipeline can't perform any action such as it cannot deploy to ElasticBeanStalk their is usally an IAM Role issue (Means he service
      role u have attached to ur pipeline doesn't have permission), So u should trouble shoot the policy    - @@@
          
          Code Pipeline Hands On ->
          - Go to CodePipeline Dashboard
          - then enter pipeline name and choose source  provider(Github, S3, CodeCommit) and then select branch and repository , and
          also choose the detection option which is through Cloudwatch(recommended one) and other one is to use CodePipeline Periodically
          for changes (this means the codepipeline ping the repository all the time which is inefficient)
          - then choose build and deployment providers and then choose the Service Role or IAM role for code pipeline u can use the already
          available role 'AWS-CodePipeline-Service'.
          - artifacts are stored in S3 bucket u will be shown the location for artifacts in buckets at the time of review before creating 
          pipeline.
          - To allow the Manual Approval for deploy the code we can go to the view pipeline dashboard of the codepipeline and a new stage
          and choose action 'Approval' for that stage and approval type as 'Manual' and then do the Update

AWS CodeBuild -> It is used for building and testing an application
  - CodeBuild is a fully managed build service (means that as an alternative to other tools such as Jenkins it's going to be able to
  continously scale , So as a user, u don't have any servers to manage or provision , their is no build queue on the other hand for
  the tools such as jenkins which is an open source build tool u have to provision it onto ur ec2 machines and make it run and provision
  jenkins agents and when u have build and if ur agents r buzy they r going to build queue ,So basically in code build u don't need to
  manage any servers or provision anything)
@@@ - U pay for usage means the time it takes to complete the builds (It's better then jenkins as u do not need to pay for the whole day)
  - Codebuild under the hood leverages docker , Docker basically allows for a reproducible to builds to happen onto various kind of other
  in system with different configurations   - @@@
  - It's possible to extend the capabilities of codebuild by using our own base Docker images(So it's preferred to use own Docker images
  over the AWS provided Docker images as it will also enable us to have more customization over the build)  - @@@
  - Codebuild is secure as it has integration with KMS when u want to encrpt build artifacts, it also has the integration with IAM for the
  build permissions So when ever the code build interacts with other AWS components it will use IAM , u can run code build i ur own VPC if
  u want more security in network and u can also use the Cloudtrail for the API call logging , So these feature make the code build more
  secure.           - @@@
  
  CodeBuild Overview ->
    - It can source code from various places such as Github, codecommit, etc
    - Build instructions can be defined in code (this file is called buildspec.yml file)        - @@@
    - codebuild logs are basically outputed to S3 and cloudwatch logs(this u have to configure) but basically when the code build finishs
    the whole container that run the build goes away and the only thing that is left for u to troubleshoot are S3 logs and the cloudwatch
    logs.   - @@@
    - u have metrics to manage ur codebuild statistics and make sure it doesn't time out or fail.
    - u can also use the cloud watch alarms to detect failed builds and also trigger notifications
    - u can use the cloud watch events or AWS Lambda as a Glue for everything and trigger the SNS notifications     - @@@
    - u r able to reproduce Codebuild locally to troubleshoot in case errors       - @@@
    
 THeir are two ways to define   - @@@
      - u can define a code build within ur pipeline
      - u can define a codebuild in codebuild itself (both these ways cab interact with each other and they don't play well with each other)
      
 Code Build supported environments ->   - @@@
    - Java, Ruby, python , go ,nodejs, android ,.NET core and PHP
    - With docker u can extend any environment u like 
    - Base of code build is docker images 
 
 How code build works?      - @@@
 - we have the source code in the code commit  and we will be needing the buildspec.yml file at the root of ur code and we also gonna need a
 docker image so we can either use Amazon-managed Docker image or we build our own , Their is code build container that is going to start
 as soon as code build is triggered and the contianer is basicaly using the Docker image we set from before as the source image from the
 run and then the run instructions to tell what will container do to our code from buildspec.yml
 Their is optional S3 cache bucket which basically means that as u go and do multiple builds, u r able to cache such as dependencies or artifacts
  and u can pull in this cache when ur build starts in order to increase the performance,
  When the code build after the code container finishes and passes succesfully it pass the output to the S3 bucket which is ur artifacts 
  bucket and the logs will be saved to CLoudWatch
  And the code build finishes successfully the cache file within the code build would be put back to S3 again for having the feedback loop
  
  Codebuild buildspec.yml file->        - @@@
    - buildspec.yml file must be at the root of ur code.
    - In buildspec.yml file we
      - Define environment variables -
          - Plaintext variables
          - For Secure secrets we can use SSM Parameter store
      - Phases(these r the specific commands to run) , their r four phases-
          - Install phase : to install dependencies u may need for ur build
          - Pre Build phase : final cmds to execute just before build
          - Build phase : it has the actual build commands
          - Post Build phase : for the finishing touches (such as the zip file for output)
      - Artifacts: to decide what to upload to S3(encrypted with KMS)
      - Cache:u decide which Files to cache(usally dependencies) to s3 for future build speedup at it givs the performance improvements
      
  CodeBuild Localbuild ->   - @@@
    - this is for the case u need to do the deeper troubleshooting beyond logs
    - u can run codebuild locally on ur desktop(after installing docker)
    - For this u also need to leverage the codebuild agent (Google code build agent to know more)
              
 Codebuild HandsOn -> Go to code pipeline dashboard and edit the pipeline add a stage after sourcing and select the action for that stage
 as Test then selct ur own Docker image for the environment image
 - then select the type of cache u want (for better performance select S3)      - @@@
 - If during the tests the build fails then the codebuild will show u the logs for trouble shooting
 - To create the buildspec.yml and Hands on things related to it then go to the link
  -> https://www.udemy.com/aws-certified-developer-associate-dva-c01/learn/v4/t/lecture/11851338?start=0
 - Once their is new commit the code commit the code pipeline will get executed automatically
 - In the code build we can check for the triggers and the metrics
 
 CodeDeploy -> It is in the context of that we want to deploy our application automatically to many EC2 instances
 So the ec2 instances of v1 version will be get upgraded to the v2 version without Elastic Beanstalk
 - Their are several ways to handle deployments using open source tools such as(Ansible, Terraform,Chef, Puppet etc) all these technologies
 allow us to basically deploy application onto EC2 instances.
 
 AWS Codedeploy - Steps to make it work ->  - @@@
  - Each EC2 machine(or on machine Premise machine) must be running the codedeploy agent(it is kind of little software running in machine)
  - So the agent is basically going to continously pull for the AWS CodeDeploy service and ask it do I have to deploy new stuff?
  - then the code deploy will send the appspec.yml file or point to it atleast
  - the application is pulled from Github or S3
  - ec2 will run the deployement instructions
  - Codedeploy agent will report of success/failure of deployement on the instance
  
  Scenerio -> So the developer has the source code and appspec.yml file , the file should be present at the root of ur source code and it
  define how the application gets deployed, SO the developer push the source code to the repository which will trigger a deployment 
  notification to AWS code DEPLOY. Now the ec2 instances continously pull from the code deploy and they will realize that they have
  triggered the deployement and then they will start the deployement
  So they will download the source code and the appspec.yml file into the EC2 instances and the agent will take care of running of the 
  instructions which are in the appspec.yml file to deploy the application correctly    - @@@
  
  - EC2 instances are grouped by deployement groups which are dev, prod , test etc      - @@@
  - u have lot of flexibilty regarding what u want to call the deployement groups
  - codedeploy could be chained to code pipeline and can use artifacts from the codepipeline    - @@@
  - codedeploy can reuse existing setup tools such as like anything u have on ur ec2 instances can be reused by codedeploy, it works with
  any application and it has autoscaling group
  - So codedeploy is little bit more powerfull than Elastic beanstalk   - @@@
  - Note: u can do Blue/Green Deployements but it only works with EC2 instances not on on-premise instances - @@@
  - Code deploy only deploys the application it does not provision resources so it assumes ur ec2 instances are already existing    - @@@
  
Codedeploy Primary components ->        - @@@
  - Application: unique name
  - Compute platform: EC2/On-premises or Lambda
  - Deployement Configuration: Deployement rules for success/failure
      - EC2/On-premises: u can specify the minimum no. of healty instances for the deployement
      - AWS Lambda: specify how traffic is routed to ur updated lambda functions versions.
  - Deployement groups: group of tagged instances(allow to deploy gradually)
  - Deployement type: In-place deployement or Blue/green deployement
  - IAM instance profile: need to give ec2 the permissions to pull from s3/github
  - Application Revision: application code + appspec.yml file
  - Serivce Role: Role for code deploy for what it needs
  - Target Revision: Target deployement application version
  
  CodeDeploy AppSpec -> Their r two sections        - @@@
    - File Section: how to source and copy from s3/github to filesystem
    - Hooks: set of instructions and cmds to deploy new version(hooks can have timeouts).The order is: (The Order of the hooks is important)
        - Stop Application Hook -> The first thing we want to do when we deploy new version is to stop the current version application
        - Download bundle hook -> this basically says how do I download my new application
        - Before Install hook -> it allows u for the preperation before installing ur application version install
        - After install hook -> it happens when u wanna clean up after ur install or launch a server or something like this
        - Application Start hook -> how u start ur application
        - Validate Service -> Once ur application has started how to make sure that ur application is working , so a health check is required
        to make sure ur application is correctly deployed
        
Codedeploy deployemnt configuation ->

- Configs:  - @@@
  - One a time: deploy one instance at a time, and if one instance fails the deployement should stop
  - u can deploy half at a time so 50% this , then another 50 % 
  - all at once deploy : it's quick but then u get no healthy host, u get some downtime and it's good for development
  - u can set custom config as min no. of healthy host should be 75%
- Failures: In case if failures - @@@
  - Instances stays in failed state
  - if u do a new deployement then the deployement will first be deployed to failed state instances which guarantes u don't bring down ur 
  whole application because of a failure
  - if u want to rollback to a previous version u should just redeploy the old deployements or enable automated rollback for failures
- Deployement Targets: these are set of ec2 instances and u can assign tags to them     - @@@
  - u can deploy them directly to an ASG
  - u can do a mix of ASG and tags so that u can build deployement segments
  - u can do customization in scripts with DEPLOYEMENT_GROUP_NAME environment variable (and u cna define if u are in prod then or
  if u can in dev then do these things so u get lot of control)
  
  In Place Deployement - Half at a time     - @@@
  Scenerio -> We have 4 instances of v1 version then we take first two down and do the deployment their and then we take the another half
  down and then do deployement then and this way we get from v1 to v2
  
Note: Elastic Beanstalk uses code deploy under the hood     - @@@

In Blue Green Deployement:
Scenerio -> u have a load balancer and it's attached to one ASG of instances and then u r going to create a ASG of instances of version v2
and load balancer is going to redirect to both of these and after the pass of health check the first ASG is deleted and now Load balancer 
will just talks to ASG of v2

Code Deploy Hands On In Place Deployement ->
appsepc.yml working and process for the deployement link ->
  https://www.udemy.com/aws-certified-developer-associate-dva-c01/learn/v4/t/lecture/11851340?start=0
- Go to Code deploy dashboard then choose the type of deployement u wanna do 
- then configure instances
- to check the instances create by the code deploy for u run the cmd -> sudo service codedeploy-agent status in the ec2 instance terminal
and if shows a msg the agent is running then every thing is healthy
- then create a deployement group and preview the service role policy
- then do the deployement configuration (One at a time, half at a time, all at once)
- and then deploy

Code Deploy Hands On Blue Green Deployement ->
appsepc.yml working and process for the deployement link ->
  https://www.udemy.com/aws-certified-developer-associate-dva-c01/learn/v4/t/lecture/11851344?start=0
- Go to Code deploy dashboard then choose the type of deployement u wanna do 
- then choose the environment configurations for the deployment
- Start the deployement
- if u refresh the DNS link again and again before rerouting traffic to replacement instnaces u will see the load balancers is actually
serving between v1 and v2 application together as the changes would be visible between them on refresh after rerouting traffic to replacement 
instnaces on the refresh u will not see the change on refresh because the old instances are deleted.

Question -> Your CodeBuild has failed. What isn't a solution to troubleshoot what happened? - @@@
Answer -> SSH into Code Build Container to debug from there, // Reason -> CodeBuild containers are deleted at the end of their execution (success or failed).
You can't SSH into them, even while they're running

Question -> You would like to deploy static web files to Amazon S3 automatically. Which services should you use for this?   - @@@
Answer -> CodePipeline + CodeBuild // Reason -> CodeBuild can run any commands, so you can use it to run commands and copy
your static web files to Amazon S3.

#9-> AWS CloudFormation -> Infrastructure as code on AWS is done using Cloudformation.
  - Cloudformation is a declarative way of outlining AWS Infrastructure, for any resources(most of them r supported)
  
  Benefits of Cloudformation
  - Infrastucture as code       - @@@
    - No resource is manually created, which is excellent for control
    - the code can be version controlled for ex using Git       - @@@
    - Changes to the infrastructure are reviewed through code
  - Cost : Cloudformation itself is free - @@@
    - Each resource within the stack is stagged with an identifier so u can easily see how much a stack costs u - @@@
    - u can estimate the costs of ur resources using the cloudformation template    - @@@
    - so for ex if u have staging strategy using cloudformation in ur development environment u could automate deletion of templates at 5pm
    and recreated at 8pm safely  - @@@
  - Productivity
    - ability to destroy and recreate an infrastucture on the cloud on the fly as many as times u want.
    - u can automate generation of diagrams for ur templates(good for presentations)
    - Declarative Programming(no need to figure out ordering and orchestration)     - @@@
  - Seperation of concern: create many stacks for many apps and many layers such as we can have     - @@@
    - VPC stacks: that creates all the networks into the subnet
    - Network stacks
    - App stacks: For each application u deploy, there gonna be an application confirmation stack(each time we create an environment in
      Elastic beanstalk, it went ahead and created a cloudformation template behind the scenes)
  - Their are already cloudformation templates on the web which we can reuse        - @@@
  
  How cloudformation works?
  - we upload the templates in s3 and cloudformation pulls them from s3     - @@@
  - to update a template, we can't edit a previous template, we have to reupload  a new version of template and then cloud formation will
  do the difference and figure out what it needs to do to update from version 1 to version 2.   - @@@
  - Stacks can be identified by name.   - @@@
  - Deleting a stack deletes every single artifact that was created by cloudformation.(so every resource is deleted once the stack is deleted)
 
Deploy cloudformation templates - @@@
- Manual Way 
  - Editing templates in the cloudformation designer
  - Using the console to input parameters, etc
- Automated way
  - Editing templates in a YAML file
  - using the aws cli to deploy the templates
  - automated way is the recommended way
  
 CloudFormation Building Blocks ->
  - Template components     - @@@
    - Resources: ur AWS resources declared in the template it's manadatory to mention to them bcz ur cloudformation resources can not work
    if u don't specify resources(such as EC2 machines, elastic IP's etc)
    - Parameters: the dynamic inputs for ur templates
    - Mappings: the static inputs for ur templates
    - Outputs: we can export some stuff and other templates can reference it
    - Conditionals: List of conditions to perform resource creation
    - Metadata
  
  - Template Helpers        - @@@
    - References: link ur stuff within ur template
    - Functions: to transform data within ur templates
    
Cloudformation Simple Example->
  Scenerio -> Create Simple EC2 
              create and add elastic IP to ec2
              add two security groups to it
              
  Steps -> Go to cloud formation dashboard 
          // for existing template if u go to View/Edit template in designer u will get the diagram flow for the template
          // Cloudfroamtion stacks are region dependent 
          - click on create a stack
          - then select or upload or design or specify s3 template url for the template
          
          Template for above scenerio ->        - @@@
            Resources:
              MyInstance:
                Type: AWS::EC2::Instance            // create ec2 instance 
                Properties:
                  AvailabilityZone: us-east-1a      // confiuration for ec2 instance
                  ImageId: ami-adadada              // confiuration for ec2 instance
                  InstanceType: t2.micro            // confiuration for ec2 instance
                  
           - Enter the stack name
           - Then provide tags, IAM Role(If we don't use it then the cloudformation use the permissions defined in ur account), Rollbacks,
           , advance feature for notifying using SNS , termination option(so that no one delete's the cloudformation template) etc all
           these options are optional   - @@@
           - After above steps u will get the the tempalte URL which was uploaded to S3
           - We also get the cost estimator on the same page
           - then create the stack
        - To check what happens during the creation of a stack go to the Events tab(First thing is the event logs will be the log for the
        creation of the cloudformation stack)       - @@@
        -U can go to the resources tab to see the resources created by the stack
        
  Cloudformation update and delete stack Hands On->
    
    - New Cloudformation Template->     - @@@
    
        Parameters:
          SecurityGroupDescription:
            Description: Security Group Description
            Type: String

        Resources:
          MyInstance:
            Type: AWS::EC2::Instance           
            Properties:
              AvailabilityZone: us-east-1a      
              ImageId: ami-adadada              
              InstanceType: t2.micro            
              SecurityGroups:
                - !Ref SSHSecurityGroup               // assigning these security groups to ec2
                - !Ref ServerSecurityGroup
                
         # an elastic IP for ur instances
         MyEIP:
           Type: AWS::EC2::EIP
           Properties:
              InstanceId: !Ref MyInstance
         
         # our EC2 security Group
         SSHSecurityGroup:
          Type: AWS::EC2::SecurityGroup
          Properties:
            GroupDescription: Enable SSH via port 22
            SecurityGroupIngress:
            - CidrIp: 0.0.0.0/0
            FromPort: 22
            IpProtocol: tcp
            ToPort: 22
            
         # our second EC2 security Group
         SSHSecurityGroup:
          Type: AWS::EC2::SecurityGroup
          Properties:
            GroupDescription: !Ref SecurityGroupDescription
            SecurityGroupIngress:
            - IpProtocol: tcp
            FromPort: 80
            ToPort: 80
            CidrIp: 0.0.0.0/0
            - IpProtocol: tcp
            FromPort: 22
            ToPort: 22
            CidrIp: 192.168.1.1/32
            
- Go to ur previous stack then go to actions dropdown and select the update stack and upload the above template
- We can not change the name of an already created cloudformation template
- after the upload of new template the SecurtiyGroupDescription textbox will appear bcz in our template we have mentioned in our 'Parameters'  - @@@
- then specify the other common steps for configuration just like in simple cloudformation template example or if u want to edit them
- On the Review Page u can see the option for the 'Preview ur changes' and see the changes going to happen to the stack
- if the 'Replacement' for any resource in 'Preview ur changes' is true then that means that resource is going to be replaced with new one
- Click on Update

To delete stack go to the stack then to the action dropdown and select delete stack

YAML->
  - Cloudformation supports JSON and YAML
  - JSON is horibble for CloudFormation
  - YAML works on Key Value Pairs
  - YAML has support for array ,Ex=         - @@@
             product:
                - sku : dadadafwrwerew  // here '-' represents the array , the product is an array of sku,quantity,ttt
                  quantity : 666
                  ttt: aaeqw  
                  
 - YAML has Nested objects and multi line string support also       - @@@
        built-in:
            given: sadadas
            address:                      // address object nested in built-in object
              lines: |                    // used for the multi line string
                    dassdaas dadada dasda
                     asdas dasa dadada
            
 - Include comments with Ex: # Comment_String - @@@
 - We can define lists - @@@
    SecurityGroups:
                - !Ref SSHSecurityGroup               // this is a list of security group
                - !Ref ServerSecurityGroup

What r Resources?
- Resources r core of a cloud formation template(they r Mandatory)      - @@@
- Resources represent different AWS components that will be created and configured
- Resources are declared and they can reference each other      - @@@

AWS figures out creation, update and deletion of resources for us
There are over 224 types of resources   - @@@
Resource types identifiers r of the form: AWS::aws-product-name::data-type-name     - @@@

How do find the resources documentation?
google AWS cloud formation aws-template-resource-type-ref

can I create a dynamic amount of resources?     - @@@
No u can't, everything in the cloudformation template has to be declared. u can't perform code generation there

Is every AWS service supported?
Almost, there are few that are not supported , u can work around them using AWS Lambda custom resources

What r Parameters?
- Parameters r a way to provide inputs to ur AWS cloud formation template       - @@@
- They r used for
  - If u want to reuse ur templates across the aws accounts or regions          - @@@
  - Some inputs can be determined ahead of time, Ex: The key pair u r going to link ur ec2 instances

Parameter r extremely powerful,controlled and can prevent errors from happening in ur templates (with types)

Where to use parameters?        - @@@
If u r going to change the resource configuration in cloudformation in future then use Parameters
As u don't have to re-upload a template to change its content

Parameter Settings      - @@@
  - Parameters can be controlled by Type (String, number, commadelimitedlist, List<Type>, AWS Parameter(to help catch invalid values))
  - Description
  - Constraints
  - ConstraintDescription(String)
  - Min/Max Length for String
  - Min/Max value for numbers
  - Defaults
  - AllowedValues(array) // when u want to restrict the number of values a user can pick
  - AllowedPattern(regexp)
  - NoEcho(Boolean)
  
How to reference a Parameter?       - @@@
use function 'Ref' , Parameters can be used anywhere in a template
Shorthand for Ref function in YAML is !Ref
Functions can be used to reference other elements within the template

Pseudo Parameters Concept
  - AWS offers pseudo parameters in any cloudformation template - @@@
  - They can be used at any time and enabled by default     - @@@
  - We use !Ref with pseudo parameters      - @@@
  Ex: AWS::AccountId , AWS::NotifiactionARNs, AWS::NoValue, AWS::Region, AWS::StackId, AWS::StackName
  
What r Mappings?
- Mappings r fixed variables within ur cloudformation template(they have to hard coded)     - @@@
- They r very handy to differentiate between different environmets(dev vs prod), regions, AMI types etc     - @@@
Ex: Mappings:                                                           RegionMap                  
      Mapping01:                                                          us-east-1:
        Key01:                                  other EX                     "32": "ami-3213132"
         Name: Value01                                                       "64": "ami-2323432"
        Key02:
         Name: Value02
        Key03:
         Name: Value03
        
When would u use mappings vs parameters?        - @@@
Mappings are great when u know in advance all the values that can be taken and they can be deduced from variables such as
  Region, AZ, AWS Account, Environment(dev vs prod) etc
They allow safer control over the template but use Parameters when the values r really user specific

Fn::FindInMap for accessing mapping values      - @@@
- We use Fn::FindInMap to return a named value from a specific key
- Syntax - !FindInMap [MapName, TopLevelKey, SecondLevelKey]
    Ex: !FindInMap [RegionMap, !Ref "AWS::Region", 32] // here !Ref "AWS::Region" is a pseudo parameter

What r Outputs?
- The output section declares optional outputs values that we can import into other stacks(if u export them first)  - @@@
- u can also view the outputs in the AWS Console or in using the AWS CLI
- They r very useful in cases such as if u define a network cloudformation and export the output such as VPC id and ur subnets ids and then
u can reuse them in other cloudformation templates
- It's a best way to perform some cross stack collaboration , as u let experts handle their own part of the stack and as an app developer
u just reference these values
- U can't delete a cloudformation stack if it's outputs r being referenced by another cloudformation stack      - @@@

Ex of Outputs ->        - @@@
    Outputs:
      StackSSHSecurityGroup:
        Description: The SSH security group
        Value: !Ref MyCompanySecurityGroup
      Export:                               // this is an optional block if u don't specify it the value will not be exported for others use
        Name: SSHSecurityGroup

// To import above value we use function Fn::ImportValue
    Resources:
      MySecureInstances:
        Type: AWS::EC2::Instance
        Properties:
          SecurityGroups:
            - !ImportValue SSHSecurityGroup    // importing from other stack

What r conditions used for?
conditions r used to control the creation of resources or outputs based on a condition  - @@@
conditions can be whatever u want them to be,but common ones r:     - @@@
  - Environment(dev/test/prod)
  - AWS Region
  - Any parameter value
Each condition can reference another condition, parameter value or mapping      - @@@

How to define a condition?      - @@@
Conditions:
  CreateProdResources: !Equals [ !Ref EnvType, prod ]     //here we r checking that the value of the reference EnvType should be equal to prod
- The intrisnic function (logical) can be of the following:
    - Fn::And
    - Fn::Equals
    - Fn::If
    - Fn::Not
    - Fn::Or
    
- conditions can be applied to resources,outputs etc  - @@@
  Ex: Resources:
        MountPoint:
          Type: "AWS::EC2::VolumeAttachment"
          Condition: CreateProdResources
         
CloudFormation Instrisic Functions  - @@@
  
  Fn::Ref -> it is using for referincing    - @@@
    - Parameters as it returns the value of a parameter
    - Resources as it returns the physical ID of the underlying resource(ex: EC2 ID)        - @@@
   
   -Shorthand in YAML is !Ref   - @@@
   
  Fn::GetAtt ->         - @@@
    - Attributes can be attached to any resources u create
    - To know the attributes of the resources look into the documentation
    
      Ex: Resources:
            EC2Instance:
              Type: AWS::EC2::Instance
              Properties:
                ImageId: ami-32sfdf
            MountPoint:
              Type: "AWS::EC2::Volume"
              Condition: CreateProdResources
              Properties:
                Size: 100
                AvailabilityZone: !GetAtt EC2Instance.AvailabilityZone
                
   Fn::FinInMap -> it is used to return a named value from a specific key       - @@@
      - Syntax - !FindInMap [MapName, TopLevelKey, SecondLevelKey]
        Ex: !FindInMap [RegionMap, !Ref "AWS::Region", 32] // here !Ref "AWS::Region" is a pseudo parameter     - @@@
  
   Fn::ImportValue -> it is used to import values that r exported in other templates        - @@@
        Resources:
          MySecureInstances:
            Type: AWS::EC2::Instance
            Properties:
              SecurityGroups:
                - !ImportValue SSHSecurityGroup    // importing from other stack

    Fn::Join -> Join values with a delimiter            - @@@
      -Syntax -> !Join [delimiter, [comma-delimited list of values]]
       Ex -> !Join [":",[a,b,c]] // this put colon between a, b and c
       
     Fn::Sub -> also use as !Sub as a shorthand is used to substitude variables from a text, it allows u to fully customize ur templates - @@@
      - For example u can combine Fn::Sub with references or AWS pseudo variables
      - String must contain ${VariableName} and substitude them
      
      Synatx -> !Sub
                  - String
                  - { Var1Name: Var1Value, Var2Name: Var2Value}
                  
     Conditions Functions -> use above reference for conditions
     
   CloudFormation Rollbacks->       - @@@
    If stack creation fails then:
      - by default: everything rolls back(gets deleted)     - @@@
      - their is also option to disable the rollback and troubleshoot what happened     - @@@
    If stack update fails then:          // such as in case the ami id is not valid at the time of updation - @@@
      - the stack will automatically rollback to the previous known working state       - @@@
      - u get the ability to see in the log what happened and error msgs            - @@@

    Note: if some resources were created during before the update fail those resources will be deleted back - @@@
    
 Question -> To make your infrastructure created with CloudFormation evolve over time, you should do which of the following?    - @@@
 Answer -> Upload a new version of a cloudformation template with modified code and apply it in the cloudformation console
 
 Question -> The !Ref function can be used to reference the following except...         - @@@
 Answer -> Conditions
    
#10-> AWS Monitoring, Troubleshooting and Audit ->

AWS Cloudwatch-> It allows to collect and track key metrics
- It allows u to collect, monitor, analyze and store log files      - @@@
- it allows for events to send notifications when certain things happen in ur AWS
- it provides alarms to react in real time to metrics and events logs           - @@@

AWS X-Ray-> It allows u to troubleshoot application performance and errors      - @@@
- it allows for the distributing tracing of microservices       - @@@

AWS Cloudtrail -> it allows for the internal monitoring of API calls being made
- it audit changes to AWS Resources by users

Cloudwatch metrics ->
  - cloud watch provide the metrics for every services in AWS
  - Metric is a variable to monitor(such as CPU Utilization, Network In etc)        - @@@
  - Metrics belong to namespaces
  - Dimension in a metric is an attribute of metric(instance id, environment etc)
  - we can have upto 10 dimensions per metric
  - Metrics have timestamps (to tell when certain events or things happen)
  - we can create cloudwatch dashboards of metrics
  
cloudwatch ec2 detailed monitoring ->
  - by default ur ec2 instances metrics have metrics every 5 minutes        - @@@
  - Question -> how to have ec2 instances metric every one minute?     - @@@
    Answer-> For extra cost, u can get data every one minute by enabling detail monitoring
  - Using the detailed monitoring u will be able to prompt more scale prompt ur ASG(means u will be able to react CPU changes quickly)   - @@@
  - the AWS free tier allows us to have 10 detailed monitoring metrics      - @@@
  
  Note: EC2 RAM or Memory usage is by default not pushed(if u want to push that RAM usage as a metric, u must be pushing it from instance
  as custom metric)     - @@@
  
Cloudwatch Custom Metrics ->
  - It is possible to define and send ur own custom metrics to cloud watch(they can be what ever u want)
  - u have ability to use dimensions(attributes) to segment metrics( such as instance.id, Environment.name)
  - Metric Resolution -     - @@@
      - by default the standard resolution is 1 min.
  - Question -> We want to get data every 10 seconds, how to do that?       - @@@
    Answer-> u need to enable high resolution custom metrics, So u can go upto 1 sec (means every sec u can get or push the custom metric)
    and it is going to be higher cost because u push more data and can enable high resolution and get very fine grained metric
 
  - The API call parameter to enable high resolution is called Storage Resolution       - @@@
  - to send metric to cloud watch u need to use PutMetricData which is an API call      - @@@
  - u should use exponential back off in case of throttle errors so if u send to many metrics to cloudwatch it tells u "hold on u r sending
  too fast" u need to use the exponential backoff strategy      - @@@
  
  Cloudwatch Hand's on ->
    - Go to cloudwatch dashboard u can see we get options such as dashboard , alarms, events, logs and metrics
    - So in metric and see we have been already provided with many(about 801) metrics
    - Metrics could be in a number or graph or stack area
    - once u name ur graph u r able to share it or add it to dashboards
    - For detailed monitoring Go to EC2 instnace and then to monitoring and click on "Enable Ddetailed Monitoring" or go to ASG and then 
    to monitoring and u will see the option "Enable Group Metrics Collection" buy default it is not enabled
    
 Cloudwatch Alarms ->
  - Alarms are used to trigger notifications for any metric - @@@
  - Alarms can be attached to ASG, EC2 Actions, SNS notifications           - @@@
  - their are various options so u can choose to metric to alarm on sampling, percentage , max, min etc
  - Alarms can be in 3 states   - @@@
    - OK state - means ur alarm is not doing anything
    - INSUFFICIENT_DATA - this is when u r not sending enough data for ur alarm, so the metric is missing some data points
    - ALARM - when ur alarm threshold is being passed
  - In terms of Period
    - u need to specify length of time in seconds to evaluate the metric    - @@@
    - for high resolution custom metric, u can choose 10 sec or 30 sec as the period for the evaluation of ur alarm     - @@@
   - To check the alarms for the ASG u can go to scaling policies of that ASG and check the policies for the activation of alarms and actions on them
   - In the ASG if the alarm action is to remove 1 instance and thr min no. of instance is 1 and their is only 1 instance the instance will
   not being removed    - @@@
   
Cloudwatch logs -> Applications can send logs to cloudwatch using the SDK   - @@@
  - Cloudwatch has integration and can collect logs from many different event services some of them are
      - Elastic beanstalk - collection of logs from application
      - ECS - collection from containers
      - AWS Lambda - collection from function logs
      - VPC Flow logs - VPC specific logs
      - API Gateway
      - cloudtrial based on filter
      - cloudwatch log agents: for ex on ec2 machines
      - Route53: log DNS queries
   - cloudwatch logs themselves can be processed in cloudwatch but they can be also exported to S3      - @@@
   - or cloudwatch logs can be streamed to an elasticsearch cluster for further analytics       - @@@
   - cloudwatch logs can use filter expressions(if u want to search through them)       - @@@
   - their is log storage architecture in it their is   - @@@
        - log groups: it usually represents an application  - @@@
        - log streams: it usually represents instances within ur application or log files or containers  - @@@
   - u can define log expiration policy to either never expire or have 30 days etc  - @@@
   - u can use aws cli so we can tail the cloudwatch logs
   - if u want to send logs through cloudwatch, u need to make sure that ur IAM permissions r correct       - @@@
   - for security all the logs can be encrypted using KMS at rest at the group level, so need to first define a log group   - @@@
   
   cloudwatch logs hands on - Go to cloud watch dashboard and then to logs
   - In log group their u will see log streams and inside the log streams u can see all the logs that happened.
   - select the log group then select the time under the Expire Events After column and u can set the retention policy for expire of group
   - select the log group and then go to actions dropdown and u will see can export the log group to S3
   - Go to Elastic beanstalk and then to environment and then to configuration then to monitoring then modify and selct the metrics u want
   to use and u can also define health monitoring rules so to enable it click on option "Enable" of log streaming
   - we also get the option to keep logs even after the termination of environment
   
   Cloudwatch Events -> 
    - u can either have schedule so u can define right on Cron job to schedules events on cloud watch so u can trigger notifications on
    demand  - @@@
    - u can have event pattern in cloud watch events that allows to define rules such as if a service in AWS is doing something we can
    trigger an event if that event match the rule       - @@@
      Ex: Code pipeline state changes   - @@@
    - we can trigger lambda functions or send a msg to SQS, SNS OR Kinesis      - @@@
    - when u have cloud watch events as an output the event it creates is a small JSON document to give information about the change
    Cloudwatch events hands on ->
      - Go to cloudwatch dashboard and then to events and then click on create rule
      - u can choose between event pattern and Schedule and then add targets(aws resources) to it
      - then create the rule
      
AWS X-Ray -> to do debugging in production the good old way is:
    - Test Locally
    - Add log statements everywhere
    - Re-deploy in production
    and from the logs try to figure out what is the issue and so this way is not so efficient
    and if u have different log stuff and different applications then they all will have different formats so it became really hard to 
    centralize insights and navigating cloudwatch log is going to be hard.
    - so if u have one huge application doing everything it's sort of easy to debug but if u have distributed services running about 100
    microservices in ur AWS account it becames really hard to debug
    - so their is no common view of ur entire architecture

    these all above mentioned problems could be solved by X-ray
    So X-ray gives u visual analysis of ur application so it will trace what happens happens when we talk to our AWS resources
    
    X-Ray advantages ->
    - u can troubleshoot the performance of ur application and identify bottlenecks     - @@@
    - understand dependencies in ur microservice architecture and u can visually see how ur microservices interact
    - we can pinpoint which service is giving issue
    - we can see how each request is behaving
    - we can find errors and exceptions based on the request
    - we can see that if we r meeting SLA in terms of latency or it's time to process a request     - @@@
    - we can see which services slows down and trottle us       - @@@
    - we can identify which users are impacted by ur errors
    
    X-Ray Compatibility is with:
      - Lambda
      - BeanStalk
      - ECS
      - ELB
      - API Gateway
      - EC2 instances or any application server(even on premise)
      
     How X-ray works?       - @@@
     it leverages something called tracing
     
     - Tracing is an end to end way to follow a request, So when I made a request to my application server so each component that we deal
     with a request such as database, gateway, loadbalancer etc will add it's own trace     - @@@
     - And so trace will be made of segments and segments can be made of sub-segments
     - we can also provide annotations to traces to provide extra information around what happened      - @@@
     - So this way u have the ability to trace :    - @@@
        - every request
        - Sample Request(if u only want to get a percentage of the total request or may be 5 requests per minute)
     - For X-Ray Security we have:
        - IAM authorization
        - KMS encryption at rest
        
      How to enable x-ray?
      their r two ways
      1) ur code(Java, Python, Node, Go, .Net) must import the AWS X-ray sdk    - @@@
          - very little code modification is required
          - then the application sdk will then capture:
              - calls to AWS services
              - HTTP/HTTPs requests
              - Database calls(MySql,PostresSQL,DynamoDB)
              - Queue Calls(SQS)
       2) Install the x-ray daemon or enable X-ray AWS integration      - @@@
       -So if we run on the machine on premise server or ec2 instance we need to install a daemon(Daemon is a little program that works as a
       low-level UDP packet interceptor ,it can run on Linux, windows or mac), So u have to install it on ur machine        - @@@
       - if u use lambda or other services that already have integration with X-ray then they will run the daemon for u     - @@@
       - Each application also must have the IAM rigths to write data to X-ray      - @@@
       
 Question - My X-ray application works on my computer when I test locally but doesn't work on my EC2 machine ,Why?  - @@@
 Answer - bcz on ur machine u r running the X-ray daemon but when u deploy it to ur ec2 instance it's not running the X-ray daemon so the
 X-ray doesn't see ur calls
 
 AWS X-ray troubleshooting
  - If x-ray is not working on EC2      - @@@
    - then make sure the EC2 IAM role has the proper permissions
    - and also ensure the ec2 instance is running the X-ray daemon
  
  - to enable x-ray on lambda       - @@@
    - make sure the lambda has an IAM execution role with proper policy(AWSX-RayWriteOnlyAccess)
    - make sure that X-ray code is imported and also the X-ray integration is enabled on AWS Lambda
    
    X-ray Hands-on ->
    - Got to x-ray dashboard (it is currently enable for few regions such as N.virginia)- Not sure of this point
    - then selct ur progamming language and then add x-ray sdk to code as shown theri, then choose the machine on which u wanna run X-ray
    and follows steps as told
    Note: Elastic Beanstalk needs to have .ebextension file for to have x-ray integration enabled
    
 AWS Cloudtrail ->
  - Provides u governance,compliance and audit for ur AWS account
  - Cloudtrail is enabled by default        - @@@
  - So u can get the histroy of API calls and events made within ur AWS account by:
      - console
      - SDK
      - CLI
      - AWS Services
  - u can put all the logs from cloudtrail into cloudwatch logs , this gives u the ability to keep the events all the time - @@@
  - if a resource is  deleted in AWS, then look into cloud trail first -Important   - @@@
  
 Question -> To send a custom metric to CloudWatch, which API should we use?    - @@@
 Answer -> PutMetricData
 
 Question -> Your CloudWatch alarm is triggered and controls an ASG. The alarm should trigger 1 instance being deleted from your ASG,
 but your ASG has already 2 instances running and the minimum capacity is 2. What will happen?  - @@@
 Answer-> The alarm will remain in "ALARM" state but never decrease the no. of instances in my ASG
 
#11-> AWS Integration & Messaging ->
  - When we start to deploying multiple applications they will need to communicate with each other
  - There r 2 patterns of application communication
    1) Synchronous Communications(application to application) // Ex: to sell something online we have two services buying service and 
    shipping service so at the time of buying the buying service need to talk to shipping service so they r directly connected to each
    other as the synchronous communication is going on.
    2) Asynchronous/Event based(application to queue to application) // here queue is middleware and it is connecting the applications,So
    this time the buying service will say something is bought so I'm gonna put that into a queue and shipping service will check that if
    their is something for it
    
    - Synchronous between applications can be problematic is their are sudden spikes in traffic So it's better to decouple ur application
    and having the decouplying layer scale for u , in that case that can be :   - @@@
        - SQS for a queue model
        - SNS for pub/sub model
        - kinesis for real time streaming(as you have big data)
        using above three methods ur services can scale independently from SQS,SNS and Kinesis
        
AWS SQS ->

- SQS is the oldest offering of AWS

What is queue?
queue takes msgs which r sent by producers and u can have as many producers u want and on the other end of the queue u r having consumer
and the consumer will poll msgs from queue and u can have as many consumers u want and the no. of consumers does not need to be same as
the no. of producers
 
 Their are different kinds of queues:
  - Standard Queue: 
        - It is fully managed
        - it can scale from 1 msg per second to 10,000 msgs per second (it is built for massive scale)  - @@@
        - default retention of msgs is 4 days (so the msgs r short lived inside of ur queue) and maximum retention is 14 days // So once the
        msg is consumed it is gone so u need to consume ur msg within 4 days if u don't set 14 days     - @@@
        - their is no limit how many msgs can be in the queue   - @@@
        - this is low latency service (< 10ms  on pubish and receive)
        - their is horizontal scaling in terms of no. of consumers      - @@@
        - u can have duplicate msgs - @@@
        - u can have out of order msgs // sometimes ur SQS queue will deliver msgs to ur consumers in a different order then msgs were recieved
        - there is a limitation u can't send huge msgs througth standard queue u can send upto 256kb per msg data // so don't send videos 
        through SQS, it is gud for JSON data        - @@@
        
   - Delay Queue:
        - in delay queue u can delay msgs so consumers don't see them immediately and u can delay msgs upto 15 mins   - @@@
        - by default for the queue it is 0 sec // means msg is available right away     - @@@
        - u can set the default at the queue level
        - u can override the default using the DelaySeconds parameter when u send data to SQS   - @@@
        - so the producer will send the msg to the queue and we have suppose 5 mins of delay so the msg will be in the queue for 5 mins
        and after 5 mins the consumer will be able to see the msg   - @@@
     
 - SQS- Producing msgs -> we have to define a body at max size of 256kb and it has to be string data and then u can add msg attributes
 and metadata, So msgs attributes r key value pair so u have a name , value and a type and it's optional for u to provide and we can
 also provide a delay delivery and that's also an optional thing , now the msg is send to SQS and we get back from SQS is a msg identifier
 as like an ID and a MD5 hash of a body     - @@@

 - SQS- Consume msgs -> Consumers poll SQS for messaging(receive 10 msgs at a time) - @@@
    - consumers have to process the msg within the visibility timeout    - @@@
    - once they r done with the msg within the visibility timeout they can delete the msg using the msg ID that was received and it is called
    receipt handle      - @@@
    - So from the SQS queue the msgs r polled by the consumer and then processed by the consumer and when the msg is processed the consumer
    tells the SQS that u can delete the msg now - @@@
        
     - SQS - Visibility Timeout - When the consumer polls a msg from the queue, the msg is invisible to other consumers for a defined period
     So suppose 10 consumers r consuming msgs from queue and once it got poll from the queue to one consumer, it is invisible to other 
     consumers for a period of visibility timeout   - @@@
     - u can set the visibility timeout from 0 sec to 12 hours and by default it is 30 sec      - @@@
     - if the visibility timeout is set for 15 mins and the consumers fails to process the msg for any reason such as database is out of
     order or any other reason then we need to wait for the time(visibility period defined time) before processing the msg again , may be
     for the same consumer or another consumer and this could be a big problem
     and if u set the visibility timeout too low(30 sec) and if the consumer needs more time(2 mins) to process the msg then another 
     consumer will recieve that msg bcz it is going to available only for 30 sec and it wasn't deleted by the consumer fast enough and
     so the msg will be processed more then once        - @@@
     
     So u don't wanna set visibility timeout too high or too low.       - @@@
     
     - if the consumer receives the msg and figures out that it need more time to process the msg u can change the timeout by using the
     ChangeMessageVisibilty API - Important     - @@@
     - when u r done with the msg u use DeleteMessage API to tell SQS that the msg was successfully processed       - @@@
     
 AWS SQS- Dead Letter Queue ->if a consumer fails to process a msg within the visibility timeout then the msg goes straight back to the queue
 and other consumers can get it and if that happens too many times may be bcz msg is not really gud or no one can process it and it goes back
 to the queue again and again   - @@@
 So we can define a threshold and it's called a redive policy and after the threshold is exceeded we can put it into a Dead Letter Queue
 And we have to make sure the msgs in the Dead Letter Queue are processed before they expire    - @@@
 
 AWS SQS - Long Polling(Important) -> When a consumer requests msg from the queue, the queue can respond right away yes or no but u can also
 wait for msg to arrive if they r not in the queue and this is long polling , it is optional    - @@@
 - long polling decreases the no. of API calls that u make to SQS and u increase the efficiency of ur application bcz as soon as the msg
 is available it is send to consumer    - @@@
 - the wait time can be configured between 1 sec and 20 sec(20 sec is preferable)   - @@@
 - usally long polling is preferable to Short polling
 - long polling can be enabled at the queue level as a default or when u consume u can use the WaitTimeSeconds API  - @@@

AWS SQS FIFO Queue -> FIFO (First In First Out) //this is new offering from AWS and it's not available for all regions  - @@@
- the name of the queue must end with .fifo     - @@@
- it has lower throughput (up to 3000 per second with batching, 300 msg per sec without)    - @@@
- Messages are processed in order by the consumer   - @@@
- Messages r sent exactly once  - @@@
- u can not set a per msg delay u can set only per queue delay  - @@@
- u get the ability to do de-duplication in the queue   - @@@
- the de-duplication interval is 5 min and u can use Duplication ID - @@@
- U r able to set Message Groups in ur FIFO queue // so only one worker can be assigned per msg group   - @@@
- For the creation of a FIFO queue all the settings are same as Standard queue except the Name of the queue will end with .fifo and we got
extra option also which is 'Content-based-duplication' which is for de-duplication // in de-duplication if use the same msg group id again
for any msg it will not count that msg as a new msg so no new msg is created as it is rejected  - @@@

SQS Extended Client -> as we know the msg limit is 256kb then how do we send the larger msgs and for this we use the extended client and
it is a JAVA library so basically it leverages S3 over the SQS queue,
So in this the producer will not send the large msg to SQS queue instead it will send it to S3 bucket and put msg their and send a small
metadata msg to SQS queue for indicating for the larger msg, now the consumer will recieve the medata msg and pull the larger msg from
the S3  - @@@

SQS Security -> their is in flight security using HTTPS endpoint    - @@@
- we can also enable SSE(Server Side Encryption) using KMS      - @@@
    - So here we can set the CMK(Customer Master Key) we want to use
    - can set the data key reuse period(between 1 min to 24 hrs)    - @@@
          - so lower u set the period the more KMS API will be used - @@@
          - and higher u set the period the less KMS API will be used // and this way u control the cost and default time is 5 mins - @@@
    - SSE will only encrypt the body not the metadata(message ID, timestamp, attributes)    - @@@
    - IAM policy must allow usage of SQS
    - SQS queue access policy: it's for finer grain control, u can use IP to decide who can access and control over the time the requests
    come in - @@@
    - their is no VPC endpoint so u must have internet access to access SQS, So u can't access SQS privately within ur VPC for now  - @@@
    
SQS- Must know API  - @@@
  - CreateQueue, DeleteQueue
  - PurgeQueue : delete all the msgs in Queue and this takes upto 60 seconds to happen
  - SendMessage, DeleteMessage, RecieveMessage
  - ChangeMessageVisibilty: to change the timeout of a msg
  
  - Batch APIs for SendMessage, DeleteMessage, ChangeMessageVisibilty which helps u decrease ur cost and RecieveMessage can recieve only
  upto 10 msgs so their is no Batch RecieveMessage  - @@@
  
AWS SNS -> To send one msg to many receivers , so we use pub/sub, So the producer publishes the data to SNS topic and in SNS topic we have
many subscribers and all these subcribers will get data in real time as a notification

- In SNS the event producer only sends one msg to SNS topic - @@@
- u can have as many event recievers or subcribers to listen to SNS topic notifications - @@@
- each subcriber to the topic will get all the msgs(now their is new feature to filter msgs)    - @@@
- u can have upto 10 million subscriptions per topic    - @@@
- u have 100,000  topics limit  - @@@
- Subscribers can be:   - @@@
    - SQS
    - HTTP/HTTPS endpoints (u can set how many times u want to set the delivery)
    - lambda
    - Emails
    - SMS msgs
    - Mobile Notifications
    
 - SNS integrates with lots of AWS products
 - Some services can send the data directly to SNS for notification - @@@
 - cloudwatch(for alrams)
 - ASG notifications
 - S3(on bucket events)
 - Cloudformation(upon state change ,ex: when their is failure to build)
 
 AWS SNS- How to publish?
 - u can do topic publish(within ur aws server using the sdk or cli)
    - u create a topic
    - create subscriptions
    - publish to topic
 
 - u have Direct Publish(this is for mobile applications)   - @@@
    - so u create platform application
    - u create platform endpoint
    - Publish to the platform endpoint
    - it works with Google GCM,Apple APNS,Amazon ADM etc
    
 - SNS + SQS is called Fan Out  - @@@
    - So if u want to publish a data to many SQS queues, so basically u push it once in SNS and many SQS queues receives the msg
    - So the SQS queues r subscribed to the SNS topic   - @@@
    - this is fully decoupled , so ur one service publish one msg and it is received many times by many queues  - @@@
    - their is no data loss - @@@
    - u r able to add receivers of data later so ur service still keeps on sending data to SNS topic but u can add queues later - @@@
    - SQS allows for delayed processing
    - SQS allows u for retrying ur work - @@@
    - u can have many workers on one queue or just one worker on another queue, so it allows u completely scale ur paradigm easily  - @@@
    
 - Kinesis -> Kinesis is a managed alternative to Apache Kafka
  - it is Great for application logs,metrics, IOT, clickstreams basically for any thing which is real-time big data - @@@
  - it is compatible for many streaming processing frameworks(Spark,Nifi etc)
  - Data is automatically replicated to 3 AZ    - @@@
  
  their are 3 sub kinesis products: - @@@
    1-> Kinesis Stream also commonly know as kinesis, it for how to ingest streams at scale with a low latency
    2-> Kinesis Analysis: this is perform real time analytics on streams using SQL, So it allows u to perform filters, computation,
    aggregations in real time using SQL language
    3-> Kinesis Firehose: it is to load ur streams into other parts of AWS such as S3, Redshift, ElasticSearch etc
    
Kinesis at high level:  - @@@
  So kinesis is composed of kinesis streams, we get streams from clickstream IOT devices, metrics, logs will be producing data
  directly into our Kinesis streams after having the data in the kinesis streams the kinesis performs computations, metric, monitoring,
  alerting whatever u want and for this u need to perform some computations in real time
  So this is Amazon Kinesis Analytics and once the computations r done it's gud to have them stored into S3, DB, Redshift etc
  So u use Kinesis Firehose to put it in S3 or Redshift
  So at the high level kinesis is in the middle for the streaming real-time data and it allows u to basically quickly board data in
  mass in real time for big data use cases all the way to analytics and to final store where u want to keep ur data for a long time

      Kinesis Streams Overview-> 
        - Streams r divided into ordered Shards or Partitions(Shards can be think of as one little queue)   - @@@
        - So producers r going to produce to a kinesis stream which is divided into shards and the consumer will consume from the either
        shard   - @@@
        - So more the shards the more data can go through   - @@@
        - So for the higher throughput we increase the number of shards     - @@@
        - Data retention is 1 day by default and can go upto 7 days - @@@
        - Kinesis allows u to reprocess and replay data // in SQS once the data is consumed it is gone but with kinisis the data is
        still their and it will expire after some time  - @@@
        - u r able to have multiple applications consume the same stream    - @@@
        - so this enables u for the real time processing and u have real scale of throughput bcz u add shards   - @@@
        - kinesis is not a database - @@@
        - So the data is inserted into kinesis it can not be deleted it is called immutability  - @@@
        - so ur add data is called log  - @@@
        - u add data over time and then u process using the consumers and data remains in kineses for 1 to 7 days and then u do something
        with it
        
        Kinesis Stream Shards ->
          - one stream is made of many shards   - @@@
          - one shard represents 1mb per second or 1000 msgs per second on the write side   - @@@
          - So producer can write upto 1000 msgs per second - @@@
          - on the read side we have 2 mb per second throughput per shard   - @@@
          - So if u need 5 mb per sec throughput u probabaly gonna need 3 shards for read and 5 shards for write    - @@@
          - the shards r provisioned, so u pay the for the no. of shards u r using  - @@@
          - if u have more throughput then ur shards then u will have throughput issue  - @@@
          - u have the ability to batch the msgs and calls  - @@@
          - this allows u to basically push msgs into kinesis   - @@@
          - the no. of shards can evolve over time  - @@@
          - so u can add and remove shards and provide some auto scaling for ur stream  - @@@
          - Records r ordered per shard - @@@
          - In SQS we have no order
          - In SQS FIFO we have one queue and all the records will be going to that queue          
          
     - Adding of shards is called resharding and the removing of shards os called merging   - @@@
     - consumers and producers know how to react to the changes of less and high throughput and adding and removing of shards and change
     their consumption patterns     - @@@
     
     AWS kinesis API - Put Records->
     - Put records API is a way to send data to kinesis     - @@@
     - For this u need to send data and a partition key     - @@@
     - So ur data(whatever string) + msg key will get hashed to determine the shard ID      - @@@
     - So key is basically a way for u to route the data to a specific shard    - @@@
     - and the rule is that the same key goes tp the same partition     - @@@
@@@  - So if u want to get all the data in order for the same key, then u provide that key to every data point and they will be order for u
     - So when ever the data is produced , now the msg know where to go to which shard bcz of the msg key   - @@@
     - and ur data will be produced to shard one, two, three etc but not at the same time   - @@@
     - ur data goes to only one data at a time      - @@@
     - so when the msg are sent to shard, they get a sequence no. and the sequence no. is always increasing - @@@
     - and if u need to choose the partition key, u need to choose one that is highly distributed bcz is ur key isn't distributed then all
     ur data goes to same shard and that is called hot partition or a hot shard     - @@@
     - u can use batching with PutRecords to reduce costs and increase throughput       - @@@
     - if u get an exception called ProvisionThroughputExceeded and this occurs when u go over ur limits and for this u can use retries and 
     exponential backoff    - @@@
     - we can use CLI,AWS SDK or producer libraries from various frameworks.
     
     AWS Kinesis API - Exceptions ->
     - ProvisionedThroughputExceeded Exeption : it happens when sending more data that was provisioned(exceeding mb/s or transcations 
     per second for any shard) , so we need to make sure we don't have a hot shard and our partition key is distributed widely
     Solution for this Exceptions is to retry backoff or to increase the no. of shards or to ensure ur partition key is distributed - @@@
       
    AWS Kinesis API - Consumers ->  - @@@
      - u can use a normal user using the cli and sdk
      - u can also use kinesis client library which is avilable in Java,NodeJS,Python,Ruby or .Net and it also uses DynamoDB to checkpoint
      offsets
      - and kinesis client library also uses the DynamoDB to track other workers and share the work amongst shards
      
      So basically the kinesis app that uses the kinesis client library will checkpoint the progress with DynamoDB and they will synchronize
      the work between them to consume msgs from different shards      - @@@

    - Kinesis Security
      - u use IAM policies to control the access and authorization
      - we get encryption in flight using the HTTPS endpoints
      - we can enable encryption at rest using KMS so this will be server side encryption for kinesis
      - we get the possibility to encrypt or decrypt the data client side       - @@@
      - if u don't want internet to send data to kinesis u can enable VPC endpoints and this only to enable the access of kinesis within
        ur VPC - @@@
    
    - Kinesis Data Analytics    - @@@
      - We perform real time analytics using SQL and these real time queries go to kinesis Stream
      - For Data Analytics there will be
        - Auto Scaling
        - it will be managed so we don't have to provision any server
        - it's continous so it's real time
      - We pay for actual consumption rate
      - We can create streams out of these real time queries
      
    - AWS Kinesis Firehose  - @@@
       - it is a fully managed service , there is no administration
       - it's near real time so their is 60 sec latency
       - this use for ETL so we load data into Redshift/S3/ElasticSearch/Splunk
       - We get automatic Scaling
       - we got support for the many data format but if u want to convert between one format to another format u have to pay for the conversion
       fee
       - If u have lot of amount of data then u have to pay for the little data so u have to pay on the basis of the amount of data
       - So u r basically going to pay only for what goes through firehose
       
    - SQS vs SNS vs Kinesis(Important)  - @@@
      - SQS ->
        1: Consumer pull data
        2: Data is being deleted after it is consumed
        3: Can have as many consumers(max 10,000) as we want
        4: No need to provision throughput it will scale automatically for u
        5: No ordering Guarantee(except the FIFO queues but then u get limited throughputs)
        6: their is individual msg delay capability
        
      - SNS ->
        1: Push data to many subsribers
        2: U can have upto 10 million subscribers
        3: Data is not persisted so that means it is lost if not delivered
        4: their is Pub/Sub , So u can have upto 10,000 topics
        5: U don't need to provision the throughput
        6: integrate with SQS for fan-out architecture pattern
      
      - kinesis ->
        1: this is for pull data
        2: we can have as many consumers as we want but we can only have one consumer per shard
        3: Possibility to replay data is available so we can reprocess the whole data if we want
        4: It is more meant for real-time big-data,analytics and ETL
        5: their is ordering but it is at the shard level
        6: the data expires after x no. of days so their is some data retention but it is temporary
        7: u must provision ur throughput in advance
        
Question -> Your consumers poll 10 messages at a time and finish processing them in 1 minute.   - @@@
You notice that your messages are processed twice, as other consumers also receive the messages. What should you do?
Answer -> Increase the visibility timeout

Question -> Your SQS costs are extremely high. Upon closer look,    - @@@
you notice that your consumers are polling SQS too often and getting empty data as a result. What should you do?
Answer -> Enable Long Polling

#12-> AWS Lambda -> Serverless doesn't mean their is no server, it means their is no need to manage, provision or see them but in the
 background someone is running them for u

Severless offerings in AWS ->
  - Lambda & Step Functions
  - DynamaoDB
  - Cognito
  - API Gateway
  - S3
  - SNS & SQS
  - Kinesis
  - Aurora Serverless
  
Why Lambda?
- Virtual Functions no servers to manage while in EC2 these r virtual servers in the cloud
- limited by time - short executions while in EC2 these r limited by RAM and CPU    - @@@
- Run on-demand while in EC2 these r continously running        - @@@
- Scaling is automated while in EC2 here Scaling means intervention to add/remove servers   - @@@

Benefits of AWS lambda->
  - Easy Pricing: Pay per request and compute time
      - we get free tier of 1 million lambda requests then u pay $0.20 per 1 million requests thereafter and then u pay per duration
      in increments of 100ms so the way it computes is 400,000 gb/s of compute time means u get 400,000 seconds of execution if u r using
      1 gb of RAM after that u pay $1.0 for 600,000 gb-seconds  - @@@
      So it is cheap to run AWS lambda
  - Integrated with the whole AWS Stack(means lambda can be used anywhere to orchestrate,integrate and improve how u run AWS)   - @@@
  - it can be used with many programming languages
  - Easy monitoring through AWS cloudwatch  - @@@
  - Easy to get more resources per functions (upto 3gb of RAM and the smallest is 128 mb)   - @@@
  - more RAM u get it will also improve CPU and network - @@@
  - lambda scales up and down automatically to handle workloads, so if their is lot of processing going on automatically lot of lambda
   functions r executing simultaneously.   - @@@
  
AWS language Support ->
  - Nodejs
  - Python
  - Java
  - golang
  - .net
  - Powershell  - @@@
  
List of main Resources with whom lambda is integrated -> API Gateway,Kinesis,DynamoDB,S3,IOT,Cloudwatch Events,Cloudwatch logs,SNS,SQS,Cognito

Lambda Configurations ->
  - Timeout: default 3 sec, max 300 sec // this is how long our function can run before failing - @@@
  - we can also send environment variables which we can access in our code by OS module of respective programming languages, they allow 
  u to modularize the code and we can encrypt them  - @@@
  - Allocate Memory (From 128mb to 3gb)
  - we r able to deploy within VPC + assign security group for our functions //Ex: we can talk to RDS instance deployed in the VPC by
  deploying a lambda function in the same VPC.  - @@@
  - IAM execution role must be attached to the lambda function.
  - On the particular Lambda function the Resources list on the left hand side could be used to trigger that lambda function and the
  list of resources on the right hand side r the resources currently accessible to it
  - In the block name code entry type we get the options for the code inline(normal lambda usage), upload zip file or file from s3
  - we can select the runtime for the Program   - @@@
  - we can tag lambda functions - @@@
  - if u select a VPC it will be bit slower to start a function in comparision when u have not select any VPC ,if u select VPC then
  u need to select the subnets, security groups - @@@
  
Lambda Concurrency and Trottling ->
  - Concurrency: ur lambda function can have 1000 execution that r simultaneously across ur entire region // means if u have lot of load
  u can scale to 1000 lambda functions running at the same time (that limit can be increase through a ticket)   - @@@
  - so if we have small load on our lambda functions we may have 2 lambda functions at the same time, so two invocations of the same lambda
  functions running
  - u can set a "reserved concurrency" at the function level // basically to say that a function should have at least a reserved capacity
  to be able to run upto lets say 100 functions concurrently at the function level  - @@@
  - Now each invocation of the concurrency, over the concurrency limit, will trigger a trottle  - @@@
  - throttle has the behaviour how u invoke the lambda function , if u invoke it synchronously for ex from the aws console this is 
  synchronous invocation -> then it return trottle error 429    - @@@
  if it as asynchronous invocation it will retry automatically and then if it retries too many times, it goes to something called
   DLQ(Dead Letter Queues)  - @@@
  
Lambda Retries and DLQ ->
  - if a lambda function asynchronous invocation fails,it will be retried twice - @@@
  - after all retries, unprocessed events go to th DLQ  - @@@
  - DLQ can be a SNS topic(if u want to receive an email notification) or SQS Queue(if u want to have workers work on these requests and
  to debug and understand why it failed and maybe process them later)   - @@@
  - Ex: S3 is an asynchronous trigger   - @@@
  - to set DLQ go to ur lambda function and then to option of Debugging and Error handling and their u have 2 options SQS and SNS   - @@@
  - and u need to also set the IAM role for using SNS and SQS accordingly and then set the concurrency according to ur need - @@@
  - u also have the trottle test simulator at the top of the lambda - @@@
  
Lambda Logging, Monitoring and Tracing ->
  - CloudWatch ->
    - Lambda execution logs r stored in cloudwatch logs - @@@
    - lambda metrics r displayed in cloudwatch metrics
    - make sure ur lambda function has an execution role with an IAM policy that authorizes write to cloudwatch
  - X-ray: we can integrate with X-ray  - @@@
    - it's possible to trace lambda with X-ray
    - enable in lambda configuration(and then run the x-ray daemon for u) // to enable it go to lambda function and then selct the option
    Enable active tracing in Debugging and error handling
    - use AWS SDK in code
    - Ensure lambda function has correct IAM Execution role

Lambda Limits ->
- Execution
  - Memory allocation goes from 128mb to 3008mb (and it gois to 64 mb increaments)
  - maximum execution time: 300 sec(5 mins for exam now it is upto 15 mins)
  - the disk capacity in the function container(in /tmp directory is writable): u can write upto 512 mb - @@@
  - concurrency  limits: 1000 but u can improve it through a support ticket 
- Deployement
  - lambda function deployement size(compressed .zip) is 50 mb max  - @@@
  - size of uncompressed deployement(code + dependencies) is 250mb max  - @@@
  - if u want to go over 250mb u can load file or dependencies at startup and write to ur /tmp directory    - @@@
  - size of environment variables is upto 4kb   - @@@
  
Lambda Versions ->
  - when u work on the lambda function u work on the $LATEST    - @@@
  - $LATEST is mutable, means we can change it as we want   - @@@
  - when we r ready to publish a lambda function we create a version which is immutable means we cannnot change it ever it's a snapshot - @@@
  - and the versions keeps increasing the version number    - @@@
  - each version will get their own ARN - @@@
  - version = code + configuration(environment variable,timeouts etc nothing could be change everything is immutable)   - @@@
  - each version of the lambda function can be accessed using ARN   - @@@
  - we create versions to have dev environment , test environment and prod environment  - @@@
  
  We can use Lambda Aliases for wiring up the versions  - @@@
  - Aliases r pointer to lambda function versions , So versions r immutable and aliasis r not
  - So we can define a "dev", "test", "prod" aliases and have them point at different lambda versions and we can change these aliases
  over time because they r mutable.
  
  Scenerio -> So we have three lambda versions($LATEST,v1,v2) and we can create a DEV alias which is for the development environment which
  we can point to $LATEST version, So the users now don't directly interact with Functions they interact first with alias and in same way we
  can create alias for the other versions which r immutable
  So if we want to do Blue/Green deployement or update our prod environment to migrate from v1 to v2 so we can do deployement by assigning
  weights So we can say the PROD alias 95% of this we r going to point v1 and  5% to v2 and this we can see with little traffic how the
  v2 is doing   - @@@
  
  - So we r able to integrate basically in the back end while providing a stable configuration for our users, event triggers and destinations
  
  Lambda versions hands on ->
  - go to the lambda function then to the option Qualifiers and their u will see version and alias
  - go to the actions dropdown and then click on publish new version and then this will take the snapshots and make it version 1
  - and same way jsut edit some code and create the second version
  - the go on action dropdown and then selct the create alias option and then give the name of alias and the version for it and it can point
  to one or two versions
  - to update the version of the alias don't directly change the version bcz that will update all the prod traffic and send it straight to
  version 2, so go to the alias and their u will see the option for "u can shift traffic between 2 versions....." and then selct their new
  version and weight for it and now the PROD environment is pointing to 2 lambda functions versions and if u test the alias and test
  it again and again u will see changes between the outputs based on which version of lambda is executing and when u r satisfied with
  testing then change the main alias version to 2 directly.
  
 Lambda Best Practices ->
  - Perform heavy duty work outside of ur function handler:
    -So basically u should connect to database outside of ur function handler otherwise everytime ur function handler is invoked u r 
    going to reconnect to ur database   - @@@
    - Initialize the AWS SDK or any long stuff outside of ur function handler   - @@@
    - Pull in dependencies or datasets outside of ur function handler using execution context , So ur AWS lambda will be able to reuse 
    all this heavy duty and only have subsequent execution of ur function to be more efficient  - @@@
  - Use environment variables for:
    - Database connection strings,bucket etc, don't ever put them directly into ur code it's bad practice
    - and for passwords and other sensitive values definately use enviroment variables for them they can be encrypted using KMS and
    remember ur environmnet variables can only 4kb size max
  - Minimize ur deployement package size to its runtime necessities
    - So break down the function if it's too big    - @@@
    - Remember the lambda limits in terms of how big ur package must be So 50mb compressed and 250 uncompressed - @@@
  - Avoid using recurrsive code, never have a lambda function call itself //otherwise u will have concurrency hell  - @@@
  - Don't put ur lambda function in a VPC unless u have to // bcz if u put it in VPV it will take a little bit longer to initialize
  
  Hands on for best practices->
    - Go to ur lambda function and put the big code such as code to connect to database outside of the main handler function
    
Question -> Your lambda function is invoked asynchronously and some events fail from being processed after 3 retries.
You'd like to collect and analyze these events. What should do you? - @@@
Answer -> Add a Dead letter SQS to send messages to SQS
    
#3-> AWS Dynamo DB (NoSql Serverless Database)->
  - In traditional architecture the client interacts with the application layer which contains EC2,ASG,ELB etc and then this application
  layer interacts with the Database layer, the traditional applications leverages RDBS databases
  - These databases have the SQL query languages
  - and these databases have strong requirements about how the data should be modeled
  - u get the ability of Joins, aggregations ,computation and so these databases can become compute-heavy and can become costly
  - there is vertical scaling means usally getting more powerfull CPU/RAM/IO    - @@@
  
  - NoSql databases r non-relational databases and r distributed and so their is horizontal scaling on them     - @@@
  - NoSql databases does not support Join //unless a little feature is added to it - @@@
  - All the data that is needed for a query is present in one row
  - And NoSql databases don't perform any computations for u such as "SUM"
  - that's why the NoSql is better option for Serverless
  
DynamoDB ->
  - It is fully managed,highly available with replication factor across 3 AZ.       - @@@
  - it scales to massive workload bcz it is distributed and horizontally scaled         - @@@
  - u can scale millions of request per second,trillions of rows and 100s of TB of storage
  - it has Fast and consistent performance as it is distributed(low latency of retrieval)   - @@@
  - it is integrated with IAM for security, authorization and administration
  - we can do event driven programming with DynamoDB streams
  - it has low cost and also has auto scaling capabilities              - @@@
  
 DynamoDB Basics ->
  - it is made of tables
  - each table has a primary key(it must be decided at the creation time of table)
  - each table can have infinite no. of items(=rows)
  - each item has attributes(=coulumns and these can be nested) // attributes can be added over time and can be null
  - Max size of a item is 400kb                 - @@@
  - Data type Supported are: String,Number, Boolean,Binary,Null and also can support List and Map So we can have list or map as an
  attribute, Set types are String Set, Number set,binary set // so these allows u to do nested stuff        - @@@
  
  DynamoDB - Primary Key ->
    - First Option is to have Partition Key only this is called Hash            - @@@
    - Partition key must be unique for each item
    - Partition key must be diverse enough so that the data must be distributed across many partitions
    - Ex: user_id for User Table as partition key and for attributes we can have First Name,Age
    
    - Second Option is to have Partition key + Sort key // this is dynamoDB specific
    - the combination of partition key and sort key must be unique
    - data is grouped by dynamodb under partition key so all the data sharing the same partion key will be together and then it will be
    sorted by sort key, Sort key is also called range key and this allows u to do efficient queries         - @@@
    - Ex: user-game table u can use user_id for the partition key and game_id for the sort key and combination of user_id + game_id
    can be our primary key      - @@@
    
    Question -> We r building a movie database with movie_id,producer_name,leader_Actor_name,movie_language what is the best partion key
    here to maximize the distribution?
    Answer -> movie_id
    
DynamoDB Provisioned Throughputs -
  - Table must have provisioned read and write capacity units       - @@@
  - RCU(Read Capacity Units): throughput for reads
  - WCU(Write Capacity Units): throughput for writes
  - u r able to independently scale RCU's and WCU's
  - u can also get option to setup the auto scaling of throughput to meet demand        - @@@
  - throughput can be exceeded temporarily using the "burst credit"     - @@@
  - if u over use the burst credits and the burst credits r empty bcz u overuse RCU or WCU, u will get the "ProvisionedThroughputException" - @@@
  - so it's advised to use exponential back-off retry  - @@@
  
DynamoDB -Write Capacity Units -                  - @@@
  - one WCU represents one write per second for an item up to 1kb in size
  - if the items r larger than 1kb,more WCU r consumed
    Ex 1: so we write 10 objects per second of 2kb each
      then we need 2*10 = 20 WCU
    Ex 2: if we write 6 objects per second of 4.5kb each
      then we need 6*5 = 30 WCU (4.5 gets rounded to 5kb)
    Ex 3: if we write 120 objects per minute of 2kb each
      then we need 120/60 * 2 = 4 WCU
      
Strongly Consistent Read vs Eventually Consistent Read       
 - First in DynamoDB u get the option to choose between Strongly Consistent Read and Eventually Consistent Read
 - Eventually consistent read is that if u read just after u write something, it's possible u will get an unexpected response bcz of
 the replication // means u may get old data or may get no data based on the role that was before    - @@@
    Eventually consistent means that if u keep on asking eventually u will get the right results (about in few 100 ms)   - @@@
  - Strongly Consistent Read is that if we read data just after write we will get the correct data   - @@@
  - By Default DynamoDB will use Eventually consistent Reads for GetItem, Query and Scan but their is consistent read option parameter
  that u can set to true and by doing it u will get Strongly Consistent Read    - @@@
  
  Scenerio for Strongly Consistent Reads and Eventually Consistent Reads ->     - @@@
    - So our application writes to the DynamoDB and it reach to our First server (as we a availabilty on 3 AZ's so we have 3 servers)
    and after the first server confirms the write correctly then in background then the data will be replicated to server 2 and server 3
    of dynamoDB and this replication may get time and so if u read right after writing or u read before the replication has happened
    or is fully complete then u r in Eventual Consistent Read pattern and so if we read from server 3(we don't know exactly from where the
    read is going to happen) it will show Eventual Consistent Read.
    but if we ask for Strongly Consistent Read then the read will be just a little bit longer but get right data
    
DynamoDB - Read Capacity Units     - @@@
  - One read capacity unit represents one strongly consistent read per sec, or two eventually Consistent Reads per sec, for an item
    upto 4kb in size
  - if the items r larger then 4kb, more RCU r consumed
  
  Ex 1: if we have 10 strongly Consistent Reads per second of 4 kb each
        then we need 10 * 4kb / 4kb = 10 RCU
  Ex 2: if we have 16 eventual Consistent Reads per second of 12 kb each
        then we need (16/2) * (12/4) = 24 RCU
  Ex 3: if we have 10 strongly Consistent Reads per second of 6 kb each
        then we need 10 * 8kb / 4k = 20 RCU (we have to round up 6kb to 8kb as it goes by increaments of 4kb)

DynamoDB - Internal Partitions
  - Data is going to be divided internally into different partitions then distributed       - @@@
  - Partiton keys go through hashing algortithm to know to which partition data go to
  - To compute the no. of partitions:                           - @@@
    - By capacity: (Total RCU/3000) + (Total WCU/1000)
    - u get the size of the data set: total size / 10 GB
    - total partitions = CEILING(MAX(Capacity,Size))
  - WCU and RCU are spread evenly between partitions, So if u have 10 partitions and u have 100 WCU and 100 RCU, then each partition
  gets 10               - @@@
  
DynamoDB - Trottling        - @@@
  - if we exceed our RCU or WCU, we get ProvisionedThroughputExceededExceptions         - @@@
  - Reason could be:        - @@@
    - u have Hot key: one partition key is being read too many times
    - Hot Partitions
    - vary large items: ex if u get 400 kb item then u need to assign more RCU and WCU
  - Solutions for these r:          - @@@
    - Exponential Back-off when u get an exception(it is already included in SDK)
    - Distribute partition keys as much as possible
    - if it is an RCU issue, we can use DynamoDB Accelerator(DAX) // it's basically a cache
 
 RCU and WCU Hands On ->
  - Go to the table and then to the capacity tab u can alter the RCU and WCU as u need and their is also capcaity calculator and see
  in it that their is Eventual consistent or strongly consisitent and switch between them as u line and u get to know other things 
  such as estimated cost
  - u can also enable auto Scaling and if u do enable it for read and write capacity then u can't provision or change RCU and WCU 
  for database anymore and then decide that how much percentage of RCU or WCU u want all the time (such as I want 70% of RCU all
  the time) and minimum is 5 units and maximum is 40,000 units and this will auto scale ur dynamodb table to match the load of ur
  application           - @@@
  
DynamoDB - API's ->
  - DynamoDB - Writing Data ->
    - PutItem -  it is to write data (put data, replace data or create data) to DynamoDB and it consumes WCU
    - UpdateItem - it is to update data in DynamoDB (this does the partial update of attributes) // So if u use the putitem to update
    data it will do full replace where as the update item will update the attributes u indicate         - @@@
        - U can also use Atomic counters in UpdateItem and increase them if u want to count Such as no. of page hits        - @@@
    - Conditional writes - as u r in distributed system and there may be different applications accessing the same row at the same time
    u may want to write a condition such as the "write or updates r only valid if the conditions r accepted otherwise rejects"    - @@@
    Ex: I want to update this row only if this attribute still has this value so if the two applications run at the same time to the
    same row and one of them does the write before the other one bcz it has specified a condition it will gets it write denied
        - this helps with concurrent access to items
        - no performance impact       - @@@
 - DynamoDB - deleting data ->      - @@@
     - DeleteItem - for delete item we will use only an individual row or item
          - u will be able to perform a conditional delete(Such as delete an item only if this matches as a condition and someone 
            didn't modify the row before me)
     - DeleteTable - this deletes an entire table and all its items
          - for much quicker deletion then call DeleteItem  on all items
          
      Question -> We need to wipe every day, whole table and delete all data,should u use DeleteItem ot DeleteTable?  - @@@
      Answer -> It is much better and much cost efficient to use Delete table and maybe recreate the Table afterwards
      
 - DynamoDB Batching Writes -> Batching is always efficient within Network              - @@@
    - BatchWriteItem - U can do 25 PutItem or DeleteItem in one call // their is no update          - @@@
        - their is limit of 16mb of data written
        - and limit of 400kb of data per item
      - Batching allows u to save in latency by reducing the no. of API calls done against DynamoDB // bcz we can do one call to dynamodb 
      instead of 25         - @@@
      - Batching is also more efficient bcz DynamoDB will perform the operations parallel for u             - @@@
      - it's possible for a part of batch to fail (Ex: if u get throughput exception), in this case we have to try the failed items(using
      the exponential back off algorithm)       - @@@
      
  - DynamoDB Reading Data ->
      - GetItem -> it is to read based on Primary key
            - Primary key is either a Hash or Hash Range // either a partition key alone or a Partition key + sort key
            - u get eventual consistent reads by default
            - u also have to use strongly consistent reads(this take more RCU and might take longer time)
            - if u want only certain attributes in the results u can use something called ProjectionExpression // this helps u save
             in network badwidth - @@@
            
       - BatchGetItem -> u can batch upto 100 items         - @@@
            - and get upto 16mb data                - @@@
            - items will be recived in parallel to minimize latency    - @@@
            
   - DynamoDB Query ->
       - Query return items based on the Partition key value(it must be an equal(=) operator for partition key value)
       - Sortkey value for it u can specify (=,<,<=,>,>=,Between,Begin) this is optional in ur query we use Sortkey to get tailored results
       - FilterExpression to further filter(the filtering only happens client side)             - @@@
       
     - Return -> it return upto 1mb of data                 - @@@
        - or number of items specified in limit             - @@@
        - and u r able to do the pagination on results      - @@@
        - u can query a table, a local secondary index or a global secondary index          - @@@
        
   - DynamoDB Scan -> the inefficient way of querying the dynamoDB is called Scan
      - Scan scans the entire table and then filter out data (inefficient)
      - it returns upto 1mb of data by default and u need to use the pagination to keep on reading      - @@@
      - as u read the entire table it consumes a lot of RCU // this so extremely inefficient        - @@@
      - to limit the impact on scan on ur production workload, may be u want to use a limit or reduce the size of the results and pause
      - for faster performance on scan u can use parallel scans:            - @@@
          - multiple instances scan multiple partitions at the same time
          - this will increase the throughput and it consume very much RCU // it is more efficient way of scanning but u will maximize ur RCU
          - If u will limit the impact of parallel scans u would do it just like scan
       - for scan u can also use the ProjectionExpression + FilterExpression (no change to RCU) - @@@
           
To undestand LSI and GSI properly -> https://www.udemy.com/aws-certified-developer-associate-dva-c01/learn/v4/t/lecture/11851416?start=0            
DynamoDB-LSI(Local Secondary Index) ->
  - it is an alternate range key for ur table , and this range key is local to the hash key or the partition key
  - u can have upto 5 local secondary indexes per table             - @@@
  - the sort key consists of exactly once scalar attribute          - @@@
  - the attribute u choose must be a scalar string,number or binary
  - LSI must be defined at table creation time not afterwards       - @@@
  
DynamoDB-GSI(Global Secondary Index) -> 
  - to speed up the queries on key attributes use GSI               - @@@
  - GSI = Partition key + optional sort key //so it basically define a whole new index        - @@@
  - the index is a new table and we can project attributes on it            - @@@
      - the partition key and the sort key of the original table r always projected(KEYS_ONLY)
      - U Can also specify extra attributes to project (INCLUDE Parameter)
      - all the attributes in the table should be projected onto the new Global Secondary Index (ALL)
  - Must Define RCU/WCU for the index               - @@@
  - Possibility to add/modify GSI(not LSI)          - @@@
  
  - So LSI is local to a partition key where as GSI is a whole new different table          - @@@
  - u can check the LSI and GSI in the indexes tab
  
DynamoDB Optimistic Concurrency ->          - @@@
  - DynamoDB has a feature for Conditional update and conditional delete this means that u can ensure that when u update or delete an
  item it hasn't changed, So as we can access stuff over the internet it is possible that multiple clients access the same object at the
   same time
   So we want to make sure that we delete something only if a certain condition is validated
  - this makes the dynamoDB an optimistic locking/concurrency database 
  Scenerio -> Client 1 and client 2 wants to update the item but only if it has version 1 so the both clients wants to update on a
   condition at the same time so any one of the update for client 1 or client 2 can happen but the another one update will be stopped
   and this is optimistic locking
  
DynamoDB DAX->
  - It means it is a seemless cache for dynamoDB and u can enable it and u don't need to re-write any of the ur applications        - @@@
  - but now all the writes u do to the dynamoDB they go through DAX     - @@@
  - DAX basically provides u micro seconds latency  // it's a very low latency in case u have a cache read and cache query, So
    it's cache for reads        - @@@
  - It solves the hot key problem
  - By defaults ur items live 5 mins in ur cache   - @@@
  - u can have to 10 nodes in to ur cache cluster, so u can scale a lot     - @@@
  - it is multi AZ, So u can enable mutliple nodes in Multile Availablilty zones(3 nodes minimum recommended in production)     - @@@
  - DAX is secure, u get encryption at rest using KMS, u have VPC endpoints, u have IAM permissions and u can use cloud trail  - @@@
  
  DAX Hands On ->
  To enable DAX go to DAX dashboard in DynamoDB dashboard and the create a cluster and specify the nodes u want and cluster size and
   configure other configurations according to ur needs
  
DynamoDB Streams ->
  - Any change in DynamoDB(Create, Update, Delete) can end up in a DynamoDB Stream          - @@@
  - these streams can then be read by AWS lambda, and do things like:                   - @@@
      - React to changes in real time(such as sending a mail)
      - We can do analytics         - @@@
      - create derivative tables/ views
      - insert data into elasticsearch
  - could implement cross region replication using streams                  - @@@
  - streams in dynamodb only have 24 hrs of data retention                  - @@@
  
 Hands On-> Go to ur table and then go to Stream Details in Overview Tab then click on Manage Stream and selct ur option, if u want ur
  stream to be integrated with Lambda then go to triggers tab and then create a trigger and then configure the options and then enable
  trigger and provide proper IAM role for it such as Policy AWSLambdaDynamoDBExeutionRole and in cloudwatch we can check the logs for
  the streams               - @@@
 
DynamoDB Security and Other Feature ->              - @@@
   - Security ->
      - VPC Endpoints r available to access DynamoDB without internet
      - Access fully controlled by IAM
      - Encryption at rest using KMS
      - Encryption in Transit using SSL/TLS
    - BackUp and Restore Feature available
       - Point in time restore like RDS
       - Enabling this feature does not have any performance impact
    - Global Tables // helpfull if ur application is in many regions
        - Multi Region, fully replicated, high performance
    - U can use Amazon DMS can be used to migrate data to DynamoDB(From Mongo,Oracle,MySql,S3 etc)          - @@@
    - If u want to do development on a machine then u can launch local DynamoDB on ur computer for development purposes

Global Tables -> Amazon DynamoDB global tables provide a fully managed solution for deploying a multi-region, multi-master database,
 without having to build and maintain your own replication solution. When you create a global table, you specify the AWS regions where you
 want the table to be available. DynamoDB performs all of the necessary tasks to create identical tables in these regions, and propagate
 ongoing data changes to all of them.

 - Requirements for adding a New Replica Table
        -The table must have the same partition key as all of the other replicas.
        -The table must have the same write capacity management settings specified.
        -The table must have the same name as all of the other replicas.
        -The table must have DynamoDB Streams enabled, with the stream containing both the new and the old images of the item.
        -None of the replica tables in the global table can contain any data.
   -If global secondary indexes are specified, then the following conditions must also be met:
       -The global secondary indexes must have the same name.
       -The global secondary indexes must have the same partition key and sort key (if present).
- Best Practices and Requirements for Managing Capacity
    - Using DynamoDB auto scaling is the recommended way to manage throughput capacity settings
    - If you decide not to use DynamoDB auto scaling, then you must manually set the read capacity and write capacity settings on each
     replica table and secondary index.

  - Monitoring Global Tables ->
    - You can use Amazon CloudWatch to monitor the behavior and performance of a global table. Amazon DynamoDB publishes ReplicationLatency
     and PendingReplicationCount metrics for each replica in the global table

Question -> You want to use Query equal operation on a non key attribute. How can you do it?        - @@@
Answer -> Create a Global Secondary Index

Question -> You would like to have query for a non key attribute for the >= predicate while keeping the same partition key. You should
Answer -> Create a Local Secondary Index            - @@@

Question -> You would like to react in real-time to users de-activating their account and send them an email to try to bring them back.
The best way of doing it is to...
Answer -> Integrate lambda with a DynamoDB Stream

Question -> How can you select the attributes to retrieve in the response while using the GetItem DynamoDB CLI?
Answer -> ProjectionExpression              - @@@

#4-> AWS APIGateway ->
    - it is a way for us to build, deploy and manage API's, API's r interfaces we can expose to other people
    - client request as rest API to APIGateway and and then pass those requests to lambda functions
    - We can handle versioning in it
    - It Handle different environments(dev,test,prod...)        - @@@
    - Handle Security // so we can remove security concerns out of our lambda functions and have them taken all the way through API Gateway
    - we can create API keys, handle request throttling             - @@@
    - we have integration with Swagger/Open API import to quickly define API's          - @@@
    - it helps us to transform and validate requests and responses          - @@@
    - it generate SDK and API specifications
    - it also has the embedded caching layer so u can cache api response and limit the load that happens on ur lambda functions  - @@@
    
API Gateway Integration -> U have to seperate them Outside of VPC and Inside of VPC     - @@@
    - Outside Of VPC
      - u can run AWS Lambda functions
      - endpoints of EC2 instances
      - load balancers
      - Any AWS service
      - External and publicly available HTTP endpoints
    - Inside of VPC
      - Lambda in VPC
      - ec2 endpoints in ur VPC
      
 API Gateway Stages and Deployement ->
    - When u make changes in the API gateway does not mean they're effective right away, u need to make a deployement and they can be in
    effect after we deploy these changes    - @@@
    - changes r deployed to 'Stages' // we can have as many stages we want
    - use the name u like for stages(dev/test/prod)     - @@@
    - Each stage will have it's own configuration parameters
    - Stages will have the history of all the deployements made to them so we can rollback to deployement if we want to.
    - we use stages bcz we can have stage variables         - @@@
    
 API Gateway - Stage Variables ->
    - Stage variables r like environment variables for API Gateway          - @@@
    - we can use them to change often changing configuration values         - @@@
    - They can be used in :     - @@@
        - lambda function ARN
        - HTTP Endpoint
        - Parameter mapping templates
    - Use cases:        - @@@
        - we can configure HTTP endpoints ur stages talk to(dev,test,prod...)
        - we can pass configuration parameters to lambda through mapping templates// So the lambda function can know is it's dev,test 
        and prod
    - Stage variables r passed to the "context" objects in AWS lambda
    
 API Gateway Stage Variables and Lambda Aliases ->    - @@@
      - lambda aliasis were pointing to lambda versions so We can create a stage variable to point to specific lambda aliase
      - and our APIGateway will automatically invoke the right lambda function based on the aliase pointing to stage variable
      
 API Gateway - Canary Deployement -> when we depoly an API, we sort of want to test a little bit, the improvements of ur API before going
 all in and this is called canary deployement     - @@@
    - Canary deployements can be used at any stage but usally it is done in production stage
    - u choose the % of traffic the canary channel receives
   
   Scenerio -> we have v1 and v2 version of our API so client maybe redirect 95% traffic to v1 API and 5% to our canary channel and we are
   able to have logs and metrics seperate, so using these logs and metrics we can ensure that our API gateway is not happening any error
   responses and every thing works correctly in v2 API and when we r ready we can shift all traffic to v2   - @@@
   it is possible for u in canary stage to override stage variables  - @@@
   So this is kind of blue/green deployement with AWS lambda and API gateway        - @@@
   
   Hands oN-> First deploy API after it go to the stages tab then go to the Stage Variables tab and add the alias and value of it then go
   to canary tab for creating canary stage
 
 API Gateway Mappings ->
    - Mapping templates can be used to modify request and responses
    - we can do :
        - Rename Parameters
        - Modify body content
        - add headers
        - Map JSON to XML for sending to backend or back to client
        - for this we can use language VTL(Velocity Template Langauge): for loop, if statement etc      - @@@
        - filter output results and remove unnecessary data
      - we can use XML and JSON both in Mapping Template        - @@@
 
API Gateway Swagger/Open API Spec -> it is a common way of defining REST API's and use API as code
    - To API Gateway we can import an Existing Swagger/OpenAPI 3.0 spec to APIGateway, and API gateway will generate an API for us
        That include -
            - Method
            - Method Request
            - Integration Request
            - Method Response
            - AWS extentions for API gateway so u can setup every sigle option with ur API gateway using these extentions
    - U can export current API as Swagger/Open API spec     - @@@
    - Swagger can be written in YAML or JSON            - @@@
    - Using Swagger we can also generate SDK for our applications
    
    Hands On -> go to the stage then got to export tab then choose which one u wanna use Swagger or Open API 3 and then clicking
    on JSON or YAML u will see whole script file ur API reperesents same way like in Cloudformation where we have our infrastructure as a
    code
    U can use swagger to create API at the time of creating an API using the Import from Swagger option
    
API Gateway Caching-> caching reduce the no. of calls made to the backend
    - the default time to live in a cache 300 sec and u can set between 0 sec and 3600 sec      - @@@
    - cache can be defined per stage so we can have dev cache , prod cache etc      - @@@
    - u can encrypt ur cache    - @@@
    - u can have the size of cache between 0.5 GB to 237 GB     - @@@
    - their is possibility to override cache settings for specific methods// so u get control over which method should be cached and which
    shouldn't.  - @@@
    - u r able to flush entire cache to invalidate immediately from the console or using an API call        - @@@
    Question -> can clients invalidate the cache or bypass the cache?       - @@@
    ANSWER-> Yes, if they r authorized to do so, client can use the header "Cache-Control: max-age=0" and by this they can bypass the cache
    and the client get the data straight from the backend
    
    HandsOn -> Go to stages the in settings click on the option Enable API cache and then select the cache capacity and decide for the
    cache time to live(TTL) and configure other settings
    
 API Gateway - Logging,Monitoring, Tracing -> It is integrated with cloudwatch logs but we need to enable it
      - CloudWatch Logs:
          - Enable cloudwatch logging at the stage level
          - we can override its settings on per API basis(ex: Error logs,Debug logs, info logs etc)
          - log can contain information about the request and response body
      - Cloudwatch metrics:
          - Metrics are by stage
          - we can ebale detail metrics to get even more metrics
      - X-Ray:
          - We can enable tracing to get extra information about requests in API Gateway to lambda function to dynamoDB for example
          - API gateway + AWS lambda tracing gives u really the full picture about how requests go through ur entire system
          
     HnadsOn -> On the stages got to Log/Tracing tab and select the option "enable tracing logs" and then select log level or
     "enable detailed cloudwatch metrics" or both as u like and other options as u like and provide IAM role as required
     
 API Gateway - CORS(Cross Origin Resource Sharing) -> CORS must be enabled when u receive API calls from another domain
      - For it u need to enable the OPTIONS pre-flight request and they must contain following headers:     - @@@
          - Access-Control-Allow-Methods
          - Access-Control-Allow-Headers
          - Access-Control-Allow-Origin
       - CORS can be enabled through AWS console

API Gateway - Usage Plans & API keys ->
      Question - What if u want to limit ur customers usage of API?     - @@@
      Answer -> - U can have Usage plans such as:
                    - Throttling: set overall capacity and burst Capacity
                    - Quotas: number of requests made per day/week/month
                    - U can associate these usage plans with desired API stages
                - API Keys: // when u have usage plan then u need to associate it with a API key
                    - U would generate one key per customer
                    - Associate these keys with usage plans
                - U r able to track usage for API keys and bill ur clients for these API keys
                
 API Gateway Security -> There r three aspects of security:
      - IAM Permissions : create an IAM ploicy authorization and attach to User/Role 
          - API Gateway verifies IAM permissions passed but the calling application
          - it's good to provide access within ur own infrastructure
          - it leverages "Sig v4" capability where IAM credentials are in headers and header is passed to API Gateway      - @@@
          - their is no added cost to this
      - Lambda Authorizer(formerly known as Custom Authorizers) : 
          - Uses lambda to validate the token in header being passed
          - option to cache the result of authentication // u don't need to call custom authorizer every time request came      - @@@
          - it is used when u have some 3rd party authentication such as Oauth, SAML etc
          - lambda must return an IAM policy for the user
      - Cognito User Pools
          - Cognito will manage full user lifecycle
          - API gateway verifies identity automatically from cognito
          - No need to implement any custom lambda function
          - Cognito only helps with authentication not authorization
          
      Summary ->
        - IAM: It is for when u have users or rolls already within ur AWS account
              - Handle authorization and authentication through IAM policies
              - it leverages Sig v4
        - Custom Authorizer: 
              - it's great for 3rd party tokens
              - u r flexible of what IAM policy is going to be returned
              - U can handle Authentication and Authorization
              - u pay per lambda invocation // but u can use caching to limit the no. of calls u do to ur lambda function for authorizing
        - Cognito User Pool:
              - u manage ur own user pool(can be backed by Facebook,Google login etc)
              - No need to write any custom code
              - u must implement the authorization layer in the backend
              
 AWS Cognito Overview : it is used when we want to give our users an identity so they can interact with our application
    Cognito User Pools:
        - Sign In functionality for app users
        - Integrated with API gateway
    Cognito Identity Pools(Federated Identity):         - @@@
        - this is to provide AWS credentials directly to our App users so they can access AWS resources directly
        - it also has integration with cognito user pools as an identity provider
    Cognito Sync:
        - Synchronize data from device to cognito
        - it is may be deprecated and replaced by AppSync
 
  AWS Cognito User Pools(CUP):
    - this creates a serverless database of user for ur mobile apps
    - Simple Login: Username(or email)/ password combination
    - U can verfiy emails and phone no. and u can add MFA(Multi Factor Authentication) , u can have password policies
    - u can enable Federated Identities(Facebook, SAML, Google ...)
    - Sends back a JSON Web Token (JWT) // it is used to verify the identity of someone.        - @@@
    - It can be integrated with API Gateway for the Authentication
  
  AWS Cognito Identity Pools(Federated Identity):       - @@@
    - its goal is to provide direct access to our AWS resources from the client side // no proxy, no API just straight access
    - we do this by login into federated identity provider or we can remain anonymous and we get temporary AWS credentials back from
    the Federated Identity Pool
    - These credentials come with a pre-defined IAM policy stating their permissions
    
    Scenerio -> App goes to Identity Provider(Google, SAML) and receives a token then app pass the token to AWS Federated Identity then that
    token is again send to Identity Provider to verify and then the response for the verfication came back whether user is valid or not,Once
    the token is verified the Federated Identity talks to STS service to get temporary credentials for AWS, Once it has that it will pass
     on the temporary credentials back to ur application and once ur application has these temporary AWS credentials it is For example able
     to interact directly with our S3 bucket and we have IAM policy which allows to do certain things and not do other things   - @@@
    
 AWS Cognito Sync:  - @@@
      - it is depreicated and we use AWS AppSync now
      - it is used to store user prefernces,configuration and state of our application  - @@@
      - it has cross device synchronization capabiltiy(any platform ios, android etc)   - @@@
      - it has offline capability (if u change ur prefernces offline and then u go online then they are synchronized automatically) - @@@
      - for working well u need to use Federated Identity Pools in Cognito not User Pools
      - the data is stored in datasets(and each dataset can be upto 1mb)  - @@@
      - we can have upto 20 datasets to synchronize     - @@@
      
Questions -> You would like to authorize an EC2 instance to use your internal API. You should use
Answer -> IAM permissions

Question -> You would like to provide a Facebook login before your users call your API hosted by API Gateway.
You need seamlessly authentication integration,you will use     - @@@
Answer -> Cognito User Pools

#5-> AWS Serverless Application Model (SAM)
    - It is a serverless development framework which allows u to deploy ur service applications onto AWS.
    - all the configurations is in YAML code
    - but it is not simple YAML code, it's actually cloudformation code, so simple YAML code in the SAM format  - @@@
    - so it generate complex cloudformation from simple SAM YAML file
    - SAM supports anything that cloudformation supports        - @@@
    - it's only two cmds to deploy to AWS
    - SAM can use code deploy to deploy lambda functions - @@@
    - SAM can help u to run lambda,API Gateway, DynamoDB locally // so we can run entire stack straight from our computer
    
    How does SAM work?      - @@@
    their is 'Transform' Header that indicates it is SAM template as 
        - Transform: 'AWS::Serverless-2016-10-31'
    u can write ur code ,u can use 3 resources that are specially use by SAM format
        - AWS::Serverless::Function  // this is for lambda
        - AWS::Serverless::Api  // this is for API gateway
        - AWS::Serverless::SimpleTable  // this is for DynamoDB
    for package and deploy we can use aws coudformation package as a cli or SAM package if u use SAM cli // in exam it asks about
    cloudformation package - @@@
    after u package ur application u deploy it using aws cloudformation deploy / SAM deploy     - @@@
    
  Deep Dive into SAM deployement -
   how cloud formation package work?
   we use first cmd -> aws cloudformation package - @@@
   we have our SAM template YAML file and we also have our application code and u can also have swagger file for ur API gateway and by
   using aws cloudformation package it will upload the zip file of our code straight to the S3 bucket, so in s3 bucket we will have all
   our deployements and the cloudformation package will also transform our SAM template into cloudformation template , So this
   'transform' header indicates how cloudformation should transform their templates into much more complex cloudformation YAMLs,
   And we uploaded our code to s3 bcz this generated templates will have a reference to the uploaded code in S3
   then we use our second cmd -> aws cloudformation deploy - @@@
   this will do the deployement of our generated template, it will in particular create and exchange a change set, So change set is
    basically figuring out how cloudformation should take it's existing state and move to next state based on the modifications we
    have generated,So we have our chain stack and cloudformation will apply it to our stack comprised of things such as API gateway,
    lambda, IAM policies etc
   to see the whole diagram go to link ->
   https://www.udemy.com/aws-certified-developer-associate-dva-c01/learn/v4/t/lecture/11851436?start=0
   
  Installing the SAM cli->
    - google aws-sam-cli to go to it's github page for documentation
    - then follow the instructions to install according to ur OS
    
  Creating the SAM project ->
     - use cmd-> sam init // to generate a SAM project for u
     - or don't use sam init and go one by one to create all the files:     - @@@
        - create src folder and then create app.py file in it       - @@@
        - create template.yaml file outside of src folder which is going to be our SAM file
        - create commands.sh file outside of src folder         - @@@
        - in app.py file and write code their for lambda // google serverless-application-model example apps for getting sample apps
        - in template.yaml file write the code according to ur needs    - @@@
        - in commands.sh write code:
        aws s3 mb s3://bucket_name // to create bucket
        aws cloudformation package help // to know about parameter for this cmd
        aws cloudformation package --s3-bucket bucket_name --template-file template.yml --output-template-file gen/template-generated.yml //this is
        to create our generated file and 'gen' is the diractory to put the file and  template-generated.yml is the file name and this will
        upload code to S3
        to use 'sam' cmd we just replace 'aws cloudformation' with 'sam'
        aws cloudformation deploy --template-file gen/template-generated.yml --stack-name name_of_the_stack --capabilities CAPABILITY_IAM
         // to deploy file and this
        will basically create a chain set and --capabilities CAPABILITY_IAM is use to provide proper IAM permissions
          
      SAM with API Gateway -> https://www.udemy.com/aws-certified-developer-associate-dva-c01/learn/v4/t/lecture/11937084?start=0
      SAM with API DynamoDB -> https://www.udemy.com/aws-certified-developer-associate-dva-c01/learn/v4/t/lecture/11937086?start=0
      
      SAM CloudFormation Designer and Application Repository -> it shows how th template is created and the working is automated to
      see that go to link -> https://www.udemy.com/aws-certified-developer-associate-dva-c01/learn/v4/t/lecture/11937088?start=0
      
      u can use the option "AWS Serverless Applicaion Repository" at the time of creating a new lambda function which contains bunch of
      SAM templates already created. - @@@

      Note - SAM also uses the cmds cloudformation package and cloudformation deploy
#6-> Encryption 101->
  -Encryption in Flight(SSL) -> The data is encrypted before sending and decrypted after receiving,but only server and sender knows how
  to do this
      - SSL certificates help with Encryption(HTTPS)
      - Encryption in flight ensures that no MITM(Man in the Middle) attack can happen
      
  - Serverside encryption at rest -> the data is encrypted after being received by the server, So in the case the server get hijacked the 
  someone else not be able to read the data
      - Data is being decrypted before being sent back to our client
      - it is stored in an encrypted form using a key(usally a data key)
      - the encryption/decryption keys must be managed somewhere usally called KMS(key Management Service) and the server must have right
       to access that key
      
  - Client side encryption -> data is encrypted by the client and never decrypted by the server
        - data will be decrypted by the receiving client
        - the server should never be able to decrypt the data 
        - for this we can leverage Envelope Encryption
        
  KMS Overview ->
     - anytime u hear about encryption for an AWS service, it's most likely KMS
     - KMS is an easy way to control access to ur data, data is gonna encrypted by keys and AWS KMS will manage this key for us
     - So KMS is a store we have some control over it but somethings we cannot do with this store
     - KMS is fully integrated with IAM for authorization // So basically u r saying that this application can access this key through
      IAM and encrypt and decrypt this data
     - KMS is fully integrated with many AWS services such as
        - EBS encrypted volume
        - S3 to do server side encryption of objects
        - Redshift for encryption of data
        - RDS for encryption of data
        - SSM for Parameter Store etc
     - we can also use the CLI/SDK to perform encryption on our side
     
    KMS 101 ->
      - Anytime u need to share sensitive information with someone within application use KMS such as:
          - Database passwords
          - Credentials to external Service
          - Private key of SSL certificate
      - the value in KMS is data the CMK(customer master key) it will be used to encrypt ur data and u can never retrieve it and see what
       it is and CMK is managed by KMS
      - KMS can rotate ur CMK and make it quite often
      - So u r going to use KMS to encrypt and decrypt data and u never get to see the key itself
      - never store ur secrets in plaintext especially in ur code
      - so beforehand to KMS to encrypt them and then ur encrypted secrets can be stored in ur code or in environment variables and no
       one can decrypt the secrets unless they have access through IAM to decrypt it with KMS
      - KMS can only help in encrypting upto 4kb of data per call       - @@@
@@@   - If u wanna give access to KMS to someone, u make sure that the key policy allows the user and the IAM policy allows the API call.

      Question -> we want to encrypt 1mb data, how we do it?    - @@@
      Answer-> if ur data is more then 4kb we can still use KMS, but we have to use a technique called envelope encryption

      - So KMS gives us the ability to manage the keys and policies
          - We can create keys
          - We can rotate keys
          - We can disable keys
          - We can enable keys
          So we get the full capability to manage keys but we never see these keys themselves
      - We can audit the key usage using cloudtrail - @@@
      - there r 3 types of CMK's in KMS   - @@@
          - AWS Managed Service Default CMK: it's a free key and basically anytime u want to encrypt EBS using the default key then this 
            is the key
          - User keys created in KMS: it cost $1 per month
          - User keys imported(must be 256-bit symmetric key): it cost $1 per month, u can put these keys in KMS storage
      - every time u call KMS for doing encryption/decryption or any api call u pay $0.03 for 10,000 calls      - @@@
      
    - How does KMS works?
    - KMS is multitude of APIs first two api's r encrypt and decrypt api
    Scenerio-> client is going to use cli or SDK has a secret suppose password and it is less then 4kb then we can use encrypt and
    decrypt api so suppose encrypt api call is done to KMS and send the secret to KMS and define which CMK we wanna use then KMS will
     first check the clients IAM permissions and after verifying the KMS within service will perform encryption and as a response send
      the encrypted secret data and same process will be for decryption.
    
    KMS and Lambda Practice ->
      - Go to IAM dashboard and then got to Encryption Keys dashboard then choose ur region first and u will see the the list of keys already
      being used by u in other services if u do encryption then click on creat key if u wanted and then provide details and selct the type
      of key u want KMS or External then define permissions for the key to be able to use it which could be user or any role and then u will
      be provided by key policy which tells what and who can perform actions and then Finish 
      Now after creation u can click on the key and u can view key features and u can later add external accounts that can use that key and
      u can also enable key rotation
      - Now go tot Lambda dashboard and create a lambda function and choose custom role for it and choose the role if u have already created
      for it or edit it later after it go to environment variable and write the key and value in it and then go to the 'encryption configuration'
      option and selct the option "Enable helpers for encryption in transit" and then u will get the option to encrypt it along side of ur
      environment variable and in the 'encryption configuration' u also get the option of 'AWS KMS key to encrypt at rest' and u can choose
      between default and use a CMK so choose this option and selct ur created key and then click on encrypt option aside the environment
      variable u want to encrypt , if it is not getting encrypted then that means that the lambda function does not have access to use this
      CMK, So go to IAM dashboard then to encryption keys dashboard and then to key users section and then click on Add button then choose
      the IAM role of ur lambda function and after this go to ur role and selct the role of ur lambda and then click on Add inline policy
       and as a service choose KMS and selct the things the type of actions u wants to be allowed and select the resource type as specific
       and then in add arn give the alias name and then select the availabilty of it region wise and account wise and then review policy and
       then create policy and then go back to lambda and re-enable the "Enable helpers for encryption in transit" and now click on Encrypt
       button on the Enviroment variable and we also get the code button their and this code is going to help us to decrypt the secrets 
       so copy the code their and paste it in ur lambda code
       Note: use the decryption code outisde of the lambda handler so that we don't have to decrypt every time pur function gets called
       
  Encryption SDK Overview ->
          - What if u want to encrypt over 4kb using KMS?
          - we need to use Envelope Encryption, the aws encryption sdk help us use Envelope encryption with some API's
          Note: it is very deifferent from S3 Encryption SDK
          - the encryption SDK also exists as a CLI tool we can install
@@@       - Encryption SDK also means Envelope Encryption and the api call that will be done by the envelope encryption is Generate Data Key API
          
      - Deep Dive into Envelope Encryption(GenerateDataKey API) ->
          - So we have our clients in cli and sdk and we have a big file for example lets just say 10mb and we want to encrypt that using
          KMS, So first thing we want to do is to call KMS with an API called the GenerateDataKey       - @@@
          GenerateDataKey is an API call in which u have to specify a CMK in KMS, so u can use CMK for example and say that we like to
          generate a data key for us using CMK, So KMS will perform IAM permissions to make sure that we do have access to the CMK key and
          make sure we do can do GenerateDataKey API if it verifies then it will generate a data key for u and encrypt that data key with CMK
          ,So here it did two things it generate the data key and encrypt the data key with CMK - @@@
          The data key is a plain text and it is send back to the client and it will also send back the encrypted data key So now we have plain
          text data key and encrypted data key
          Now using the plain text data key we r able perform client side encryption and using it we can get the encrypted file and then we
          delete the plain text data key , So now we have only encrypted data key and encrypted file and So because their is two level of 
          encryption like one encryption into another encryption that is why it is called envelop encryption
          
       - Deep Dive into Envelope Encryption(Decrypt envelope data) -> so at the client side we have encrypted DEK and encrypted file inside
       envlope file So now we make a decrypt api call to KMS , So KMS will look into the CMK and check the IAM permission and then it will
       decrypt the file sent to it using CMK then in response to client send plaintext data key and after getting the plaintext data key back
       we are able to perform client side decryption using the plain text encryption key and we get back the plain file.So we can see the
       decryption always happens the client side as well
       
     Encryption SDK cli Hands on ->
        - So if u google AWS Encryption CLI then we will see how to install it after installing it in documentation
        - got to examples secion in the documentation so we will see u need to export cmkARN and then use cmd -> aws-encryption-cli and other
        parameters of it to encrypt the file and decrypt the file
        - So on terminal ->
            key="arn_of_ur_key"
            vi hello.txt // put data into it of 1mb
            mkdir output
            aws-encryption-cli --encrypt --input hello.txt --master-keys = $key --metadata-output metadata/ --output output/  // encryption is done
            now if u go to the output directory u will see file as hello.txt.encrypted
            cat output/hello.txt.encrypted // we will see unreadble data in it
            mkdir decrypted
            aws-encryption-cli --decrypt --input output/hello.txt.encrypted --metadata-output metadata/ --output decrypted/
            now if we look into that decrypted directory we will have file hello.txt.encrypted.decrypted
            
AWS SSM Parameter Store -> it's a way to securely store ur configuration and secrets    - @@@
    - u have option to use Seamless Encryption using KMS // so u basically use KMS to encrypt   - @@@
    - So  Parameter store is a gud way to store database passwords  - @@@
    - it's serverless(u don't need to manage any server), it's scalable(u can have many parameters), it's durable(u don't need to worry
    about the parameters going away), it's free and there is really easy SDK to use it  - @@@
    - u r able to do version tracking of configurations and secrets // so u can check the old secrets too   - @@@
    - U can get all configuration through IAM and all the secrets r under path // means using IAM u can restrict who can view which
    database passwords and their is also KMS integration, so u also need to define KMS policies to allow someone to decrypt a secret
    - u can get notified of anything happening with cloud watch events // such as secrets changing or parameter changing  - @@@
    - their is integration with cloudformation for the case if u need to have Parameter store into ur cloudformation template  - @@@
    
    Scenerio -> So ur application(EC2/lambda/cli anything) interacts with the Parameter Store to retrive the parameters, So for this we
    can use the SDK and the AWS parameter store will send u a plain text configuration and now the parameter store will check with IAM
    that whether the right to access parameter and configuration is available or not. and if it's plain text just fine it will send it
    back.       - @@@
    but the second use case is to use encrypted configuration so in same way the we will ask Parameter store that we want this encrypted
    configuration and then check IAM permissions and if Okay it will also call the decryption service on KMS side So KMS will be use
    to decrypt it and if everything is fine will send us back configuration, So we don't need to directly interact with KMS to retrive
    some encrypted data, We just interact with SSM and the decryption happens for us    - @@@
    
    AWS Parameter Store Hierachy ->         - @@@
      /my-department/
        my-app/
          dev/                  // dev environment
            db-url              // database url
            db-password         // database password
          prod/                 // prod environment
            db-url              // database url
            db-password         // database password
         other-app/
       /other-department/
       
       It depends on u how u define the hierachy        - @@@
       
       Scenerio -> Suppose we have dev lambda function and we want to get Dev parameters then we will use the GetParameters or
       GetParametersByPath API and we would ask directly for Dev parameters and if we had Prod lambda Function it would automatically
       ask for prod parameters
       So where we can differentiate different parameters for different environments So this is very gud and helpful
       U just need to remember that the stores are hierarchy and that u get encrypted and plain text parameters and that u have
       GetParameters or GetParametersByPath API         - @@@
    
   SSM Parameter store hands on (CLI) -> Go to the EC2 dashboard then go to the dashboard Parameter Store // this is one way to use
   - the other way is to Go to System Manager Dashboard and then go to Parameter Store Dashboard then click on create parameter
   and then write the name as the hierarchy Such as '/my-app/dev/db-url' then select the type (String,StringList,SecureString) and then
   put the value in value textbox and then create the parameter
   Go to link for whole process -> https://www.udemy.com/aws-certified-developer-associate-dva-c01/learn/v4/t/lecture/11851512?start=0
   
   SSM Parameter store hands on(Lambda) -> https://www.udemy.com/aws-certified-developer-associate-dva-c01/learn/v4/t/lecture/11851458?start=0
   
   IAM Best Practices and STS -> 
      - Never use Root Credentials, enable MFA for Root Account
      - Grant Least Privilege to Role or User
          - Each Group/User/Role should only have the minimum level of permission it needs
          - Never grant a policy with "*" access to service
          - Monitor api calls made by a user in Cloudtrail(especially Denied ones)
      - Never store IAM key credentials on any machine but ur personal computer or on-premise server        - @@@
      - On premise server best practice is to call STS to obtain temporary security credentials     - @@@
      
      IAM Roles->
        - EC2 machines should have their own roles      - @@@
        - lambda functions should have their own roles      - @@@
        - ECS tasks should have their own roles(For this it's just a environment role = ECS_ENABLE_TASK_IAM_ROLE = true)    - @@@
        - Code Build should have its own service role       - @@@
        - u should create a least privileged role for any service that requires it
        - create a role per application/lambda function(do not reuse roles)     - @@@
        
      IAM Cross Account Access ->       - @@@
        - Define an IAM role for another account to access
        - Define which accounts can access this IAM role
@@@     - Use AWS STS(Security Token Service) to retrieve credentials and impersonate the IAM role u have access to(AssumeRole API)
        - Temporary credentials can be valid between 15 min to 1hr          - @@@
        Scenerio -> user wants to access a role may be in same account or another AWS account So u make AssumeRole api call to STS, then
        STS is going to check with IAM if the Permissions r correct,if it is then it is going to give u temporary credentials, and these
        credetials r going to give u the ability to do what the role u want to endorse.         - @@@

#7->   AWS CloudFront -> it is a CDN(Content Delivery Network)          - @@@
      - it improves the read performance, because the content u r going to use is going to get cached at edge // edge r going to be location
      around the world      - @@@
      - U have 136 point of presence globally(edge locations) // means ur content will be delivered to 136 caches around the world that's
      why it is called Content Delivery Network         - @@@
      - It's popular with S3 bcz S3 buckets r created in one region and u r going to distribute the content of the S3 bucket over the
      world using CloudFront, It also works with EC2 with load balancing - @@@
      - It can also help u protect against network attacks like DDoS,it has SSL encryption capabilities at the edge using ACM which is
      the certificate manager by Amazon     - @@@
      - U can have HTTPS through cloudfront, it can also talk to ur application using SSL or HTTPS, So it's fully secure for inflight 
      encryption
      - It supports video and media using RTMP protocol - @@@
      
  AWS Step Functions ->
      - it is a way to build a serverless visual workflow to orchestrate ur lambda functions
@@@   - All the flow is represented as JSON state machine // most of the time state machine is used for Step functions remember for exam
      - U can use Sequence of lambda functions, parallel, conditions, timeouts, error handling      - @@@
      - u can also integrate with other services such as EC2, ECS, On Premise Servers, API Gateway      - @@@
      - Maximum Execution time is 1 year        - @@@
      - u have possibility to implement Human Approval Feature          - @@@
      - use cases:
          - Order Fulfillment
          - Data Processing
          - Web Applications
          - or any workflow
          
          So in exam anytime it say about workflow, lambda orchestrations, state machines then it is for Step Functions     - @@@
          
  Visual Workflow in Step Functions ->
      - U r going to create a JSON state machine and it is going to give u a flow chart
      - and u can see what thing happens such as which succed , fail or got cancelled etc
      
 AWS SWF - Simple WorkFlow Service ->
    - It co-ordinates work amongst applications         - @@@
    - the code runs on EC2 (not Serverless)         - @@@
    - their is 1 year max runtime           - @@@
    - their is concept of 'Activity Step' and 'Decision Step'
    - Has built in 'human intervention' step
    - it has order fullfillment from web to warehouse to delivery
    - SWF is older and it is pretty much not supported by AWS that much except if u need external signals to intervene in the process
      and if u need child processes that return values to parent processes      - @@@
      
AWS ECS ->
    - It is a container orchestration service
    - It helps u to run docker containers on EC2 machines      - @@@
    - ECS is made of:
        - ECS Core(not real word): in the ECS, where u run docker container and ECS on user-provisioned EC2 instances // here u
        provisioned ur own server
        - Fargate: u run ECS on AWS provisioned compute this time, it's more serverless // u run it on Amazon Servers       - @@@
        - EKS: Running ECS on AWS powered kubernetes        - @@@
        - ECR: it is the docker container registry hosted by AWS
    - In exam when ever u see microservice it is either for ECS or Lambda
    - their is IAM security and roles are at the ECS task level
    
    Docker:  it is a container technology and u basically run a containerized application on any machine that has Docker installed,
     So first           - @@@
    u containerize ur application and then it can run anywhere
        - So container allows ur application to work the same way anywhere  - @@@
        - containers r isolated from each other on machine          - @@@
        - control how much memory and CPU is allocated to ur container      - @@@
        - u have ability to restrict network rules // so how container communicates with one another        - @@@
        - it is more efficient then virtual machines        - @@@
        - U can scale containers up and down very quickly(in matter of seconds)
        
      UseCases for ECS:
        - Run Microservices:    - @@@
          - Ability to run multiple containers on same machine      - @@@
          - u can do service discovery feature to enhance communication     - @@@
          - u have direct integration with application load balancer    - @@@
          - u have auto scaling capability      - @@@
        - u can run batch processing and schedule tasks     - @@@
          - Schedule ECS containers to run on-demand/Reserved/Spot instances for EC2 instances
        - Migrate applications to the cloud     - @@@
          - Dockerize legacy applications running on premise
          - Move Docker containers to run on ECS
          
    AWS ECS Concept ->
      - u r going to run suppose 3 ec2 instances and then u define a ECS cluster and ur ECS cluster is going to be the set of these
      instances and then on top of it u r going to define ECS Services, Services is basically an application,So it's an application
      defination and u r going to be running on ur ECS cluster and here u basically says how many applications u r running and
      the auto-scaling rules etc and then ur ECS service is going to create ECS task ,So task is going to be a docker container
      running on ur EC2 machine and they can be running on any machine of that cluster, So u can have 4 or 6 task container running
      of 3 ec2 machines,So it is possible to for the same service to run multiple times on the same EC2 machine and that's provided
      by as a capability by Docker,      - @@@
      U can run Another ECS service also inside same cluster and u don't need to know that it runs across all ur EC2 machines and
      each task container can have IAM role ,So each task container can be isolated and secure and have its own set of permissions   - @@@

      SERVICE On the -> ECS cluster set of instances (ECS agent with ECS config file in instnace) -> then service creates tasks

  AWS ECS - ALB integration ->
     - ALB(Application Load Balancer) has a direct integration feature with ECS called 'port mapping', this is new feature of ALB the 
       classic load balancer does not have this feature - @@@
     - this allows u to run multiple instances of the same application on the same EC2 machine      - @@@
     - Use Cases:
        - Increased resiliency even if running on one EC2 instance
        - Maximise utilization of CPU and cores     - @@@
        - u have ability to perform rolling upgrades without impacting application uptime       - @@@
        
     So basically we can run four containers they all run on nodejs running on one ec2 instance and we have 4 ports, they all
     different and they r all dynamic and the ALB is before the ec2 machine is exposed on port 80 for HTTP or 443 for HTTPS and
     because of this dynamic port mapping feature, the ALB itself will redirect the traffic to each container with the right port
     So with this architecture u r able to run the same application, the same container many times over the same ECS instance   - @@@
     
 AWS ECS- ECS Setup & Config file ->
- for this u run an EC2 instance and u will install the ECS agent with ECS config file      - @@@
- if u use an ECS ready Linux AMI u already have ECS agent installed on it and running but u will still need to modify the config file - @@@
- ECS config file location is at /etc/ecs/ecs.config  - @@@
- their r 4 configs u need to know about and their r about 35 configs for this file     - @@@
      - ECS_CLUSTER = MyCluster # Assign EC2 instance to an ECS cluster
      - ECS_ENGINE_AUTH_DATA = {...} # to pull images from private registeries
      - ECS_AVAILABLE_LOGGING_DRIVERS = [...] # cloudwatch container logging
      - ECS_ENABLE_TASK_IAM_ROLE = true # Enable IAM roles for ECS tasks

      Note: Remember the above configuration tasks performed in sequence  - @@@
          
  AWS ECR - Elastic Container Registry -> - @@@
    - In ECR the docker containers r stored before they run
    - it is fully integrated with ECS and IAM
    - the data is sent over HTTPS(encryption in flight) and encrypted at rest
    - for containers to arrive at ECR u need to push them and u can use cli or code build for ur CICD to automate this task
    - and ECS request pull of container form ECR and also check with IAM whether u have permissions for it or not
    
 AWS SES(Simple Email Service) ->
    - u can send emails to people using:
       - SMTP interface         - @@@
       - or AWS SDK
    - u r also able to receive emails so u can integrate with S3, SNS or Lambda
    - and to send or receive emails u can have IAM permissions
    
AWS Databases Summary ->        - @@@
  - RDS is in and out is for relational database, it's for transactional processing, so OLTP and u have
      - PostgreSQL, MySql, Oracle...
      - Aurora + AuroraServerless
      - it's going to be Provisioned database
  - DynamoDB: NoSql DB
      - it is managed, key value, document store
      - Serverless
  - ElasticCache: it's an in memory DB
      - Redis / Memcached // it's great to store in memory 
      - cache capacity
  - RedShift: OLAP // OLAP stands for Analytic Processing
      - Data warehousing/ Data Lake
      - Analytics queries
  - Neptune: Graph Database
  - DMS: Database Migration Service // it is amazon service to quickly enable u to load data to any of the above databases

AWS OpsWorks -> AWS OpsWorks is a configuration management service that helps you configure and operate applications in a cloud enterprise
 by using Puppet or Chef. AWS OpsWorks Stacks and AWS OpsWorks for Chef Automate let you use Chef cookbooks and solutions for configuration
 management, while OpsWorks for Puppet Enterprise lets you configure a Puppet Enterprise master server in AWS. Puppet offers a set of tools
 for enforcing the desired state of your infrastructure, and automating on-demand tasks.

 AWS OpsWorks Stacks monitors instance health, and provisions new instances for you, when necessary, by using Auto Healing and Auto Scaling.

 AWS OpsWorks Stacks, the original service, provides a simple and flexible way to create and manage stacks and applications. AWS OpsWorks
 Stacks lets you deploy and monitor applications in your stacks. You can create stacks that help you manage cloud resources in specialized
 groups called layers. A layer represents a set of EC2 instances that serve a particular purpose, such as serving applications or hosting
 a database server.

Question -> In order to retrieve the region ID of an EC2 instance created with CloudFormation, which function should you use?
Answer -> !GetAtt
//The Fn::GetAtt intrinsic function returns the value of an attribute from a resource in the template.
We can use the above function to get the regions ID attribute of the required EC2 instance by passing region ID as the attributeName and
EC2 instance ID as logicalNameOfResource.

Question -> Your Elastic Beanstalk application must encrypt payloads of up to 10MB. Which method will help you achieve that?    - @@@
Answer -> S3 Encrypt API call
//The AWS Encryption SDK is an encryption library that helps make it easier for you to implement encryption best practices in
your application.

Question -> What is the most simple way to run a CLI command against a region that hasn't been configured as the default?
Answer -> use the --region parameter
//If the region parameter is not set, then the CLI command is executed against the default AWS region.

Question -> Which pseudo parameter can you use to make your CloudFormation independent of the accounts they're running under?
Answer -> AWS::AccountId
//AWS::AccountId returns the AWS account ID of the account in which the stack is being created

Question -> Which of the following integration technology pushes data to consumers?
Answer -> SNS //SNS pushes while others pull

Question -> You are looking to push Docker images into ECR with your AWS CodePipeline and CodeBuild.
 The last step fails with an authorization issue. What is the issue?
Answer -> Double Check ur IAM permissions for CodeBuild service
//By default, IAM users don't have permission to create or modify Amazon ECR resources, or perform tasks using the Amazon ECR API.

Question -> You want to provision your own Docker images that can be used as an input for CodeBuild.
 These images will contain cached dependencies as well as special tooling for your builds that are proprietary to your company.
 Where should you push these images?    - @@@
Answer -> ECR
//Amazon Elastic Container Registry (ECR) is a fully-managed Docker container registry that makes it easy for developers to store,
 manage, and deploy Docker container images.

Question -> You would like to implement a 3rd party authentication for your AWS API Gateway. Which authorizer do you choose?
Answer -> Lambda Authorizer
//An Amazon API Gateway Lambda authorizer is a Lambda function that you provide to control access to your API methods.
A Lambda authorizer uses bearer token authentication strategies, such as OAuth or SAML.
 It can also use information described by headers, paths, query strings, stage variables, or context variables request parameters.

Question -> CodePipeline fails with an authorisation issue. What is the best way to determine the root cause of the problem?
Answer -> Check the service IAM permissions
//Any authorization related failure directly indicates to us that our first point of debugging should be validating the IAM permissions
 for the service.   -   @@@

Question -> You are looking to automatically deploy AWS Lambda functions, with precise deployment rollout strategies.
 Which AWS service should you use?
Answer -> CodeDeploy
//You can deploy a nearly unlimited variety of application content, such as code, serverless AWS Lambda functions, web and configuration
 files, executables, packages, scripts, multimedia files, and so on.

Question -> Should non exported CloudFormation outputs must a region-level unique name?
Answer -> NO
//Only the exported CloudFormation, Export names must be unique within a region for each AWS account

Question -> You would like to have a secured and versioned way of storing your database credentials. Which service do you pick?
Answer -> SSM Parameter Store
//AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management.
You can store data such as passwords, database strings, and license codes as parameter values.

Question -> You have deployed a Java application on EC2 and made sure to enable the X-Ray SDK.The application was sending data to
X-Ray when you tested it from your personal computer. The deployed application fails to send data to X-Ray.
 Which of the following component debugging won't help resolve the problem? - @@@
Answer -> X-Ray Sampling
// The X-Ray SDK applies a sampling algorithm to efficiently trace and provide a representative sample of the requests that your
 application serves. This can be utilised post the data is being successfully being sent to X-Ray and in no way helps us in determining
 the cause of failure to send the data to X-Ray.

Question -> Which protocol is not supported by CloudFront?
Answer -> UDP

Question -> You would like to query items based on an attribute that is not part of the DynamoDB partition key. What should you do?
Answer -> Create a GSI  - @@@
// Some applications might need to perform many kinds of queries, using a variety of different attributes other than the specified
partition key as query criteria. To support these requirements, you can create one or more global secondary indexes and issue Query
 requests against these indexes.

Question -> You want to deploy an application that relies on RDS and ElastiCache for storing data and user session data respectively.
 You have chosen Elastic Beanstalk as the platform to deploy the platform. The requirements are that the RDS database must be able
 to be used across different environments and never lost, while ElastiCache can be safely deployed for each environment and lost if an
  environment is destroyed.
Which configuration will allow you to achieve that? (select two)
Answer -> Elasticache defined in ./ebextentions
RDS Database defined externally and referenced through environment variables
//Any resources created as part of your .ebextensions is part of your CloudFormation template and will get deleted if the environment
 is terminated and the Resources that need to persist environments deletions must be created externally

Question -> Your client is migrating an application to the cloud and needs a technology to ensure your application can be stateless and
 store data in a low latency in memory store. What do you recommend?    - @@@
Answer -> ElasticCache
//ElastiCache is a managed service offering from AWS for in-memory data storages.

Question -> You have a deployed a Lambda functions written in Golang that has loaded the X-Ray SDK. The IAM permissions of the Lambda
function do allow you to send data to X-Ray. But things still don't work, what's a likely source of the problem?
Answer -> Lambda X-Ray active tracing must be enabled
//If your Lambda function runs on a schedule, or is invoked by a service that is not instrumented, you can configure Lambda to sample
and record invocations with active tracing

Question -> Which of the following is not a valid CloudFormation Parameter Type?
Answer -> Dependent Parameter

Question -> While you send messages one by one to Kinesis Stream, you are getting a ProvisionedThroughputException. How can you improve
 the situation while keeping cost constant? (select two)
Answer -> Use Exponential Backoff
Batch Messages
// To reduce overhead and increase throughput, the application produce records in batches.

Question -> You application is deployed on EC2 and needs to access an internal API through API Gateway. Which authentication method will
 provide the easiest security? - @@@
Answer -> IAM permissions with sigv4
//Although you can grant user access to the API created with API Gateway at the individual IAM user level, it is recommended that you
grant access to Amazon API Gateway APIs at the IAM group level

Question -> Which of these RDS features will force your developers to change their application code to fully take advantage of it?
Answer -> RDS Read Replicas       - @@@
//RDS read replicas create new databases that have to be referenced by your application code

Question -> What is the maximum number of messages than can be stored in a SQS queue?
Answer -> No Limit

Question -> What do you need to do to ensure your Kinesis Stream can scale? (select two)
Answer -> You need to add shards
The partition key must take great no. of different values
//Kinesis Data Streams segregates the data records belonging to a stream into multiple shards. It uses the partition key that is
associated with each data record to determine which shard a given data record belongs to. Partition keys are Unicode strings with a
 maximum length limit of 256 bytes.A stream is composed of one or more shards, each of which provides a fixed unit of capacity. Each
 shard can support up to 5 transactions per second for reads, up to a maximum total data read rate of 2 MB per second and up to
 1,000 records per second for writes, up to a maximum total data write rate of 1 MB per second (including partition keys). The data
  capacity of your stream is a function of the number of shards that you specify for the stream. The total capacity of the stream
  is the sum of the capacities of its shards.   - @@@

Question -> How can you easily encrypt your build artifacts coming out of CodeBuild?
Answer -> Specify a KMS key to use

Question -> You have enabled X-Ray integration with AWS lambda but can't see any data go through X-Ray. What's a likely reason of this?
Answer -> Fix the IAM Role
//Create an IAM role with write permissions and assign it to the resources running your application. AWSXRayDaemonWriteAccess includes
permission to upload traces.

Question -> You are hosting a public AWS website on S3 bucket but every user is currently getting a 403 (Forbidden) error. How can you
 remedy this error?
 Answer -> Create a bucket Policy
//Bucket policy is an access policy option available for you to grant permission to your Amazon S3 resources. It uses JSON-based access
 policy language.

Question -> You would like to allow a production on-premise instance to run code using the AWS SDK. You already have an internal secure
 way of identifying production machines within your infrastructure. What's the most secure way of achieving this?
Answer -> Enable Federated Identities integration with cognito  - @@@
//Federation enables you to manage access to your AWS Cloud resources centrally. With federation, you can use single sign-on (SSO) to
access your AWS accounts using credentials from your corporate directory.

Question -> An IAM user has two policies attached.The first policy states that the user has explicit Denied on all EC2 actions. The
second policy states that the user has Allow on EC2:Describe permission. The user tries to describe an EC2 instance using the CLI.
 What will happen?
 Answer -> The user will get denied because the policy has an explicit denied
 //An explicit deny in any policy overrides any allows. - @@@

Question -> Your are deploying a mobile application that needs to access to API. Users will need to register and then access your API.
 Which authentication mechanism do you advise for your API Gateway layer?
 Answer -> Cognito User Pools
 //A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon
 Cognito. Your users can also sign in through social identity providers like Facebook or Amazon, and through SAML identity providers.

Question -> Your API Gateway is receiving requests from an authorized external domain name you don't control. As a result, your costs are
sky-rocketting. What should you do to prevent illegitimate requests?
Answer -> Restrict CORS
//When your API's resources receive requests from a domain other than the API's own domain and you want to restrict servicing these
requests, you must disable cross-origin resource sharing (CORS) for selected methods on the resource.

Question -> Your entire stack is integrated with AWS X-Ray but now you have too much data going into X-Ray and your costs are
skyrocketting. Which feature can help you reduce costs?     - @@@
 Answer -> Enable X-Ray Sampling
 // To ensure efficient tracing and provide a representative sample of the requests that your application serves, the X-Ray SDK applies a
 sampling algorithm to determine which requests get traced. By default, the X-Ray SDK records the first request each second, and five
 percent of any additional requests.

Question -> You want to setup a highly-available application that consumes messages from SQS. Which Elastic Beanstalk fits your needs?
Answer-> ASG Worker Nodes
//If your application performs operations or workflows that take a long time to complete, you can offload those tasks to a dedicated
 worker environment. Decoupling your web application front end from a process that performs blocking operations is a common way to ensure that
  your application stays responsive under load.     - @@@

Question -> You have an RDS database deployed within your VPC, in a private subnet. You would like your Lambda function to connect to it.
 How can you achieve this?  - @@@
Answer -> Deploy a VPC and assign a security group

Question -> Your manager has a requirement that anyone who pushes code in CodeCommit must have signed an CLA first. These requirements
 must be checked in real-time. He has provided you with the code to check whether a committer has signed a CLA. How can you implement this
 solution easily?   - @@@
Answer -> AWS Lambda
//Lambda can be used to retrieve commits, analyse code and committers and perform creative tasks such as checking a CLA

Question -> What are the correct statements about EBS encryption?
Answer -> Restoring a volume from an encrypted snapshot must be an encrypted volume
A snapshot of an encrypted volume is always encrypted
// Amazon EBS encryption uses AWS Key Management Service (AWS KMS) customer master keys (CMKs) when creating encrypted volumes and any
 snapshots created from them.

Question -> You want to have parameters to control your Lambda function connection string to your RDS database. Which feature helps you
 achieve this?
Answer -> Environment Variables     - @@@
// Environment variables for Lambda functions enable you to dynamically pass settings to your function code and libraries, without
 making changes to your code. Environment variables are key-value pairs that you create and modify as part of your function
  configuration, using either the AWS Lambda Console, the AWS Lambda CLI or the AWS Lambda SDK.

Question -> An RDS database is experiencing read heavy workload (99% of read versus 1% of writes) and the number of rows you have is
 quite small. You would like to scale the read performance to possibly high numbers as your website scales, but don't want to have to
  experience linear cost as the read load increases. You don't mind changing the application code logic. Which solution fits best
   this use case?
Answer -> Setup Elastic Cache in front of RDS

Question -> Which of the following AWS services rely on Cloudformation to provision resources? (select two) - @@@
Answer -> Step Functions , Elastic BeanStalk
//You can use the Resources key in a configuration file to create and customize AWS resources in your environment. Resources defined
 in configuration files are added to the AWS CloudFormation template used to launch your environment. All AWS CloudFormation resources
  types are supported.You can now use CloudFormation templates to create and delete Step Functions state machines and Activities.

Question -> What two types of deployment are supported by CodeDeploy? (select two)  - @@@
Answer -> Blue/Green Deployement , In-Place deployement
//In-place deployment: The application on each instance in the deployment group is stopped, the latest application revision is
installed, and the new version of the application is started and validated.Blue/green on an EC2/On-Premises compute platform: The
instances in a deployment group (the original environment) are replaced by a different set of instances (the replacement environment)
Blue/green on an AWS Lambda compute platform: Traffic is shifted from your current serverless environment to one with your updated
 Lambda function versions.

Question -> You would like to trigger an external HTTP service (also called webhook) when someone pushes code to AWS CodeCommit.
 How can you implement that with no code changes?
 Answer -> SNS + HTTP integration   - @@@
 //You can create a trigger for an AWS CodeCommit repository so that events in that repository trigger notifications from an Amazon
  Simple Notification Service (Amazon SNS) topic. You might want to create a trigger to an Amazon SNS topic to enable users to
   subscribe to notifications about repository events, such as the deletion of branches.

Question -> You have enabled a DLQ for AWS Lambda. Which of the following will put a message into a DLQ after being processed
by AWS Lambda? (select two)
Answer -> The Invocation Failed , The invocation was asynchronous
//Any Lambda function invoked asynchronously is retried twice before the event is discarded. If the retries fail and you're
 unsure why, use Dead Letter Queues (DLQ) to direct unprocessed events to an Amazon SQS queue.

 Question -> You would like to scale CodeBuild to run parallel builds. How can you achieve it?
 Answer -> Codebuild scales automatically
 //CodeBuild runs build in parallel automatically and has a 20 concurrent build limit that you can increase.    - @@@

Question -> Your API Gateway is started to feel pressured by the same GET REST API calls done over and over by a variety of
clients. The results of these GET API calls is always the same. What can you do to improve performance and reduce costs?
Answer -> Enable API Gateway Caching
//You can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of
calls made to your endpoint and also improve the latency of requests to your API. When you enable caching for a stage, API Gateway
 caches responses from your endpoint for a specified time-to-live (TTL) period, in seconds.

Question -> Your manager would like messages to be stored in SQS for 12 days. What can you do to achieve that?
Answer -> Change the Settings for message retention
//Amazon SQS automatically deletes messages that have been in a queue for more than maximum message retention period. The default
 message retention period is 4 days. However, you can set the message retention period to a value from 60 seconds to 1,209,600 seconds
 (14 days) using the SetQueueAttributes action.

Question -> You are getting Throttle errors when you push data to CloudWatch using the PutMetric API, and would like to ensure the
 data eventually gets to CloudWatch. What should you do?
Answer -> Use Exponential Backoff for retries
//each AWS SDK implements exponential backoff algorithm for better flow control. The idea behind exponential backoff is to use
 progressively longer waits between retries for consecutive error responses. You should implement a maximum delay interval, as well
  as a maximum number of retries. The maximum delay interval and maximum number of retries are not necessarily fixed values, and
  should be set based on the operation being performed, as well as other local factors, such as network latency.

Question -> You would like to deploy a Lambda function which unzipped amounts to 300 MB of data. How can you do that?  - @@@
Answer -> You need to upload a smaller function and load extra files at runtime into the /tmp directory

Question -> You know that your build time should not exceed 5 minutes for a particular application. Recently, some HTTP network
 was hung and CodeBuild was running for over an hour. How can you prevent that issue in the future?
Answer-> Enable Codebuild timeouts  - @@@
//By setting the timeout configuration, the build process will automatically terminate post the expiry of configured timeout.

Question -> How many arguments does the CloudFormation !FindInMap function take?
Answer -> Following are the argumentsMapName TopLevelKey SecondLevelKey

Question -> The company has a requirement to delete the data from CloudWatch Logs after 1 week for security compliance. Which feature
 will help you in achieving that?
Answer -> Cloudwatch log expiration policies
//By default, logs are kept indefinitely and never expire. You can adjust the retention policy for each log group, keeping the indefinite
 retention, or choosing a retention periods between 10 years and one day.   - @@@

 Note -> Scheduled instances allow you to reserve capacity on a recurring basis with a daily, weekly, or monthly schedule over the course
  of a one-year term.   - @@@

 Question -> You would like to distribute your API to various customers and ensure their consumption is limited by the plans they have
  subscribed to. Which feature allows you to do that?
 Answer -> Usage Plans and API keys
// You can configure usage plans and API keys to allow customers to access selected APIs at agreed-upon request rates and quotas that
meet their business requirements and budget constraints.

Question -> An application is hosted by a 3rd party and is exposed as yourapp.3rdparty.com. You would like to have your users access
your application using mydomain.com , which you own and manage under Route53. What record should you create?
Answer -> Create a CNAME record
// CNAME records can be used to alias one name to another

Question -> You assigned the security group ("sg-app") to an EC2 instance named "php-app", and you assigned the security group ("sg-db")
  to an RDS instance named "php-db". Both instances are in the same subnet. How do you configure the security groups in order to let an
  application running in "php-app" connect to the database "php-db"
  Answer -> Add an inbound rule to "sg-db": Protocol=TCP, Port=3306, Source=sg-app
//Since php-db allows access to php-app, sg-db defines the inbound rule and not the opposite..

Question -> You have an IAM group called "web-designers". You have added all the web designers in that group. After one year, all web
designers have resigned. What action you should take?
Answer -> Delete their user accounts from the group and keep the group empty
// An IAM group does not contain only a set of users, but also it contains a set of policies. If users are deleted, the IAM
group is still useful for the subsequent users that needs policies attached to that group.

Note -> AWS Cloudfront is global services   - @@@

Question -> You are hiring a new developer for a new project that will be implemented using Step Functions. The HR team requested from
 you to help them write a job description. You wrote: "Required skills: - Proficiency in"
 Answer -> JSON
 //The State machines in AWS Step functions is written with JSON

Question -> Your are deploying your static website to an S3 bucket. You are planning to automate this deployment while reducing the cost
 of this automation as much as possible. Which AWS services should you consider in this case?
 Answer -> CodeBuild + CodePipeline
 //Since it is a deployment to S3, AWS CodeBuild should be enough since static websites can be uploaded to s3 using the AWS CLI and
  without the need of provisioning an EC2 instance. CodeDeploy is only used to deploy to EC2 instances or Lambda functions  - @@@

Question -> You want to retrieve some items from a DynamoDB table using the primary key. What API call will provide better performance
 amongst the 4 below ? (select two)
 Answer -> GetItem and Query

 Note -> A single shard is able to ingest up to 1MB or 1,000 PUT(s) per second of streaming data, and emitting data at a rate of
 2MB per second.    - @@@

Note -> Kinesis Streams requires manually provision to meet the needed capacity, while Kinesis Firehose scales out automatically. - @@@

Question ->You contacted the AWS support team asking them to release Lambda with the 10.3.0 Node.js runtime instead of the current
version, which is 8.10. The AWS support team apologized for not being able to launch this version at the moment. What should you do to
 migrate to 10.3.0?
 Answer -> You cannot do anything. U should wait for the subsequent release

 Note -> The S3 bucket storage is unlimited. However, the maximum size of a single object (file) is 5TB     - @@@

 Question -> Which statements are correct about global secondary index (GSI) and local secondary index (LSI) ? (select two)
Answer -> The hash key of the LSI is same as the hash key of the main table
          A GSI lets u query over the entire table, across all partitions

Question -> Using Elastic Beanstalk, one environment includes   - @@@
Answer -> one and only one application version

Question -> Using which AWS API can you return one or more items from one or more DynamoDB tables?
Answer -> BatchGetItem

Note -> Identity pools are the containers that Cognito Identity uses to keep your apps' federated identities organized. - @@@

Question -> You provisioned five EC2 instances to process messages from an SQS queue. Most of the time, the majority of the EC2
 instances are idle for certain reasons. How to reduce the cost of this solution while keeping the functionality as it is?
Answer -> Put the EC2 instances inside an Auto Scaling Group and scale based on the queue size
//Long polling reduces the number of requests while receiving messages. Since requests costs $0.4 per million, long polling is a
cost-effective approach. Autoscaling group allows the horizontal scaling of EC2 machines according to a metric. For this case,
ApproximateNumberOfMessagesVisible will be the suitable metric.

Question -> Your application deployed to Elastic Beanstalk from CodePipeline (CodeCommit => CodeBuild) takes a very long time to be
 deployed due to the fact it has to resolve many dependencies on each of your 100 target EC2 instances. What do you suggest to improve
 the performance while having the minimal amount of code and developer impact?
 Answer -> Bundle the dependencies in Source Code during the last stage of code build
 //Bundling the dependencies with the code during the build output phase is a great way to speed up Beanstalk deployments

 Question ->Which of the following services will not help you in understanding an authorization exception that happened when you ran
 a CLI command?
 Answer -> Cloudwatch
 //CloudWatch helps only with metrics monitoring. The rest(STS,IAM,CloudTrail) is relevant to help you debug authorization exceptions

 Notes -> In a canary release deployment, total API traffic is separated at random into a production release and a canary release with a
  pre-configured ratio. Typically, the canary release receives a small percentage of API traffic and the production release takes up the
   rest. The updated API features are only visible to API traffic through the canary. You can adjust the canary traffic percentage to
   optimize test coverage or performance.

Question -> You would like to see the logs of your failed CodeBuilds. How should you do it?
Answer -> Enable S3 and Cloudwatch integration
//If there is any build output, the build environment uploads its output to an Amazon S3 bucket.While the build is running, you can use
the AWS CodeBuild console, AWS CLI, or AWS SDKs, to get summarized build information from AWS CodeBuild and detailed build information
from Amazon CloudWatch Logs.    - @@@

Question -> You would like to deploy an application while defining all the database authentication and authorization using IAM policies.
 Which database will be the best fit?
Answer -> DynamoDB
//Access to Amazon DynamoDB requires credentials. Those credentials must have permissions to access AWS resources, such as an Amazon
DynamoDB table. You can use AWS Identity and Access Management (IAM) to help secure access to your DynamoDB service

Question -> Which of the following can be used to implement a REST API? (select two) - @@@
Answer -> ALB+ECS , API Gateway + lambda

Question -> How can you easily encrypt your build artifacts coming out of CodeBuild?    - @@@
Answer -> Specify a KMS key to use

Question -> You need to store data that needs to be available on a tablet, a mobile device and a web application for the same user. Which
 product allows you to achieve this?
 Answer -> Cognito Sync
 //Amazon Cognito Sync is an AWS service and client library that enables cross-device syncing of application-related user data. You can
 use it to synchronize user profile data across mobile devices and the web without requiring your own backend.

 Question -> You have a deployed a Lambda function written in Golang that has loaded the X-Ray SDK. The IAM permissions of the Lambda
 function do allow you to send data to X-Ray. But things still don't work, what's a likely source of the problem?   - @@@
 Answer -> Lambda X-Ray active tracing must be enabled
//If your Lambda function runs on a schedule, or is invoked by a service that is not instrumented, you can configure Lambda to sample and
 record invocations with active tracing.

 Question -> What's the best practice regarding accessing the AWS Root account?
 Answer -> It should be accessible by one admin only after enabling MFA

 Question -> When you rollback a CodeDeploy, where is the application deployed first?
 Answer-> To the failed instances
 // AWS CodeDeploy rolls back deployments by redeploying a previously deployed revision of an application as a new deployment on the failed
  instances.

 Question -> You want to send customised email to your users. You should use
 Answer -> SES
 //Amazon Simple Email Service (Amazon SES) is a cloud-based email sending service designed to help digital marketers and application
 developers send marketing, notification, and transactional emails.

 Question -> How can you retrieve a nested JSON attribute in your DynamoDB query?
 Answer -> Specify a ProjectionExpression
 //A projection expression is a string that identifies the attributes you want. To retrieve a single attribute, specify its name. For multiple
  attributes, the names must be comma-separated.

Question -> You would like to pass 1 MB of encrypted data to your AWS Lambda function, that it needs to load before properly functioning.
Which method will work best?
Answer -> Envelope Encryption and store as file within code - @@@
//AWS Lambda environment variables have a maximum size of a few KB. Additionally, the direct "Encrypt" API of KMS also has a few KB limit.
To encrypt 1 MB, you need to use the Encryption SDK and pack the encrypted file with the lambda function.

Question -> Each ECS Task must have an IAM policy attached to it. How do you achieve that?
Answer -> Assign an IAM role for ECS and assign it to tasks
//you must also create a role for your tasks to use before you can specify it in your task definitions. You can create the role using the
 Amazon Elastic Container Service Task Role service role in the IAM console. Then you can attach your specific IAM policy to the role that
  gives the containers in your task the permissions you desire.

Question -> What is the X-Ray agent?    - @@@
Answer -> The X-Ray agent collects data from log files and sends them to the X-Ray service for aggregation, analysis, and storage. The
agent makes it easier for you to send data to the X-Ray service, instead of using the APIs directly, and is available for Amazon Linux AMI,
Red Hat Enterprise Linux (RHEL), and Windows Server 2012 R2 or later operating systems.

Question -> Where is CloudFront SSL in flight encryption available?
Answer -> Between clients and Cloudfront and cloudfront and backend
//For web distributions, you can configure CloudFront to require that viewers use HTTPS to request your objects, so connections are
encrypted when CloudFront communicates with viewers. You also can configure CloudFront to use HTTPS to get objects from your origin,
so connections are encrypted when CloudFront communicates with your origin.

Question -> You would like to access the amount of RAM as a metric on your EC2 instances. How can you get that information?
Answer -> Use the cron job on the instance that push the EC2 RAM metric as a custom metric

Note - The load balancer is highly available and it's public IP may change. The DNS remains constant

Question -> You want to be able to stage your deployment to one third of your fleet, then another third and finally the last third.
 How can you best achieve this using CodeDeploy?
Answer -> CodeDeploy deployment groups      - @@@

Question -> You would like to trigger a notification when a CodeBuild fails. Which AWS service helps you in achieving that?
Answer -> AWS Cloudwatch Events + SNS

Question -> Your mobile application needs to directly perform API calls to DynamoDB. You do not want to store secrets onto the mobile
devices and need all the calls to DynamoDB done with a different identity per mobile device. Which service allows you to achieve this?
Answer -> Cognito Identity Pools

Note -> Data in AWS CodeCommit repositories is encrypted in transit and at rest. When data is pushed into an AWS CodeCommit repository
(for example, by calling git push), AWS CodeCommit encrypts the received data as it is stored in the repository.

Question -> You would like to run multiple versions of your application in Elastic Beanstalk to ensure you can perform regular development
 and load testing. What do you recommend?
Answer -> Define a dev environment with a single instance and a"load test" environment that has settings close to prod
//It is common practice to have many environments for the same application

Question -> What are some key differences between a standard SQS and SWF? (select two)
Answer -> SWF is task oriented API and SQS is message-oriented API
SWF ensures the task is assigned only once while SQS may deliver the msg multiple times
//Amazon SWF provides useful guarantees around task assignment. It ensures that a task is never duplicated and is assigned only once. Thus,
 even though you may have multiple workers for a particular activity type (or a number of instances of a decider), Amazon SWF will give a
 specific task to only one worker (or one decider instance).

Question -> At which frequency do EC2 instances report their metrics under normal configurations?
Answer -> 5 minutes
// By default, Amazon EC2 sends metric data to CloudWatch in 5-minute periods

Question -> A producer application needs to deliver many messages to consumer applications. These consumer applications have various
 consumption patterns and can either consume the message instantaneously or up to 10 days later. The number of consumer applications
  can grow and shouldn't trigger a code change for the producing application. Which solution should you use?
Answer ->  SNS + SQS

Question -> You are running an application leveraging the SDK on an EC2 instance. How do you pass credentials to the SDK?
Answer -> Use an IAM Instance Role      - @@@
// An instance profile is a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts

Question -> You would like to store small encrypted secrets in SSM Parameter store. What is the right way of doing it?
Answer -> Enable Seamless encryption with KMS integration       - @@@
//With AWS Systems Manager Parameter Store, you can create Secure String parameters, which are parameters that have a plaintext parameter
 name and an encrypted parameter value. Parameter Store uses AWS KMS to encrypt and decrypt the parameter values of Secure String parameters.
 Also, if you are using customer managed CMKs, you can use IAM policies and key policies to manage encrypt and decrypt permissions.

Question -> You would like to run an application continously for a year and can predict how much capacity you will need. You need the
application instances to be stable and not terminated abruptly as you believe this will impact your users. What should you use?
Answer -> EC2 Reserved Instances       - @@@
// Reserved instances can provide a capacity reservation, offering additional confidence in your ability to launch the number of instances you
have reserved when you need them.

Question -> You are trying to collect metrics in near real time in order to track your application performance. Which service should you use?
Answer -> Cloudwatch metrics
//Amazon CloudWatch monitors your Amazon Web Services (AWS) resources and the applications you run on AWS in real time. You can use
CloudWatch to collect and track metrics, which are variables you can measure for your resources and applications. CloudWatch alarms send
notifications or automatically make changes to the resources you are monitoring based on rules that you define.

Question -> You would like to run command from the CLI on your EC2 instance. The instance should automatically obtain credentials to perform
these commands. What feature will allow you to do this in the most secure way?
Answer -> IAM roles for EC2
//IAM roles have been incorporated so that your applications can securely make API requests from your instances, without requiring you to
manage the security credentials that the applications use. Instead of creating and distributing your AWS credentials, you can delegate
permission to make API requests using IAM roles.

Question ->CodeDeploy is deploying to EC2 instances but they seem to have problem pulling the code from S3. What should you do?
Answer -> Fix the IAM permissions for the EC2 instance Role
//Make sure the EC2 instance roles has proper permissions to pull from S3

Question -> Your application runs on Elastic Beanstalk. Its configuration values change often and the devops team does not want to
re-deploy the application every time a configuration changes. They would rather manage configuration externally and securely and have it
 load dynamically into the application at runtime. What advice do you give?
Answer -> use SSM Parameter Store
//AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can
 store data such as passwords, database strings, and license codes as parameter values.

 Question -> Your CodePipeline now fails and it seems it has been heavily modified. Which service can you leverage to figure out the origin
 of the modifications?
 Answer -> Cloudtrail
 //CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs
 , command line tools, and other AWS services.

 Question -> You are looking to perform some analytics and reporting on large datasets stored in your database. Which database solution fits
  best these requirements?
 Answer -> Redshift
 //Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. You can start with just a few hundred gigabytes of
  data and scale to a petabyte or more. This enables you to use your data to acquire new insights for your business and customers.

Question -> You have an auto scaling group with a minimum capacity of 1 and a maximum capacity of 5, configured to launch instances across
 3 AZ. It is configured to scale based on a target CPU Utilisation of 35%. During a low utilisation period of your application, an entire
  AWS availability zone went down and your application experienced downtime. What should you do to ensure that your application is highly
  available?
Answer -> Increase the minimum instances in the ASG to 2

Question -> You would like your users to be able to access your application using a stable URL. The architecture of your application may evolve
 but the users should not be aware of it. How can you do that?
Answer -> Expose a domain name created with Route53
//Route53 allows you to change the record type of your domain and lets you be completely free for what architecture you want to implement or
change in the future
 --------------------------------------------------

 Question ->  At what size does AWS recommend customers to use the multi part upload tool when uploading object to S3?
 Answer -> 100 mb

 Question -> https://www.whizlabs.com/learn/course/quiz-result/165385 question 4 (kind of msg SNS sends to endpoints and the parameters
 contained in it)

 Question -> https://www.whizlabs.com/learn/course/quiz-result/165385 question 6 (supported platforms in Elastic beanstalk)

 Question -> https://www.whizlabs.com/learn/course/quiz-result/165385 question 8 (About SWF)

 Question -> https://www.whizlabs.com/learn/course/quiz-result/165385 question 9 (Scenerio for EBS)

 Note -> S3 API 409 Conflict error cause is Your're attempting to remove a bucket without emptying the contents of the bucket first

 Question -> DynamoDB natively supports cross table joins?
 Answer -> False

 Question -> https://www.whizlabs.com/learn/course/quiz-result/165385 question 14 (Benefits of Multi part upload)

 Note -> S3 object key names r stored lexicographically(alphabetical order)

 Note -> A SWF workflow task or task execution can live upto 1year
//How long can workflow executions run?
  Each workflow execution can run for a maximum of 1 year. Each workflow execution history can grow up to 25,000 events. If your use case
  requires you to go beyond these limits, you can use features Amazon SWF provides to continue executions and structure your applications
  using child workflow executions.

  Note -> the information we can get from instance metadata
    ami-id
    ami-launch-index
    ami-manifest-path
    block-device-mapping/
    hostname
    iam/
    instance-action
    instance-id
    instance-type
    local-hostname
    local-ipv4
    mac
    metrics/
    network/
    placement/
    profile
    public-hostname
    public-ipv4
    public-keys/
    reservation-id
    security-groups
    services/

  Note -> A LSI is a index that has the same hash key as the table, but a different range key
  //A local secondary index maintains an alternate sort key for a given partition key value. A local secondary index also contains a
   copy of some or all of the attributes from its base table; you specify which attributes are projected into the local secondary index
   when you create the table. The data in a local secondary index is organized by the same partition key as the base table, but with a
   different sort key. This lets you access data items efficiently across this different dimension. For greater query or scan flexibility,
   you can create up to five local secondary indexes per table.

  Question -> What is the function for the conditional writes?
  Answer -> A change to a DynamoDB attribute will only be written if it that attribute's value has not changed since it was read

  Note -> Parts of a multi part upload will not be completed until the "complete" request has been called which puts all the parts of
  the file together

  Note -> An item stored in a DynamoDB can contain any number of attributes associated to it, although their is a limit of 400kb on the
  item size

  Note -> By Default , AWS allows u to have 256 dynamoDB tables per account, per region.

  Note -> their is nothing like ReadObject in S3 API call , U can Delete, Get, Head, Options, Post, Put, Select, List ,Upload, Complete,
  Abort , Initiate

  Question -> https://www.whizlabs.com/learn/course/quiz-result/165385 question 28 (Senerio for EC2)

  Question -> https://www.whizlabs.com/learn/course/quiz-result/165385 question 29 (AWS Opsworks)

  Question -> https://www.whizlabs.com/learn/course/quiz-result/165385 question 32 (Write throughput calculate)

  Question -> https://www.whizlabs.com/learn/course/quiz-result/165385 question 47 (SWF Workflow)

  Note -> SNS endpoints protocols available r HTTP, HTTPS, Email, Email-JSON, SQS, Application, Lambda, SMS(Short Message Service)

  Note -> Their is No Cost for VPC and Auto Scaling , but ELB is not free

  Note -> Dynamo DB supports two types of primary keys "Hash" and "Hash Range"

  Question -> To deploy an application for the development purposes on to Elastic Beanstalk environment and u also need to ensure that
  custom software is installed on the backend linux servers that r launched as part of the Elastic Beanstalk environment. U can achieve
  this by ?
  Answer -> Create a YAML file with the required package names to be installed on the server
  Place the file in the .ebextensions folder in ur Application Source Bundle

  Question -> Ur company is planning on storing documents in S3 bucket, the documents r sensitive and employees should use Multi Factor
  authentication when trying to access documents, What u should do to full fill the requirement?
  Answer -> Ensure that the bucket policy is in place with condition of "aws:MultiFactorAuthPresent":"false" with a Deny Policy

  Note -> This amounts to having ur API respond to the OPTIONS preflight request with at least the following CORS-required response headers
    - Access-Control-Allow-Methods
    - Access-Control-Allow-Headers
    - Access-Control-Allow-Origin

  Question -> ur application will make use of  Kinesis Firehose for streaming the Records to s3, to make all data encrypted at rest, U can
  achieve this in Kinesis Firehose by?
  Answer -> Enable Encryption for a Kinesis Data Firehose,
  Ensure that kinesis streams r used to transfer the data from producers
  // when u configure a kinesis stream as the data source of a kinesis firehose delivery stream, kinesis data firehose no longer stores the
  data at rest, instead the data is stored in the kinesis stream

  Question -> To make use of Jenkins for ur CI/CD pipeline and other AWS services for the deployment purpose, u will consider doing?
  Answer -> Configure an EC2 instance with Jenkins installed,
  Configure an IAM role for EC2 to access Code Pipeline

  Note -> U cannot convert an existing standard queue into a FIFO queue, u need to make a new FIFO Queue.

  Question -> Ur application is making requests to a DynamoDB table. Due to certain surge of requests, u r now getting throttling errors
  in ur application, We can resolve those errors by ?
  Answer -> Use exponential backoff in ur requests from the application,
  Change the throughput capacity on the tables

  Question -> To make use of cloudformation templates for deploying application on EC2 instances, these instances need to be preconfigured
   with NGINX web server to host the application, How can u accomplish this with Cloudformation?
  Answer -> Use the cfn-init helper script in cloudfromation
  //u can launch stacks , install and configure software applications on EC2 by using the cfn-init helper script and using the AWS::Cloudfromation::Init
  resource (by using AWS::Cloudfromation::Init u can describe the configuration that u want rather then scripting procedural steps)

  Note - For testing application using API gateway and lambda service and test the deployed new version of the application U should use canary
  over other options such as create a new version of the existing lambda function and a new api gateway , give the users the new url becuase
  although it will work but it would add too much of maintenence overhead

  Till Whizlabs question 23 only


Practice Test 4(2018) ->

Question -> https://www.whizlabs.com/learn/course/quiz-result/183319 question 38(DynamoDB capacity)

Question -> https://www.whizlabs.com/learn/course/quiz-result/183319 question 49(See the Diagram in Process)

Question -> Their is requirement to provision an application using elastic beanstalk service.it's a custom application where there are lot of
configuration files and patch that need to be download. which of the following would be the best way to provision the environment in the
least time possible?
Answer -> Use a custom AMI for the underlying instances
// When u create an Elastic beabstalk environmnet u can specify an AMI to use instead of standard AMI included in ur platform configuration
 solution stack,
 A custom AMI can improve the provisioning times when instances r launched in ur environmnet if u need to install lot of software that isn't
 included in the standard AMI.

 Question -> Docker containers need to encrpyt data, the data keys need to be generated using KMS service, the data key should be in the
 encrypted format. which of following will u most ideally use?
 Answer -> GeneratedDataKeyWithoutPlainText command (not use GeneratedDataKey will not be used since it will return a plain text key as well
 but the GeneratedDataKeyWithoutPlainText will return only the encrypted copy of the data key)

 Question -> U r testing the AWS services out by querying them using REST API, which could be used to make successful call to AWS service?
 Answer -> Access Key
 // Access keys r consisted of access key ID and secret access key and u use access keys to sign programmatic requests that u make to AWS
 if u use AWS SDKs , REST or Query API operations

 Question -> u have currently setup an API gateway service in AWS, the api gateway is calling a custom API hosted on an EC2 instance. there
 r severe latency issues and u need to diagonese the reason for those latency issues, thich could be used to address these concerns?
 Answer -> X-Ray  // X-ray is the service which allows u to trace latency issues
 // cloudwatch can not be used since this is used to log API execution operations and cloudtrail cannot be used as it is used to log the
 API Gateway management operations and VPC Flow logs can not be used since this is used to log calls into the VPC

 Note -> https://docs.aws.amazon.com/codedeploy/latest/userguide/instances.html go to the heading Comparing Amazon EC2 Instances to On-Premises Instances
 their u will know the requirements for them

 Question -> u r incharge of developing cloudformation templates which would be used to deploy databases in different AWS Accounts. in
 order to ensure that the passwords for the database r passed in a secure manner which of the following u can use with Cloudformation?
 Answer -> Parameters
 // suppose ur stack creates a new database instance. when the database is created, cloudformation needs to pass a database administrator
 password, u can pass in a password by using an input parameter instead of embedding it in ur template
 // we can not use 'metadata' since it is used to specify Objects that provide additional information about the template

 Question -> ur team needs to develop an application that needs to make use of SQS queues. their is a requirement that when msg is added to
 a queue, the msg is not visible to 5 mins to consumers. How can we achieve this?
 Answer -> Implement Delay Queues in AWS and change the msg timer value for the individual msg
 // we cannot use visibilty timeout because this will only make the msg invisible after the msg has been read and not in the beginning
// we cannot use log polling bcz it is used to reduce the cost of SQS by eliminating the number of empty responses from SQS queue

Question -> an application is curenlty using DynamoDB table for storage and retrieval of data, currently the data access for items in the
table is in milliseconds, but the company wants to improve further on the acess time? how this can be acieved?
Answer -> DAX // we cannot use the option increase the read capacity of the tables because it will not assist in providing better latency
of read access to data in the DynamoDB table

Question -> Your company is planning on creating an application that will do following
    - collect all logs from various servers on their on -premise infrastructure
    - Process the logs and then create dashboards based on the result
  Which service will be used in developing such application?
Answer -> AWS Kinesis
// Kinesis Data Streams can be used to collect logs and event data from servers, desktops, mobile etc and u can use kinesis applications to
continously process the data, generate metrics, power live dashboards and emit aggreagated data into stores such as S3

Question -> ur team is planning on hosting an application in Elastic beanstalk, the underlying environment will contain multile EC2 instances
. u need these instnaces to share data via a shared file system, Which will u use for this purpose?
Answer -> AWS EFS
//EFS can create network file systems that can be mounted by instnaces across multiple AZ. As EFS file system uses security groups to control
access over the network in ur default or custom VPC
// we can use S3 because it used for object based storage and we can not use AWS Glacier bcz it is used to achieve storage

Question -> UR developemnt team is planning on deploying an application using the elastic beanstalk service. as part of the deployement
, u need to ensure that a high end instnace type is used for the deployement of the underlying instances. which u will use to enable this?
Answer -> the launch configuration and environment manifest file
// we can include a YAML formatted environmnet manifest in the root of ur application source bundle to configure the environmnet name,
solution stack and environmnet links to use when creating an environmnet
// we can not use AWS config section bcz this is used to monitor the configuration changes of the resource

Note -> DynamoDB streams can only be integrated with Lambda not with SNS,SQS or other service

Question -> U wants to know that how much of the consumed capacity is being used for queries being fired in DynamoDB. How u can achieve this?
Answer -> Ensure to set the ReturnConsumedCapacity in the query result to TOTAL
//A Query operation does not return any data on how much read capacity it consumes, However u can specify the ReturnConsumedCapacity parameter
in the query request to obtain this information, settings for the ReturnConsumedCapacity
    - NONE - no consumed capacity data is returned // By Default
    - TOTAL - the response includes the aggregate number of read capacity units consumed.
    - INDEXES - the response shows the aggregate number of read capacity units consumed, together with the consumed
                capacity for each table and index that was accessed.

Question -> ur company has bucket which has versioning and encryption enabled. the bucket reveives thousand of PUT operations per day.
After a period of 6 months, there r significant number of HTTP 503 error codes which r being received, which of the following we can use to
diagnose the error?
Answer -> AWS S3 inventory
// If you notice a significant increase in the number of HTTP 503-slow down responses received for Amazon S3 PUT or DELETE object requests
 to a bucket that has versioning enabled, you might have one or more objects in the bucket for which there are millions of versions. When
  you have objects with millions of versions, Amazon S3 automatically throttles requests to the bucket to protect the customer from an
   excessive amount of request traffic, which could potentially impede other requests made to the same bucket.
 To determine which S3 objects have millions of versions, use the Amazon S3 inventory tool. The inventory tool generates a report that
  provides a flat file list of the objects in a bucket

Question -> u have created a code commit repository in AWS, u need to share ur repository with developers in ur team, which would be the
secure and easier way to share the repository with the developement team?
Answer -> Allow the developers to connect via HTTPS using the Git credentials
and Create credentials for the IAM user
// we cannot use the option to allow developers to connect via SSH bcz it is not an easy way to connect to repository

Note -> API keys r used to provide the Usage Plans to the customers and then track and limit the usage of ur API stages and methods for
 each API's.

 Question  -> Ur development team is testing out an application that is being deployed onto AWS Elastic beanstalk. the application needs to have
 a RDS instance provisioned as a part of Elastic BeasnStalk setup. But they want to ensure that the database is preserved for the analysis
even after the environment is torn down, How u can achieve this?
Answer -> Ensure the database is created as part of the ElasticBeanStalk Environmnet and Ensure that the retention of the database is marked
as "Create Snapshot" // this "Create Snapshot" ensure that the database lives even after the environmnet is deleted

Question -> Ur team is looking towards deploying an application into Elastic Beanstalk. they wants to deploy different versions of the
application onto the environmnet. How can they achieve this is easisit possible way?
Answer -> Create multiple environmnets in Elastic Beanstalk
//Elastic Beanstalk creates an application version whenever you upload source code. This usually occurs when you create an environment or
 upload and deploy code using the environment management console or EB CLI. Elastic Beanstalk deletes these application versions according
  to the application's lifecycle policy and when you delete the application.

// we cannot use the option to Create multiple applications in Elastic Beanstalk and to Create multiple environmnets inElastic Beabstalk bcz
in question it is mentioned for the easisit possible way and we cannot use codepipeline for the application versions

Question -> U r configuring cross origin for ur S3 bucket. u need to ensure that the external domain sites can only use the GET requests
against ur bucket. which of the following would u modify as part of the CORS ocnfiguration for this requirement?
Answer -> AllowedMethod Element // not to use AllowedOrigin Element or AllowedHeader Element

Note -> DynamoDB is Schemaless

Note -> https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html go to the table under heading SQl or Nosql to
 understand that in which case to use DynamoDB or RDBMS

Question -> Ur team is developing an Mobile based Application, the users who r going to use this application will first be authenticated using
an external provider such as Facebook. the application would then need to get temporary access credentials to work with AWS resources. which
actions would u use for the following purpose?
Answer -> AssumeRoleWithWebIdentity // it returns a set of temporary security credentials for users who have been authenticated in a mobile
or web application with web identity provider
// we cannot use the option AssumeRoleWithSAML as it returns a set of temporary security credentials for users who have been authenticated via
SAML authentication response

Note -> For cross account use of the AWS resources we can assume only roles not users

Note -> To migrate your environment's platform to a new configuration, go to the below link with same heading on page
  https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.platform.upgrade.html

Question -> U r developing an application that is working with the DynamoDB table.Some of ur request results r returning an HTTP 400 status
code. Which of the following r the possible issues with the requests?
Answer -> There r missing required parameters with the some of the requests
u r exceeding the table's provisioned throughput
// we can not use the options that the DynamoDB service is unavailable and There r network issues bcz these options will result in 5xx errors

Question -> U have to develop an application which would transfer the logs from several EC2 instances to an S3 bucket, Which will u use for
this purpose?
Answer -> AWS Data Pipeline
//AWS Data Pipeline is a web service that you can use to automate the movement and transformation of data. With AWS Data Pipeline, you can
 define data-driven workflows, so that tasks can be dependent on the successful completion of previous tasks. You define the parameters of
  your data transformations and AWS Data Pipeline enforces the logic that you've set up.
// Data Nodes -> In a Data Pipeline a data node defines the location and type of data that a pipeline activity uses as input or output.
Data pipeline supports the following types of data nodes
  - DynamoDBDataNode
  - SqlDataNode
  - RedshiftDataNode
  - S3DataNode
// TaskRunner -> it is an application that polls AWS DataPipeline for tasks and then performs those tasks
// Activity -> it is a pipeline component that defines the work to perform
// Resource -> it is the computational resource that performs the work that a pipeline activity specifies
Ex question -> https://www.whizlabs.com/learn/course/quiz-result/196967 question 19

Practice Test 2 (2018) ->

Note -> Cloudwatch Metric only give the rate at which the function is executing but not help debug the actual error

Note -> Cross Account Access ->
Question -> https://www.whizlabs.com/learn/course/quiz-result/167803 question 9
// You can configure access to AWS CodeCommit repositories for IAM users and groups in another AWS account. This is often referred to as
 cross-account access. This section provides examples and step-by-step instructions for configuring cross-account access for a repository
 named MySharedDemoRepo in the US East (Ohio) Region in an AWS account (referred to as AccountA) to IAM users who belong to an IAM group
 named DevelopersWithCrossAccountRepositoryAccess in another AWS account (referred to as AccountB).

Note -> application version lifecycle policy -> A lifecycle policy tells Elastic Beanstalk to delete application versions that are old,
or to delete application versions when the total number of versions for an application exceeds a specified number.

Note -> TTL -> Time To Live (TTL) for DynamoDB allows you to define when items in a table expire so that they can be automatically deleted
 from the database.
 TTL is provided at no extra cost as a way to reduce storage usage and reduce the cost of storing irrelevant data without using provisioned
 throughput. With TTL enabled on a table, you can set a timestamp for deletion on a per-item basis, allowing you to limit storage usage to
 only those records that are relevant.
 TTL is useful if you have continuously accumulating data that loses relevance after a specific time period. For example: session data,
 event logs, usage patterns, and other temporary data. If you have sensitive data that must be retained only for a certain amount of time
 according to contractual or regulatory obligations, TTL helps you ensure that it is removed promptly and as scheduled.

Note -> CodeStar accerelates software release with the help of AWS Codepipeline, a CI/CD service. Each Project comes preconfigured with an
automated pipeline that continously builds,tests and deploy code with each commit.So in order to provide the frequent releases to the
costumer we can use Pipeline in the CodeStar service

Note -> So when using the CLI, if the environment variables r set with the Access keys, they would take preference over the IAM role

Question -> An application currently makes use of DynamoDB tables. their is requirement that the user can only view certain items in the
table.How can this be accomplised?
Answer -> Use IAM policies with specific conditions

Note - To reduce costs with SQS queue u can use Lambda functions to process messages

Note - Elastic Beanstalk is ideal service to quickly provision development environments, u can also create environments which can be used to
host Docker based containers

Note -> Lazy loading allows for stale data, but won't fail with empty nodes. Write through ensures that data is always fresh, but may fail
with empty nodes and may populate the cache with superfluous data. By adding a time to live (TTL) value to each write, we are able to enjoy
the advantages of each strategy and largely avoid cluttering up the cache with superfluous data.

Note -> Log statements in lambda can not be use to determine performance errors
trottling errors in cloud metric will not give u the cause of the performance issues
So hence use X-ray for such tasks

Note -> There are two ways to configure server-side encryption for Amazon S3 artifacts:
- AWS CodePipeline creates an Amazon S3 artifact bucket and default AWS-managed SSE-KMS encryption keys when you create a pipeline using the
 Create Pipeline wizard. The master key is encrypted along with object data and managed by AWS.
- You can create and manage your own customer-managed SSE-KMS keys.

Note -> ECS is fully managed orchestration service. the Elastic Bean Stalk and Opswork service can efficiently manage the Docker Containers
but they r not for the fully Orchestration service

Note -> If the codepipeline breaks or their is any kind of failure then the entire process of pipeline stops running

Note -> their is nothing like EBS encryption key

Quetion -> u have been asked to develop an application on the AWS cloud. The application will involve picking up videos from users and
placing them in an ideal and durable data store, which of the following will be ideal data store, ensurng that the components r properly
decoupled ?
Answer -> S3
// DynamoDB is used to store JSON objects and not BLOB type Objects and EBS volumes could be used since they lead to tightly coupled
architecture and is not as durable as S3 and AWS Glacier could be used bcz they r used for achive storage

Question -> https://www.whizlabs.com/learn/course/quiz-result/187288 Question 55
Question -> https://www.whizlabs.com/learn/course/quiz-result/187288 Question 56

Question -> U r planning on deploying a built application onto an EC2 instance. There will be an no. of tests conducted on this instance.
U want to have ability to capture the logs from the web server so that it can help diagnose if any issues occur? How can u acheive this?
Answer -> Install the cloudwatch agent on the instance
// U can install the cloudwatch agent on the machine and then configure it to send the logs fot the webserver to a central location in
Cloudwatch

Note -> Cognito Streams gives developers control and insight into their data stored in cognito, Developers can now configure a kinesis stream
to receive events as data is updated an synchorinzed. cognito can push each dataset change to a kinesis stream in real time

Question -> https://www.whizlabs.com/learn/course/quiz-result/187288 question 64

Question -> https://www.whizlabs.com/learn/course/quiz-result/190344 question 65

Question -> https://www.whizlabs.com/learn/course/quiz-result/190344 question 60

Question -> https://www.whizlabs.com/learn/course/quiz-result/190344 question 55

Question -> https://www.whizlabs.com/learn/course/quiz-result/190344 question 52


Note -> By default, an alias points to a single Lambda function version. When the alias is updated to point to a different function version,
 incoming request traffic in turn instantly points to the updated version. This exposes that alias to any potential instabilities introduced
 by the new version. To minimize this impact, you can implement the routing-config parameter of the Lambda alias that allows you to point to
 two different versions of the Lambda function and dictate what percentage of incoming traffic is sent to each version.

Note -> U can enable server side encryption for kinesis streams also

Question -> u have been asked to move an existing development environmnet on cloud. this environmnet consists mainly of docker based container
. u need to ensure that minimum effort is taken during the migration process. Which steps would u consider?
Answer -> Create an application and environmnet  for the docker containers in Elastic beanstalk service
// the elastic beanstalk service is the ideal service to quickly provision development environmnets, u can also create environmnet which can
be used to host Docker based containers
// we cannot use option 'Create an OpsWork stack and deploy the Docekr containers' since using opswork is best suited when u have multiple
stacks and want to use configuration tools for the environmnet

Note -> Cloudwatch is used to see the invocation errors in AWS lambda

Note -> if the errors r being recieved intermittently(means at irregular level), its better to collect and aggregate the results at regular
intervals and then send the data to cloudwatch


Note -> Cloudwatch Metric only give the rate at which the function is executing but not help debug the actual error

Note -> Cross Account Access ->
Question -> https://www.whizlabs.com/learn/course/quiz-result/167803 question 9
// You can configure access to AWS CodeCommit repositories for IAM users and groups in another AWS account. This is often referred to as
 cross-account access. This section provides examples and step-by-step instructions for configuring cross-account access for a repository
 named MySharedDemoRepo in the US East (Ohio) Region in an AWS account (referred to as AccountA) to IAM users who belong to an IAM group
 named DevelopersWithCrossAccountRepositoryAccess in another AWS account (referred to as AccountB).

Note -> application version lifecycle policy -> A lifecycle policy tells Elastic Beanstalk to delete application versions that are old,
or to delete application versions when the total number of versions for an application exceeds a specified number.

Note -> CodeStar also uses the codepipeline to build pipelines

Note -> IAM roles r instantly applied, their is no need to refresh or wait


Note -> ELB provides access logs that capture detailed information about requests sent to ur load balancer, it contains informations such
 as when the requests was received,client ip address, latencies, request paths and server response, u can use these access logs to analyze
  traffic patterns and troubleshoot issues

Note -> We can use SQS to decouple distributed software systems and components

Note -> Reduce Page Size - Because a Scan operation reads an entire page (by default, 1 MB), you can reduce the impact of the scan operation
 by setting a smaller page size. The Scan operation provides a Limit parameter that you can use to set the page size for your request. Each
 Query or Scan request that has a smaller page size uses fewer read operations and creates a "pause" between each request. For example,
 suppose that each item is 4 KB and you set the page size to 40 items. A Query request would then consume only 20 eventually consistent
 read operations or 40 strongly consistent read operations. A larger number of smaller Query or Scan operations would allow your other
 critical requests to succeed without throttling.

 Question -> A developer is writing an application that will run on-premisis, but must have access AWS services through an AWS SDK. How can a
developer allow SDK to access AWS services?
Answer -> Create an IAM role with correct permissions, generate an access key and store it in aws credentials file

Note -> If a developer doesn't have access to edit the codebuild project but only have access to run the build. So for overridding the
exisitng buildwe can make use of the BuildSpecOverride attribute

Note -> Elastic Cache caching strategies -> https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html

Question -> A developer is building an application that needs access to an S3 bucket. An IAM role is created with the required permissions
to access the S3 bucket. Which API Should developer use in the application so that the codecan access to the S3 bucket?
Answer -> STS:AssumeRole
//A role specifies a set of permissions that you can use to access AWS resources. In that sense, it is similar to an IAM user. A principal
 (person or application) assumes a role to receive temporary permissions to carry out required tasks and interact with AWS resources.
  The role can be in your own account or any other AWS account. For more information about roles, their benefits, and how to create and
  configure them, see IAM Roles, and Creating IAM Roles. To learn about the different methods that you can use to assume a role, see Using
   IAM Roles.
Important - The permissions of your IAM user and any roles that you assume are not cumulative. Only one set of permissions is active at a
time. When you assume a role, you temporarily give up your previous user or role permissions and work with the permissions that are assigned
to the role. When you exit the role, your original permissions are automatically restored.
To assume a role, an application calls the AWS STS AssumeRole API operation and passes the ARN of the role to use. The operation creates
 a new session with temporary credentials. This session has the same permissions as the identity-based policies for that role.

Question -> A company is writing a lambda function that will return in multiple stages, such as dev, test and production. The function is
dependent upon several external services and it must call different endpoints for these services based on function's deployement stage.
What lambda feature will enable the developer to ensure that the code references the correct endpoints when running in each stage?
Answer -> Environment Variables
// u can create different environmnet variables in lambda function that can be used to point o different services
// we cannot use the option Aliasis bcz it is used for managing different versions of ur lambda function

Note -> RDS supports using the Transparent Data Encryption(TDE) ,look into above notes for - Transparent Data Encryption
TDE automatically encrypts data before it is written to storage and automatically decrypts data when the data is read from storage

Question -> An application is publishing a custom Cloudwatch metric any time an HTTP 504 error appears in the application logs.These errors r
 being received intermittently. their is a Cloudwatch alarm for this metric and the developer would like the alarm to trigger only if it
 breaches two evaluation periods or more?
 Answer -> The evaluation period and data points to alarms should be set to 2 while creating this alarm
 // when u create an alarm u specify three settings to enable cloudwatch to evaluate when to change the alarm state
 - Period -> it is length of time to evaluate the metric to create individual data points, it is expressed in seconds
 - Evaluation Period -> it is the number of the most recent data points to evaluate when determining the alarm state
 - Datapoints to alarm -> it is the number of data points within the evaluation time period that must be breaching to cause the alarm to
 go to the ALARM state

Question -> A developer has been asked to create an AWS Elastic Beanstalk environmnet for a production web application which needs to
handle thousands of requests.currently in dev environmnet is running on a t1 micro instance?
Answer -> Create an configuration file in S3 with an instnace type as m4.large and use the same during the environmnet creation

Question -> https://www.whizlabs.com/learn/course/quiz-result/182920 question 18 (Aurora RDS usecase)

Question -> u have beed asked to use the AWS services for development and deployemnet via API gateway, U need to control the behavour of
API's frontend interaction, we can achieve this by?
Answer -> Modify the configuration for method request
and Modify the configuration on the Method response
// bcz the front end interations r handled by Method request and Method response and the backend interactions r handled by the Integration
request and integration response

Question -> A company is planning on using the Codepipeline for their underlying CI/CD process. the code will be picked up from an S3 bucket.
The company policy mandates that all data should be encrypted at rest, what r measures should be taken?
Answer -> Ensure the server-side-encryption is enabled on the S3 bucket and
Configure AWS KMS with the customer managed keys and use it for S3 bucket encryption
// we cannot use the options such as Ensure the server-side-encryption is enabled on the codepipeline stage and option to configure the code
pickup stage in codepipeline to use in AWS KMS bcz encryption needs to be done at rest

Notes -> Time To Live (TTL) for DynamoDB allows you to define when items in a table expire so that they can be automatically deleted from
the database.
TTL is provided at no extra cost as a way to reduce storage usage and reduce the cost of storing irrelevant data without using provisioned
 throughput. With TTL enabled on a table, you can set a timestamp for deletion on a per-item basis, allowing you to limit storage usage to
  only those records that are relevant.

Question -> You need to migrate an existing on-premises application on AWS. It is legacy-based application with the little development support
.which of the following would be the best way to host this service?
Answer-> EC2 instnaces with EBS backed volumes
// Since the application is a legacy-based application with little development support, porting the application onto AWS lambda would be
difficult hence lambda based option would be incorrect
and EBS backed volumes is better for durability than instance store in which u could lose tht data if the instance is stopped

Question -> https://www.whizlabs.com/learn/course/quiz-result/182920 question 29 (SQS)

Note -> In DynamoDB encryption can be enabled only during the time of table creation

Note -> lambda@Edge - Lambda@Edge is an extension of AWS Lambda, a compute service that lets you execute functions that customize the
content that CloudFront delivers. You can author functions in one Region, US-East-1 (N. Virginia), and then execute them in AWS locations
globally that are closer to the viewer, without provisioning or managing servers. Lambda@Edge scales automatically, from a few requests per
day to thousands per second. Processing requests at AWS locations closer to the viewer instead of on origin servers significantly reduces
latency and improves the user experience.

Note -> CodeStar -> AWS CodeStar is a cloud-based service for creating, managing and working with software development projects on AWS. You
 can quickly develop, build and deploy applications on AWS with an AWS CodeStar project. An AWS CodeStar project creates and integrates AWS
  services for your project development toolchain. Depending on your choice of AWS CodeStar project template, that toolchain might include
  source control, build, deployment, virtual servers or serverless resources, and more. AWS CodeStar also manages the permissions required
  for project users (called team members). By adding users as team members to an AWS CodeStar project, project owners can quickly and simply
  grant each team member role-appropriate access to a project and its resources

Question -> Development team has created a set of Lambda functions that would be deployed in various AWS account and u need to automate the
deployment of these lambda functions. What we can use to automate the deployment?
Answer -> Cloudformation

- It is best practice to use Batching Message Actions on the SQS msgs to send, receive, delete msgs and to change the message visibility
timeout
// to combine client side buffering with request buffering with request batching, use long polling together with the buffered asynchronous
client included with AWS SDK for JAVA

Note -> Use Indexes Efficiently
- Keep the number of indexes to a minimum. Don't create secondary indexes on attributes that you don't query often. Indexes that are seldom
 used contribute to increased storage and I/O costs without improving application performance.
- Avoid indexing tables that experience heavy write activity

Note -> Resource Policies -> It let u create resouce based policies to allow or deny access to ur API's and methods from specified source
IP address or VPC endpoints

Note -> Key Policies -> this is used to provide authorization for KMS keys

Question -> U r developing a set of lambda functions for ur application. the company mandates that all calls to lambda functions be recorded
. which of the below can help achieve this?
Answer -> AWS cloudtrail
// lambda is integrated with cloudtrail, a service that captures API calls made by or on behalf of AWS lambda in ur AWS account and deliver
log files to an Amazon S3 bucket that u specify. Cloudtrail captures API calls made from the lambda console or from AWS lambda API. Using
this information collected by cloudtrail u can determine what request was made to AWS lambda, the source IP address from which the requests
was made to AWS lambda, the source IP address from which the request was made, who made the request, when it was made and so on
// cloudwatch is not correct answer for this because it gives the information on the logs from cloudwatch but not who called the lambda
function

Note -> Performance issues can be diagnosed by X-ray

Question -> u r defining a Redis cluster using elastic cache service. u need to define common values across the nodes for memory usage and
item sizes. Which of the following components of the elastic cache service allow u to define this?
Answer -> Parameter Groups
// cache parameter groups r an easy way to manage runtime settings for supported engine software. Parameters r used to control memory usage
, eviction, policies, item sizes and more. An elastic cache parameter group is a named collection of engine-specific parameters that u can
apply to cluster, buy doing this u make sure that all of the nodes in the cluster r configured in exactly the same way.

Note -> Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices,
distributed systems, and serverless applications    - @@@
Ex -> https://www.whizlabs.com/learn/course/quiz-result/194146 question 7

Note -> To connect to the DB instance u need to use the endpoints not IP address

Note -> The RDS Service contains several logs such as
    - Error log  contains diagnostic messages generated by the database engine, along with startup and shutdown times.
    - General query log  contains a record of all SQL statements received from clients, and also client connect and disconnect times.
    - Slow query log  contains a record of SQL statements that took longer to execute than a set amount of time and that examined more than
     a defined number of rows. Both thresholds are configurable.
    - Audit log  provided by using the MariaDB Audit Plugin, this log records database activity on the server for audit purposes. Refer to
     the Aurora MySQL documentation for more information on advanced auditing in Aurora.

Note -> In Elastic Beanstalk can do the swap of environmnets URL for example see below link
- https://www.whizlabs.com/learn/course/quiz-result/196967 question 3

Note -> Ad-hoc queries can run only in Relational Database

Note -> Look into scenerio of question https://www.whizlabs.com/learn/course/quiz-result/196967 question 8

Note -> If ur workload is mainly sending GET requests , then u should consider using the Cloudfront for performance optimization, see question
https://www.whizlabs.com/learn/course/quiz-result/196967 question 9

Note  -> To package ur serverless application u need to create an S3 bucket that the package command will use to upload ur zip deployment
see question to understand it https://www.whizlabs.com/learn/course/quiz-result/196967 question 11

Question -> What is AWS Config?
Answer -> AWS Config is a fully managed service that provides you with an AWS resource inventory, configuration history, and configuration
change notifications to enable security and governance. With AWS Config you can discover existing AWS resources, export a complete inventory
of your AWS resources with all configuration details, and determine how a resource was configured at any point in time. These capabilities
enable compliance auditing, security analysis, resource change tracking, and troubleshooting.

Note  -> AWS inspector is used to check EC2 instances for vulnerabilities

Note -> Typically, resources in an VPC are not accessible by AWS CodeBuild. To enable access, you must provide additional VPC-specific
configuration information as part of your AWS CodeBuild project configuration. This includes the VPC ID, the VPC subnet IDs, and the VPC
security group IDs. VPC-enabled builds are then able to access resources inside your VPC.
Ex -> Question -> https://www.whizlabs.com/learn/course/quiz-result/196967 question 13

Question -> https://www.whizlabs.com/learn/course/quiz-result/196967 question 14 (load balancer scenerio)

Note -> The X-Ray SDK provides:
    - Interceptors to add to your code to trace incoming HTTP requests
    - Client handlers to instrument AWS SDK clients that your application uses to call other AWS services
    - An HTTP client to use to instrument calls to other internal and external HTTP web services
  Question for this is https://www.whizlabs.com/learn/course/quiz-result/196967 question 18

Question -> As a database developer, u have started working with redshift. ur IT administrator has provided a Redshift cluster. u need now
to load data into the Redshift cluster from S3. Which of the following command should u use for this activity?
Answer -> COPY // not to use GET,MERGE,PUT

Question -> https://www.whizlabs.com/learn/course/quiz-result/196967 question 22 (cognito scenerio)

Note -> Amazon QuickSight is a business analytics service you can use to build visualizations, perform ad hoc analysis, and get business
 insights from your data. It can automatically discover AWS data sources and also works with your data sources. Amazon QuickSight enables
 organizations to scale to hundreds of thousands of users, and delivers responsive performance by using a robust in-memory engine (SPICE).
 Question -> https://www.whizlabs.com/learn/course/quiz-result/196967 question 23

 Note -> number_of_shards = max(incoming_write_bandwidth_in_KiB/1024, outgoing_read_bandwidth_in_KiB/2048)

 Question -> https://www.whizlabs.com/learn/course/quiz-result/196967 question 26 (Docker scenerio)

 Note -> You create a customer managed policy that allows users to administer a particular Amazon S3 bucket using the AWS Management
 Console. Upon creation, your customer managed policy has only one version, identified as v1, so that version is automatically set as the
 default. The policy works as intended.
 Later, you update the policy to add permission to administer a second Amazon S3 bucket. IAM creates a new version of the policy, identified
 as v2, that contains your changes. You set version v2 as the default, and a short time later your users report that they lack permission to
 use the Amazon S3 console. In this case, you can roll back to version v1 of the policy, which you know works as intended. To do this, you
 set version v1 as the default version. Your users are now able to use the Amazon S3 console to administer the original bucket.
 Question -> https://www.whizlabs.com/learn/course/quiz-result/196967 question 27

 Note -> Using CloudWatch Events, you can monitor the progress of jobs, build AWS Batch custom workflows with complex dependencies,
 generate usage reports or metrics around job execution, or build your own custom dashboards. With AWS Batch and CloudWatch Events, you
 can eliminate scheduling and monitoring code that continuously polls AWS Batch for job status changes. Instead, handle AWS Batch job state
 changes asynchronously using any CloudWatch Events target, such as AWS Lambda, Amazon Simple Queue Service, Amazon Simple Notification
 Service, or Amazon Kinesis Data Streams.
 Question -> https://www.whizlabs.com/learn/course/quiz-result/196967 question 29

 Question -> https://www.whizlabs.com/learn/course/quiz-result/196967 question 30 (DynamoDB Scenerio)

 Note -> If u r writing code that uses other resources, such as graphic library for image processing, or u want to use the AWS cli instead of
 the console, u need to first create the Lambda function deployment package and then use the console or the cli to upload the package
 Question -> https://www.whizlabs.com/learn/course/quiz-result/196967 question 32

 Note -> For web distributions, to control how long your objects stay in a CloudFront cache before CloudFront forwards another request to your
 origin, you can:
    - Configure your origin to add a Cache-Control or an Expires header field to each object.
    - Specify a value for Minimum TTL in CloudFront cache behaviors.
    - Use the default value of 24 hours.
 Question -> https://www.whizlabs.com/learn/course/quiz-result/196967 question 35

 Note -> By default, a subscriber of an Amazon SNS topic receives every message published to the topic. To receive only a subset of the messages, a
 subscriber assigns a filter policy to the topic subscription.
 Question -> https://www.whizlabs.com/learn/course/quiz-result/196967 question 37

 Note -> In Amazon Redshift, you can enable database encryption for your clusters to help protect data at rest. When you enable encryption
 for a cluster, the data blocks and system metadata are encrypted for the cluster and its snapshots.
 Encryption is an optional, immutable setting of a cluster. if u want encryption, u enable it during the cluster launch process and if u want
 to go from an unencrypted cluster to an encrypted cluster or the other way around, then unload ur data from the existing cluster and then
 reload it in a new cluster with the chosen encryption setting
 Question -> https://www.whizlabs.com/learn/course/quiz-result/196967 question 37

 Question -> https://www.whizlabs.com/learn/course/quiz-result/196967 question 42 (Cloudfront configure with HTTPS)

 Note -> Take advantage of Execution Context reuse to improve the performance of your function. Make sure any externalized configuration or
 dependencies that your code retrieves are stored and referenced locally after initial execution. Limit the re-initialization of
 variables/objects on every invocation. Instead use static initialization/constructor, global/static variables and singletons. Keep alive
 and reuse connections (HTTP, database, etc.) that were established during a previous invocation.
 Question -> https://www.whizlabs.com/learn/course/quiz-result/196967 question 46

 Question -> https://www.whizlabs.com/learn/course/quiz-result/196967 question 56 (Use case for Lambda Monitoring)

 Note -> AWS Lambda uses environment variables to facilitate communication with the X-Ray daemon and configure the X-Ray SDK.
    - _X_AMZN_TRACE_ID: Contains the tracing header, which includes the sampling decision, trace ID, and parent segment ID.  If Lambda
      receives a tracing header when your function is invoked, that header will be used to populate the _X_AMZN_TRACE_ID environment
      variable. If a tracing header was not received, Lambda will generate one for you.
    - AWS_XRAY_CONTEXT_MISSING: The X-Ray SDK uses this variable to determine its behavior in the event that your function tries to record
      X-Ray data, but a tracing header is not available. Lambda sets this value to LOG_ERROR by default.
    - AWS_XRAY_DAEMON_ADDRESS: This environment variable exposes the X-Ray daemon's address in the following format: IP_ADDRESS:PORT. You
      can use the X-Ray daemon's address to send trace data to the X-Ray daemon directly, without using the X-Ray SDK.

 Question -> https://www.whizlabs.com/learn/course/quiz-result/196967 question 59 (cron.yaml)

 Question -> https://www.whizlabs.com/learn/course/quiz-result/196967 question 63 (DynamoDB streams Scenerio)

 Question -> https://www.whizlabs.com/learn/course/quiz-result/196967 question 64 (Cloudwatch events to schedule lambda function)

 Question -> https://www.whizlabs.com/learn/course/quiz-result/196967 question 65 (API gateway and lambda Scenerio)

 https://www.udemy.com/aws-certified-developer-associate-dva-c01/learn/v4/t/quiz/4517280/result/133920472 question 39

 https://www.udemy.com/aws-certified-developer-associate-dva-c01/learn/v4/t/quiz/4517280/result/133920472 question 55

 Extra Notes ->

 4xx client induced errors Load balancers
5xx errors are application induced errors LB
503 then it means ur ELB has no more capacity or no registered targets

403 (Forbidden Error) S3 - make sure the bucket policy allows public reads
403 (Access Denied Error) S3 - because we need to add the bucket policy for write permissions
404 S3 - CORS are not approved
404 S3 Eventual Consistent error also
S3 API 409 Conflict error cause is Your're attempting to remove a bucket without emptying the contents of the bucket first

429 trottle error lambda for synchronous invocation(means excution of lambda from console)
If ur application is not accessible(time out), then it's a security group issue.
If your application receives a "connection refused" error, then it's a application error or it's not launched.

DynamoDB Trottling - Notes
DynamoDB Internal Partitions - Notes
Read replicas do asynchrouns replication(evantually consistent) and multi AZ do synchronous replication

Security Groups are Locked down to a region/VPC combination
AMI's are built for specific AWS region
EBS r locked at AZ level
Read Replicas can be within the same AZ or cross AZ or cross region
VPC are per account per Region
Subnets are per VPC per AZ
cloudformation stacks r region dependent

RDS Backup Retention 7 days to 35 days
RDS Backup Snapshots retention is as long as u want
SQS Queue(Standard Queue) default retention of msgs is 4 days and maximum is 14 days using the SetQueueAttributes action
in SQS can set the data key reuse period(between 1 min to 24 hrs)
Kinesis Streams data retention is 1 day by default and can go upto 7 days
DynamoDB Streams have 24 hrs of data retention
cloudwatch By default, logs are kept indefinitely and never expire but u can choose a retention periods between 10 years and one day
By defaults ur items live 5 mins in ur DAX cache
Temporary credentials can be valid between 15 min to 1hr

- Cloudformation -> event logs - To check what happens during the creation of a stack go to the Events tab(First thing is the event logs will be the log   for the creation of the cloudformation stack)
- DAX can use cloudtrail
- codecommit trigger notifications using lambda/SNS , cloudwatch events
- the state change in the code pipeline will generate a cloudwatch event and this event can trigger a SNS Notification we can also use cloudtrail
- when codebuild is in own VPC we can using cloudtrail for API call logging
- codbuild logs r outputed to S3 and cloudwatch logs //when the code build finishs the whole container that run the build goes away and the only thing that is left for u to troubleshoot are S3 logs and the cloudwatch logs
- in codebuild u also has cloudwatch metrics for the statics and use cloudwatch alarms to detect failed builds and SNS notfications
- we can use the cloud watch events or AWS Lambda as a Glue for everything and trigger the SNS notifications in codebuild
- ELB provides access logs that capture detailed information about requests sent to ur load balancer, it contains informations such as when the requests was received,client ip address, latencies, request paths and server response, u can use these access logs to analyze traffic patterns and troubleshoot issues
- API Gateway can use cloudwatch, x-ray
-We can audit the key usage using cloudtrail in KMS
- for SSM we can use cloudwatch events

"Sig v4" capability where IAM credentials are in headers and header is passed to API Gateway

codebuild - buildspec.yml - build instructions
codedeploy - appspec.yml

- Envelope Encryption it uses Generate Data key API and it is Client side encryption

- codebuild uderhood leverages docker

- DynamoDB streams can be read by AWS lambda to do changes in it and cross region replication

- api gateway authentication - IAM sigv4

- FilterExpression to further filter(the filtering only happens client side)  it is done in DynamoDB Query

- ELB - Workers - ASG

- cloudwatch can also use FilterExpression

- for scan u can also use the ProjectionExpression + FilterExpression (no change to RCU)

- ProjectionExpression - A projection expression is a string that identifies the attributes you want. To retrieve a single attribute, specify its name. For multiple attributes, the names must be comma-separated

- DynamoDB GetItem -> if u want only certain attributes in the results u can use something called ProjectionExpression // this helps u save
in network badwidth

RDS- OLTP
Redshift -OLAP

- Stage variable - it is like environment variables for API Gateway

- X-ray Agent and Other Terms - See FAQs

- Cloudfront - It is a Global Service (Read Notes)

- X-ray sampling - Not for determining cause of failure instead use to provide a representative sample of the requests that your application serves, uses sampling algorithm to determine which requests get traced

- Canary Deployement - API gateway More it is like Blue / green deployement (Read Notes)

- AWS SWF - Notes

- cron - cloudwatch

- transfer acceleration - s3 - to upload a file in another region by using endpoints

Horizontal scaling means that you scale by adding more machines into your pool of resources whereas Vertical scaling means that you scale by adding more power (CPU, RAM) to an existing machine. ... It provides an easy way to scale vertically by switching from small to bigger machines. This process often involves downtime.

- instance profile - ec2 security credentials - notes

- batch write (put and delete) in DynamoDB 25 with 16 mb data

- batch get in DynamoDB 100 with 16 mb data

- scan return upto 1mb data

- FilterExpression is used with dynamoDB query

error tracing table in respect of services and logs?
Deployement Types?
GSI and LSI ?
Security Scenerios?
Cloud trail?
global tables?
cloudstar?

transfer acceleration - s3

#3 -> Dynamo DB
#4 -> API Gateway*
#5 -> SAM
#6 -> CICD
#9 -> Cloudformation
#10 -> Monitoring

Elastic cache caching strategies -> https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html

AWS Glue?
Web Identity ,federation identity and SAML use cases and conditions?

Identity Federation uses the SAML standard(such as Active Directory)

cloudfront also works with EC2 with load balancing

ECR can use code build to automate tasks  and data is iencrypted in flight and at rest

- their r 4 configs u need to know about and their r about 35 configs for this file     - @@@
      - ECS_CLUSTER = MyCluster # Assign EC2 instance to an ECS cluster
      - ECS_ENGINE_AUTH_DATA = {...} # to pull images from private registeries
      - ECS_AVAILABLE_LOGGING_DRIVERS = [...] # cloudwatch container logging
      - ECS_ENABLE_TASK_IAM_ROLE = true # Enable IAM roles for ECS tasks

      Note: Remember the above configuration tasks performed in sequence  - @@@

An Alarm on a High Resolution Metric can be triggered as often as 10 seconds

SWF is 1 year max runtime, and code runs on EC2 (not Serverless)

To compute the no. of partitions:                           - @@@
    - By capacity: (Total RCU/3000) + (Total WCU/1000)
    - u get the size of the data set: total size / 10 GB
    - total partitions = CEILING(MAX(Capacity,Size))

DynamoDB cache 5 mins by default and have 10 nodes
Api gateway cahce
with dynamodb streams we can also do insert data into elasticsearch

for Envelope Encryption use the aws encryption sdk
SSM Parameter Store can do Seamless Encryption using KMS, u can use SSM with cloudformation template also
GetParameters or GetParametersByPath API for the SSM to define the different parameters and passwords according to environment
There are over 224 types of resources for cloudfromation
DLQ fro the lambda could be SNS and SQS
